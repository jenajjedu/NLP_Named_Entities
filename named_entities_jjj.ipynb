{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jenajjedu/NLP_Named_Entities/blob/main/named_entities_jjj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLUqIfy6JjE3",
        "outputId": "44cd5d4c-b027-44bf-b58c-951737497110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/NE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZL0t1bHJxzE",
        "outputId": "15edca2b-7b26-4893-e5c0-d33635bff13c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/NE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vR6BKtRkIGxF",
        "outputId": "62b5e349-1d80-44b0-d31e-213c7f4555fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 32.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-22.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.2.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement sklearn-learn (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for sklearn-learn\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
            "Installing collected packages: fonttools, matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "Successfully installed fonttools-4.38.0 matplotlib-3.5.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.1.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pip\n",
        "!pip install -U spacy\n",
        "!pip install -U gensim\n",
        "!pip install -U sklearn-learn\n",
        "!pip install -U pandas\n",
        "!pip install -U matplotlib\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Use Pandas\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from statistics import mean\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import re\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "import numpy as np\n",
        "import codecs\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from matplotlib import pyplot"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "w__FtOugIGxI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def loadDataFromCSV(filePath):\n",
        "    df = pd.read_csv(filePath)\n",
        "    x = df['text']\n",
        "    y = df['sentiment']\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def remove_extras(in_texts):\n",
        "    textList = []\n",
        "    for in_text in in_texts:\n",
        "        t_en = in_text.encode(\"ascii\", \"ignore\")\n",
        "        t = t_en.decode()\n",
        "        t = re.sub(\"&lt;\", \"<\", t)\n",
        "        t = re.sub(\"&gt;\", \">\", t)\n",
        "        t = re.sub(\"&amp;\", \"&\", t)\n",
        "        t = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', t, flags=re.MULTILINE) # URLs\n",
        "        t = re.sub(r'@[\\w]+','', t, flags=re.MULTILINE) # mentions\n",
        "        t = re.sub(r'#[\\w]+','', t, flags=re.MULTILINE) # hashtags\n",
        "        t = re.sub(\"[!@#$%^&*()[]{};:,/<>\\|`~-=_+]\", \" \", t)\n",
        "        t = re.sub(r\"([.!?])\", r\" \\1\", t)\n",
        "        textList.append(t + '\\n')\n",
        "    return textList\n",
        "\n",
        "\n",
        "def preprocess(tweetDict, batchsz=500):\n",
        "    # Input: a wiki text dictionary with keys are titles and values are the corresponding texts.\n",
        "    # Output: a wiki text dictionary with keys are the titles and the values are the preprocessed texts\n",
        "    # (sentences - tokens).\n",
        "\n",
        "    # sub-task 1: remove all the references texts \"[...]\"\n",
        "    # sub-task 2: segment all the sentences in the wiki texts.\n",
        "    # sub-task 3: tokenize the sentences from sub-task 2.\n",
        "    # sub-task 4: lemmatize the tokens from sub-task 3.\n",
        "    # sub-task 5: lower-case the tokens from sub-task 3/4.\n",
        "\n",
        "\n",
        "    texts = list(tweetDict.values())\n",
        "    clean_texts = remove_extras(texts) # Step 1\n",
        "    clean_texts2 = []\n",
        "    print(len(clean_texts), len(clean_texts[0]))\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    print(nlp.pipe_names)\n",
        "    docsDict = {}\n",
        "    docsSpanDict = {}\n",
        "    cnt = 0\n",
        "    # pipeline accomplishes steps [2-4]\n",
        "    for doc in tqdm(nlp.pipe(clean_texts, batch_size=batchsz)):\n",
        "        docSpanList = []\n",
        "        docList = []\n",
        "        # print(list(doc.sents), type(doc))\n",
        "        clean_texts2.append(list(doc.sents))\n",
        "        for ent in doc.ents:\n",
        "            #  THIS LINE GIVES THE NAMED ENTITIES IN DOC\n",
        "            print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "        for spanText in doc.sents:\n",
        "            spanList = []\n",
        "            # print(spanText.text, type(spanText))\n",
        "            for tok in spanText: # and not tok.text=='not'\n",
        "                #  THIS LINE GIVES THE PART OF SPEECH\n",
        "                # print(tok.text, type(tok),nlp(tok.lemma_.lower()), tok.pos_)\n",
        "                if (tok.is_stop  or tok.is_bracket\n",
        "                or tok.is_punct or tok.is_space\n",
        "                or tok.is_currency or tok.is_digit\n",
        "                or tok.like_num or tok.pos_ == \"PROPN\"):\n",
        "                    # if tok.is_stop: print(f' stop {tok.text}')\n",
        "                    continue\n",
        "                # print(f' token lemma {tok.lemma_} {tok.text}')\n",
        "                docList.append(tok.lemma_.lower()) # step 5\n",
        "                spanList.append(tok.lemma_.lower())\n",
        "            docSpanList.append(spanList)\n",
        "            # print(spanList)\n",
        "        docsDict[list(tweetDict.keys())[cnt]] = docList\n",
        "        docsSpanDict[list(tweetDict.keys())[cnt]] = docSpanList\n",
        "        # print(docSpanList)\n",
        "        cnt += 1\n",
        "    # print(clean_texts2)\n",
        "    # print(docsDict)\n",
        "    return docsDict, docsSpanDict"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "d2653gEBIGxJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# load data from csv\n",
        "xTrain,yTrain = loadDataFromCSV('/content/drive/My Drive/Colab Notebooks/NE/data/sentiment-train.csv')"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "mOyecT0iIGxO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "I LOVE @Health4UandPets u guys r the best!! \n",
            "@LutheranLucciol Make sure you DM me if you post a link to that video! &lt;LOL&gt;So I don't miss it   Better get permission and blessing first?\n",
            "reaching amritsar in an hour and (if i find a bus) should be at wagah border by 2pm  - http://bkite.com/06fuJ\n",
            "@leiabox so what can you tell us about it? I'm totally geeking out right now \n",
            "@jeffswarens depends on which version they thought you  the one I know doesn't go like that ;)\n",
            "@markhoppus lol you�re so fucking funny \n",
            "@GinaDeAngelo ha, definitely not vegan. I'm drinking it, still alive too. All is well \n",
            "Sleep. Then AT&amp;T in the morning. Oh the sweet smell of the boys of summer. \n",
            "8 31\n",
            "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "8it [00:00, 148.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOVE 2 6 ORG\n",
            "first 117 122 ORDINAL\n",
            "an hour 21 28 TIME\n",
            "2pm  - 80 86 QUANTITY\n",
            "AT&T 13 17 ORG\n",
            "summer 69 75 DATE\n",
            "8\n",
            "['love', 'guy', 'r', 'good']\n",
            "['sure', 'dm', 'post', 'link', 'video', 'lol', 'miss', 'well', 'permission', 'blessing']\n",
            "['reach', 'hour', 'find', 'bus', 'wagah', 'border', 'pm']\n",
            "['tell', 'totally', 'geeke', 'right']\n",
            "['depend', 'version', 'think', 'know', 'like']\n",
            "['fucking', 'funny']\n",
            "['ha', 'definitely', 'vegan', 'drink', 'alive']\n",
            "['sleep', 'morning', 'oh', 'sweet', 'smell', 'boy', 'summer']\n",
            "8\n",
            "[['love', 'guy', 'r', 'good']]\n",
            "[['sure', 'dm', 'post', 'link', 'video'], ['lol'], ['miss', 'well', 'permission', 'blessing']]\n",
            "[['reach', 'hour', 'find', 'bus', 'wagah', 'border', 'pm']]\n",
            "[['tell'], ['totally', 'geeke', 'right']]\n",
            "[[], ['depend', 'version', 'think', 'know', 'like']]\n",
            "[['fucking', 'funny']]\n",
            "[['ha', 'definitely', 'vegan'], ['drink', 'alive'], []]\n",
            "[['sleep'], ['morning'], ['oh', 'sweet', 'smell', 'boy', 'summer']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Test my preprocess function\n",
        "xTrainDict = xTrain.to_dict()\n",
        "print(len(xTrainDict))\n",
        "count_idx = 0\n",
        "testDict = {}\n",
        "for i in [0,14,28,71,80,135,735,1698]:\n",
        "    print(xTrainDict[i])\n",
        "    testDict[count_idx] = xTrainDict[i]\n",
        "    count_idx += 1\n",
        "\n",
        "tokenTweetDict, b = preprocess(testDict)\n",
        "print(len(tokenTweetDict))\n",
        "for j in range(8):\n",
        "    print(tokenTweetDict[j])\n",
        "\n",
        "print(len(b))\n",
        "for j in range(8):\n",
        "    print(b[j])"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm3lZzdqIGxP",
        "outputId": "9c8d4513-d6c2-4f1c-fee9-43c6b7dafbc9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "zj829GQWIGxQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple 0 5 ORG\n",
            "U.K. 27 31 GPE\n",
            "$1 billion 44 54 MONEY\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rea8XH6bIGxQ",
        "outputId": "2682f029-2fa9-4240-bc59-39dfac728c64"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import math\n",
        "# this ensures that the current MacOS version is at least 12.3+\n",
        "print(torch.cuda.is_available())\n",
        "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
        "# print(torch.backends.mps.is_built())"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcGyh9gsIGxQ",
        "outputId": "ba7807f3-b1d8-4ac7-9f80-cd3389d64d4c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.1+cu113\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlsZ_fQeIGxR",
        "outputId": "6ad3c285-e9a6-475d-c34a-4d7c381f900e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "J3Rqk8dDIGxS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "xfP7gOeXIGxS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 2300.14697265625\n",
            "199 1628.858642578125\n",
            "299 1154.302734375\n",
            "399 818.8004150390625\n",
            "499 581.5927124023438\n",
            "599 413.87158203125\n",
            "699 295.2753601074219\n",
            "799 211.41116333007812\n",
            "899 152.104736328125\n",
            "999 110.16309356689453\n",
            "1099 80.50064086914062\n",
            "1199 59.52149963378906\n",
            "1299 44.683258056640625\n",
            "1399 34.18797302246094\n",
            "1499 26.764328002929688\n",
            "1599 21.51321029663086\n",
            "1699 17.798696517944336\n",
            "1799 15.171065330505371\n",
            "1899 13.312278747558594\n",
            "1999 11.997316360473633\n",
            "Result: y = -0.05957949161529541 + 0.8533886075019836 x + 0.010278450325131416 x^2 + -0.09285356849431992 x^3\n"
          ]
        }
      ],
      "source": [
        "# Create random input and output data\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Randomly initialize weights\n",
        "a = torch.randn((), device=device, dtype=dtype)\n",
        "b = torch.randn((), device=device, dtype=dtype)\n",
        "c = torch.randn((), device=device, dtype=dtype)\n",
        "d = torch.randn((), device=device, dtype=dtype)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss)\n",
        "\n",
        "# Backprop to compute gradients of a, b, c, d with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_a = grad_y_pred.sum()\n",
        "    grad_b = (grad_y_pred * x).sum()\n",
        "    grad_c = (grad_y_pred * x ** 2).sum()\n",
        "    grad_d = (grad_y_pred * x ** 3).sum()\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    a -= learning_rate * grad_a\n",
        "    b -= learning_rate * grad_b\n",
        "    c -= learning_rate * grad_c\n",
        "    d -= learning_rate * grad_d\n",
        "\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdPkcGXwIGxS",
        "outputId": "5c555dd5-229e-4fdf-8349-54ed13d581f1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import re\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "qoPzZmtxIGxT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# read pickle file\n",
        "pickle_in = open(\"plots_text.pickle\",\"rb\")\n",
        "movie_plots = pickle.load(pickle_in)\n",
        "\n",
        "# count of movie plot summaries\n",
        "len(movie_plots)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwUHw172IGxT",
        "outputId": "9b225a88-68bd-4c06-a8c7-ad1b020abb32"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"it focuses on maritime tragedy of the town and a forbidden love between julha  and joão moço , from different fisher castes, in a community where mixed-caste marriages were not allowedala-arriba - amor de perdição and dating without parent's assent was seen as a disgrace to the family, not only in respect to women, but also men.\",\n",
              " 'as stated in the copy of one release of this film, \"the suckling has been compared to alien for its claustrophobic intensity and die hard for its non-stop action.\" given the film\\'s low budget and technical flaws, discerning filmgoers may disagree.the suckling dvd at horrortalk.com',\n",
              " 'when jonathan ogner  first shows up to prep school, he is laughed at for wearing his school uniform. he then goes up to his dorm and meets his new roommate , who introduces himself as franklin burroughs iv but tells jonathan to call him \"skip.\" skip then takes off his trench coat and is shown to be wearing a red bra and girls\\' underwear. he explains to the shocked jonathan that it isn\\'t what it looks like and that it\\'s a tradition for the seniors to parade around campus wearing only girls\\' underwear. when jonathan doesn\\'t have any, skip gives him a set that he had in his dresser. skip and jonathan travel out of the dorm together until they get to the final door where skip stays behind and locks the door. the other students begin to laugh and mock jonathan for wearing girls\\' underwear. mortified, jonathan attempts to flee the scene. after discovering that skip has locked all the doors, jonathan climbs a trellis that leads into his dorm where he finds skip lying on the floor laughing hysterically. skip tries to tell jonathan that it was all just a practical joke and to just laugh it off, but jonathan is too embarrassed to see the humor. later, during lunch time in the cafetaria, the other students again begin to taunt jonathan as he tries to eat his meal. when skip invites jonathan over to his table to sit with him and his friends, jonathan turns to reveal that he is crying from shame. skip is now deeply remorseful for having played such a prank on jonathan as he sees jonathan flee the cafeteria. when skip returns to their room to apologize to jonathan he finds jonathan hanging with a rope around his neck in an apparent suicide. skip goes to get help, but when he returns to the room where jonathan hanged himself, skip and the gathering crowd find not jonathan but a mannequin with a picture of the dean\\'s face attached to its head. the crowd begins to laugh hysterically at skip as the dean says he wants to see both skip and jonathan in his office. as the crowd disperses, skip hears laughter coming from the closet. upon opening the closet door skip finds jonathan very much alive and laughing at skip telling skip that it was just a joke. skip grudgingly accepts the prank reversal and the two become fast friends. after becoming friends the two share secrets and jonathan admits to skip that he cheated on the sat exam. after several failed attempts to find jonathan a date, skip decides that it is his sworn duty to help his friend have a successful sexual encounter fast for all their sakes. skip decides to send jonathan to chicago to meet a girl and gain sexual experience before both of their reputations are ruined. jonathan is picked up by ellen, a beautiful older woman, and has an affair with her. jonathan begins to fall in love with ellen even though the older woman knows it to be just a fling between them. jonathan lies and claims to be a ph.d. student. when jonathan proclaims his love to ellen during one of their sessions, ellen begins to have second thoughts about continuing the relationship. her decision is finalized when she discovers that jonathan is not only much younger than he had originally claimed to be, but he also attends the same school that her own son attends. over christmas break, skip invites jonathan to spend christmas with him and his family at the burroughs\\'s estate. it is here that jonathan discovers that ellen is skip\\'s mother and is married. jonathan tries to end the affair, but skip\\'s mother contacts jonathan several times. eventually jonathan agrees to meet ellen to talk. he lies to skip, claiming to need time alone. when jonathan and ellen meet, they end up in bed again. in an attempt to cheer up his friend, skip and friends go to jonathan\\'s hotel room. there they discover jonathan in bed with skip\\'s mother. skip is very upset by this and is very cold toward jonathan. however, he does not turn john in during an investigation into cheating on the sat. at the end of the movie, skip and jonathan have a violent fist fight, but make up in the last scene.',\n",
              " 'in this crime caper set in the eccentric london art world, nick edwards  owes £50,000 to the super-smooth, yet brutal, crime lord foster wright  and has four days to find the cash. nick knows nothing about working a heist of that size, but when he stumbles across a lost sketch by the legendary italian artist antonio fraccini, he believes he’s in the clear. the problem is, it’s only worth 15 grand. with the help of the eternal cynic eve  and her extremely talented yet naïve artist brother tony , the plan is hatched; to forge the drawing and sell it to five mayfair galleries within an hour before anyone cottons onto the fact there’s a scam going down.',\n",
              " 'blind mountain follows a young woman, bai xuemei, in the early 1990s who recently graduated from college and attempts to find work to help pay for her brother\\'s education. in the process, she is drugged, kidnapped and sold as a bride to a villager in the qin mountains of china\\'s shaanxi province. trapped in the fiercely traditional town, the young woman finds that her avenues of escape are all blocked. as she searches for allies, including a young boy, a school teacher and a mailman, she suffers from being raped by her \"husband\" and continued beatings at the hands of the villagers, her husband, and her husbands\\' parents.']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# sample random summaries\n",
        "random.sample(movie_plots, 5)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgvz6N7GIGxU",
        "outputId": "046ada45-25c8-4c7f-fa94-c55333f27ff6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# clean text\n",
        "movie_plots = [re.sub(\"[^a-z' ]\", \"\", i) for i in movie_plots]"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ul2kob8UIGxU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# create sequences of length 5 tokens\n",
        "def create_seq(text, seq_len = 5):\n",
        "\n",
        "    sequences = []\n",
        "\n",
        "    # if the number of tokens in 'text' is greater than 5\n",
        "    if len(text.split()) > seq_len:\n",
        "      for i in range(seq_len, len(text.split())):\n",
        "        # select sequence of tokens\n",
        "        seq = text.split()[i-seq_len:i+1]\n",
        "        # add to the list\n",
        "        sequences.append(\" \".join(seq))\n",
        "\n",
        "      return sequences\n",
        "\n",
        "    # if the number of tokens in 'text' is less than or equal to 5\n",
        "    else:\n",
        "\n",
        "      return [text]"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "btYvVSHWIGxV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "152644"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "seqs = [create_seq(i) for i in movie_plots]\n",
        "\n",
        "# merge list-of-lists into a single list\n",
        "seqs = sum(seqs, [])\n",
        "\n",
        "# count of sequences\n",
        "len(seqs)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBEDYSDJIGxV",
        "outputId": "6f0ba23f-8541-4ada-e137-b2fa2749cb95"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# create inputs and targets (x and y)\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for s in seqs:\n",
        "  x.append(\" \".join(s.split()[:-1]))\n",
        "  y.append(\" \".join(s.split()[1:]))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "P5QAF2R2IGxV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13129, 'considerable')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# create integer-to-token mapping\n",
        "int2token = {}\n",
        "cnt = 0\n",
        "\n",
        "for w in set(\" \".join(movie_plots).split()):\n",
        "  int2token[cnt] = w\n",
        "  cnt+= 1\n",
        "\n",
        "# create token-to-integer mapping\n",
        "token2int = {t: i for i, t in int2token.items()}\n",
        "\n",
        "token2int[\"the\"], int2token[14271]"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hHBP390IGxW",
        "outputId": "73de706c-5e29-434b-e0d4-21471a4fb6e3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16592"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# set vocabulary size\n",
        "vocab_size = len(int2token)\n",
        "vocab_size"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_vEgmyXIGxW",
        "outputId": "d588b6cb-f228-4060-e7b1-25bac9a6e1df"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def get_integer_seq(seq):\n",
        "  return [token2int[w] for w in seq.split()]\n",
        "\n",
        "# convert text sequences to integer sequences\n",
        "x_int = [get_integer_seq(i) for i in x]\n",
        "y_int = [get_integer_seq(i) for i in y]\n",
        "\n",
        "# convert lists to numpy arrays\n",
        "x_int = np.array(x_int)\n",
        "y_int = np.array(y_int)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "qTxp23gLIGxX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def get_batches(arr_x, arr_y, batch_size):\n",
        "\n",
        "    # iterate through the arrays\n",
        "    prv = 0\n",
        "    for n in range(batch_size, arr_x.shape[0], batch_size):\n",
        "      x = arr_x[prv:n,:]\n",
        "      y = arr_y[prv:n,:]\n",
        "      prv = n\n",
        "      yield x, y"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "opGL1hQwIGxX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class WordLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n",
        "        super().__init__()\n",
        "\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "\n",
        "        self.emb_layer = nn.Embedding(vocab_size, 200)\n",
        "\n",
        "        ## define the LSTM\n",
        "        self.lstm = nn.LSTM(200, n_hidden, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        ## define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        ## define the fully-connected layer\n",
        "        self.fc = nn.Linear(n_hidden, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network.\n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "       ## pass input through embedding layer\n",
        "        embedded = self.emb_layer(x)\n",
        "\n",
        "        ## Get the outputs and the new hidden state from the lstm\n",
        "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
        "\n",
        "        ## pass through a dropout layer\n",
        "        out = self.dropout(lstm_output)\n",
        "\n",
        "        #out = out.contiguous().view(-1, self.n_hidden)\n",
        "        out = out.reshape(-1, self.n_hidden)\n",
        "\n",
        "        ## put \"out\" through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        # if GPU is available   mps is for mac code\n",
        "        if (torch.cuda.is_available()):\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "\n",
        "        # if GPU is not available\n",
        "        else:\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "\n",
        "        return hidden"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ya_f0O32IGxX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordLSTM(\n",
            "  (emb_layer): Embedding(16592, 200)\n",
            "  (lstm): LSTM(200, 256, num_layers=4, batch_first=True, dropout=0.3)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=16592, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# instantiate the model\n",
        "net = WordLSTM()\n",
        "\n",
        "# push the model to GPU (avoid it if you are not using the GPU)\n",
        "#net.to(device)\n",
        "\n",
        "print(net)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8Fm2ndAIGxY",
        "outputId": "1034d7de-7d85-4e9d-b045-6259f0b1c7f6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "zxGk5YXsIGxY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def train(net, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
        "\n",
        "    # optimizer\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    # loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # push model to GPU\n",
        "    net.cuda()\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    for e in range(epochs):\n",
        "\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "\n",
        "        for x, y in get_batches(x_int, y_int, batch_size):\n",
        "            counter+= 1\n",
        "\n",
        "            # convert numpy arrays to PyTorch arrays\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "            # push tensors to GPU\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # detach hidden states\n",
        "            h = tuple([each.data for each in h])\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "\n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "\n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(-1))\n",
        "\n",
        "            # back-propagate error\n",
        "            loss.backward()\n",
        "\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "\n",
        "            # update weigths\n",
        "            opt.step()\n",
        "\n",
        "            if counter % print_every == 0:\n",
        "\n",
        "              print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                    \"Step: {}...\".format(counter))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "s4id12PsIGxY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/50... Step: 256...\n",
            "Epoch: 1/50... Step: 512...\n",
            "Epoch: 1/50... Step: 768...\n",
            "Epoch: 1/50... Step: 1024...\n",
            "Epoch: 1/50... Step: 1280...\n",
            "Epoch: 1/50... Step: 1536...\n",
            "Epoch: 1/50... Step: 1792...\n",
            "Epoch: 1/50... Step: 2048...\n",
            "Epoch: 1/50... Step: 2304...\n",
            "Epoch: 1/50... Step: 2560...\n",
            "Epoch: 1/50... Step: 2816...\n",
            "Epoch: 1/50... Step: 3072...\n",
            "Epoch: 1/50... Step: 3328...\n",
            "Epoch: 1/50... Step: 3584...\n",
            "Epoch: 1/50... Step: 3840...\n",
            "Epoch: 1/50... Step: 4096...\n",
            "Epoch: 1/50... Step: 4352...\n",
            "Epoch: 1/50... Step: 4608...\n",
            "Epoch: 2/50... Step: 4864...\n",
            "Epoch: 2/50... Step: 5120...\n",
            "Epoch: 2/50... Step: 5376...\n",
            "Epoch: 2/50... Step: 5632...\n",
            "Epoch: 2/50... Step: 5888...\n",
            "Epoch: 2/50... Step: 6144...\n",
            "Epoch: 2/50... Step: 6400...\n",
            "Epoch: 2/50... Step: 6656...\n",
            "Epoch: 2/50... Step: 6912...\n",
            "Epoch: 2/50... Step: 7168...\n",
            "Epoch: 2/50... Step: 7424...\n",
            "Epoch: 2/50... Step: 7680...\n",
            "Epoch: 2/50... Step: 7936...\n",
            "Epoch: 2/50... Step: 8192...\n",
            "Epoch: 2/50... Step: 8448...\n",
            "Epoch: 2/50... Step: 8704...\n",
            "Epoch: 2/50... Step: 8960...\n",
            "Epoch: 2/50... Step: 9216...\n",
            "Epoch: 2/50... Step: 9472...\n",
            "Epoch: 3/50... Step: 9728...\n",
            "Epoch: 3/50... Step: 9984...\n",
            "Epoch: 3/50... Step: 10240...\n",
            "Epoch: 3/50... Step: 10496...\n",
            "Epoch: 3/50... Step: 10752...\n",
            "Epoch: 3/50... Step: 11008...\n",
            "Epoch: 3/50... Step: 11264...\n",
            "Epoch: 3/50... Step: 11520...\n",
            "Epoch: 3/50... Step: 11776...\n",
            "Epoch: 3/50... Step: 12032...\n",
            "Epoch: 3/50... Step: 12288...\n",
            "Epoch: 3/50... Step: 12544...\n",
            "Epoch: 3/50... Step: 12800...\n",
            "Epoch: 3/50... Step: 13056...\n",
            "Epoch: 3/50... Step: 13312...\n",
            "Epoch: 3/50... Step: 13568...\n",
            "Epoch: 3/50... Step: 13824...\n",
            "Epoch: 3/50... Step: 14080...\n",
            "Epoch: 4/50... Step: 14336...\n",
            "Epoch: 4/50... Step: 14592...\n",
            "Epoch: 4/50... Step: 14848...\n",
            "Epoch: 4/50... Step: 15104...\n",
            "Epoch: 4/50... Step: 15360...\n",
            "Epoch: 4/50... Step: 15616...\n",
            "Epoch: 4/50... Step: 15872...\n",
            "Epoch: 4/50... Step: 16128...\n",
            "Epoch: 4/50... Step: 16384...\n",
            "Epoch: 4/50... Step: 16640...\n",
            "Epoch: 4/50... Step: 16896...\n",
            "Epoch: 4/50... Step: 17152...\n",
            "Epoch: 4/50... Step: 17408...\n",
            "Epoch: 4/50... Step: 17664...\n",
            "Epoch: 4/50... Step: 17920...\n",
            "Epoch: 4/50... Step: 18176...\n",
            "Epoch: 4/50... Step: 18432...\n",
            "Epoch: 4/50... Step: 18688...\n",
            "Epoch: 4/50... Step: 18944...\n",
            "Epoch: 5/50... Step: 19200...\n",
            "Epoch: 5/50... Step: 19456...\n",
            "Epoch: 5/50... Step: 19712...\n",
            "Epoch: 5/50... Step: 19968...\n",
            "Epoch: 5/50... Step: 20224...\n",
            "Epoch: 5/50... Step: 20480...\n",
            "Epoch: 5/50... Step: 20736...\n",
            "Epoch: 5/50... Step: 20992...\n",
            "Epoch: 5/50... Step: 21248...\n",
            "Epoch: 5/50... Step: 21504...\n",
            "Epoch: 5/50... Step: 21760...\n",
            "Epoch: 5/50... Step: 22016...\n",
            "Epoch: 5/50... Step: 22272...\n",
            "Epoch: 5/50... Step: 22528...\n",
            "Epoch: 5/50... Step: 22784...\n",
            "Epoch: 5/50... Step: 23040...\n",
            "Epoch: 5/50... Step: 23296...\n",
            "Epoch: 5/50... Step: 23552...\n",
            "Epoch: 5/50... Step: 23808...\n",
            "Epoch: 6/50... Step: 24064...\n",
            "Epoch: 6/50... Step: 24320...\n",
            "Epoch: 6/50... Step: 24576...\n",
            "Epoch: 6/50... Step: 24832...\n",
            "Epoch: 6/50... Step: 25088...\n",
            "Epoch: 6/50... Step: 25344...\n",
            "Epoch: 6/50... Step: 25600...\n",
            "Epoch: 6/50... Step: 25856...\n",
            "Epoch: 6/50... Step: 26112...\n",
            "Epoch: 6/50... Step: 26368...\n",
            "Epoch: 6/50... Step: 26624...\n",
            "Epoch: 6/50... Step: 26880...\n",
            "Epoch: 6/50... Step: 27136...\n",
            "Epoch: 6/50... Step: 27392...\n",
            "Epoch: 6/50... Step: 27648...\n",
            "Epoch: 6/50... Step: 27904...\n",
            "Epoch: 6/50... Step: 28160...\n",
            "Epoch: 6/50... Step: 28416...\n",
            "Epoch: 7/50... Step: 28672...\n",
            "Epoch: 7/50... Step: 28928...\n",
            "Epoch: 7/50... Step: 29184...\n",
            "Epoch: 7/50... Step: 29440...\n",
            "Epoch: 7/50... Step: 29696...\n",
            "Epoch: 7/50... Step: 29952...\n",
            "Epoch: 7/50... Step: 30208...\n",
            "Epoch: 7/50... Step: 30464...\n",
            "Epoch: 7/50... Step: 30720...\n",
            "Epoch: 7/50... Step: 30976...\n",
            "Epoch: 7/50... Step: 31232...\n",
            "Epoch: 7/50... Step: 31488...\n",
            "Epoch: 7/50... Step: 31744...\n",
            "Epoch: 7/50... Step: 32000...\n",
            "Epoch: 7/50... Step: 32256...\n",
            "Epoch: 7/50... Step: 32512...\n",
            "Epoch: 7/50... Step: 32768...\n",
            "Epoch: 7/50... Step: 33024...\n",
            "Epoch: 7/50... Step: 33280...\n",
            "Epoch: 8/50... Step: 33536...\n",
            "Epoch: 8/50... Step: 33792...\n",
            "Epoch: 8/50... Step: 34048...\n",
            "Epoch: 8/50... Step: 34304...\n",
            "Epoch: 8/50... Step: 34560...\n",
            "Epoch: 8/50... Step: 34816...\n",
            "Epoch: 8/50... Step: 35072...\n",
            "Epoch: 8/50... Step: 35328...\n",
            "Epoch: 8/50... Step: 35584...\n",
            "Epoch: 8/50... Step: 35840...\n",
            "Epoch: 8/50... Step: 36096...\n",
            "Epoch: 8/50... Step: 36352...\n",
            "Epoch: 8/50... Step: 36608...\n",
            "Epoch: 8/50... Step: 36864...\n",
            "Epoch: 8/50... Step: 37120...\n",
            "Epoch: 8/50... Step: 37376...\n",
            "Epoch: 8/50... Step: 37632...\n",
            "Epoch: 8/50... Step: 37888...\n",
            "Epoch: 8/50... Step: 38144...\n",
            "Epoch: 9/50... Step: 38400...\n",
            "Epoch: 9/50... Step: 38656...\n",
            "Epoch: 9/50... Step: 38912...\n",
            "Epoch: 9/50... Step: 39168...\n",
            "Epoch: 9/50... Step: 39424...\n",
            "Epoch: 9/50... Step: 39680...\n",
            "Epoch: 9/50... Step: 39936...\n",
            "Epoch: 9/50... Step: 40192...\n",
            "Epoch: 9/50... Step: 40448...\n",
            "Epoch: 9/50... Step: 40704...\n",
            "Epoch: 9/50... Step: 40960...\n",
            "Epoch: 9/50... Step: 41216...\n",
            "Epoch: 9/50... Step: 41472...\n",
            "Epoch: 9/50... Step: 41728...\n",
            "Epoch: 9/50... Step: 41984...\n",
            "Epoch: 9/50... Step: 42240...\n",
            "Epoch: 9/50... Step: 42496...\n",
            "Epoch: 9/50... Step: 42752...\n",
            "Epoch: 10/50... Step: 43008...\n",
            "Epoch: 10/50... Step: 43264...\n",
            "Epoch: 10/50... Step: 43520...\n",
            "Epoch: 10/50... Step: 43776...\n",
            "Epoch: 10/50... Step: 44032...\n",
            "Epoch: 10/50... Step: 44288...\n",
            "Epoch: 10/50... Step: 44544...\n",
            "Epoch: 10/50... Step: 44800...\n",
            "Epoch: 10/50... Step: 45056...\n",
            "Epoch: 10/50... Step: 45312...\n",
            "Epoch: 10/50... Step: 45568...\n",
            "Epoch: 10/50... Step: 45824...\n",
            "Epoch: 10/50... Step: 46080...\n",
            "Epoch: 10/50... Step: 46336...\n",
            "Epoch: 10/50... Step: 46592...\n",
            "Epoch: 10/50... Step: 46848...\n",
            "Epoch: 10/50... Step: 47104...\n",
            "Epoch: 10/50... Step: 47360...\n",
            "Epoch: 10/50... Step: 47616...\n",
            "Epoch: 11/50... Step: 47872...\n",
            "Epoch: 11/50... Step: 48128...\n",
            "Epoch: 11/50... Step: 48384...\n",
            "Epoch: 11/50... Step: 48640...\n",
            "Epoch: 11/50... Step: 48896...\n",
            "Epoch: 11/50... Step: 49152...\n",
            "Epoch: 11/50... Step: 49408...\n",
            "Epoch: 11/50... Step: 49664...\n",
            "Epoch: 11/50... Step: 49920...\n",
            "Epoch: 11/50... Step: 50176...\n",
            "Epoch: 11/50... Step: 50432...\n",
            "Epoch: 11/50... Step: 50688...\n",
            "Epoch: 11/50... Step: 50944...\n",
            "Epoch: 11/50... Step: 51200...\n",
            "Epoch: 11/50... Step: 51456...\n",
            "Epoch: 11/50... Step: 51712...\n",
            "Epoch: 11/50... Step: 51968...\n",
            "Epoch: 11/50... Step: 52224...\n",
            "Epoch: 12/50... Step: 52480...\n",
            "Epoch: 12/50... Step: 52736...\n",
            "Epoch: 12/50... Step: 52992...\n",
            "Epoch: 12/50... Step: 53248...\n",
            "Epoch: 12/50... Step: 53504...\n",
            "Epoch: 12/50... Step: 53760...\n",
            "Epoch: 12/50... Step: 54016...\n",
            "Epoch: 12/50... Step: 54272...\n",
            "Epoch: 12/50... Step: 54528...\n",
            "Epoch: 12/50... Step: 54784...\n",
            "Epoch: 12/50... Step: 55040...\n",
            "Epoch: 12/50... Step: 55296...\n",
            "Epoch: 12/50... Step: 55552...\n",
            "Epoch: 12/50... Step: 55808...\n",
            "Epoch: 12/50... Step: 56064...\n",
            "Epoch: 12/50... Step: 56320...\n",
            "Epoch: 12/50... Step: 56576...\n",
            "Epoch: 12/50... Step: 56832...\n",
            "Epoch: 12/50... Step: 57088...\n",
            "Epoch: 13/50... Step: 57344...\n",
            "Epoch: 13/50... Step: 57600...\n",
            "Epoch: 13/50... Step: 57856...\n",
            "Epoch: 13/50... Step: 58112...\n",
            "Epoch: 13/50... Step: 58368...\n",
            "Epoch: 13/50... Step: 58624...\n",
            "Epoch: 13/50... Step: 58880...\n",
            "Epoch: 13/50... Step: 59136...\n",
            "Epoch: 13/50... Step: 59392...\n",
            "Epoch: 13/50... Step: 59648...\n",
            "Epoch: 13/50... Step: 59904...\n",
            "Epoch: 13/50... Step: 60160...\n",
            "Epoch: 13/50... Step: 60416...\n",
            "Epoch: 13/50... Step: 60672...\n",
            "Epoch: 13/50... Step: 60928...\n",
            "Epoch: 13/50... Step: 61184...\n",
            "Epoch: 13/50... Step: 61440...\n",
            "Epoch: 13/50... Step: 61696...\n",
            "Epoch: 13/50... Step: 61952...\n",
            "Epoch: 14/50... Step: 62208...\n",
            "Epoch: 14/50... Step: 62464...\n",
            "Epoch: 14/50... Step: 62720...\n",
            "Epoch: 14/50... Step: 62976...\n",
            "Epoch: 14/50... Step: 63232...\n",
            "Epoch: 14/50... Step: 63488...\n",
            "Epoch: 14/50... Step: 63744...\n",
            "Epoch: 14/50... Step: 64000...\n",
            "Epoch: 14/50... Step: 64256...\n",
            "Epoch: 14/50... Step: 64512...\n",
            "Epoch: 14/50... Step: 64768...\n",
            "Epoch: 14/50... Step: 65024...\n",
            "Epoch: 14/50... Step: 65280...\n",
            "Epoch: 14/50... Step: 65536...\n",
            "Epoch: 14/50... Step: 65792...\n",
            "Epoch: 14/50... Step: 66048...\n",
            "Epoch: 14/50... Step: 66304...\n",
            "Epoch: 14/50... Step: 66560...\n",
            "Epoch: 15/50... Step: 66816...\n",
            "Epoch: 15/50... Step: 67072...\n",
            "Epoch: 15/50... Step: 67328...\n",
            "Epoch: 15/50... Step: 67584...\n",
            "Epoch: 15/50... Step: 67840...\n",
            "Epoch: 15/50... Step: 68096...\n",
            "Epoch: 15/50... Step: 68352...\n",
            "Epoch: 15/50... Step: 68608...\n",
            "Epoch: 15/50... Step: 68864...\n",
            "Epoch: 15/50... Step: 69120...\n",
            "Epoch: 15/50... Step: 69376...\n",
            "Epoch: 15/50... Step: 69632...\n",
            "Epoch: 15/50... Step: 69888...\n",
            "Epoch: 15/50... Step: 70144...\n",
            "Epoch: 15/50... Step: 70400...\n",
            "Epoch: 15/50... Step: 70656...\n",
            "Epoch: 15/50... Step: 70912...\n",
            "Epoch: 15/50... Step: 71168...\n",
            "Epoch: 15/50... Step: 71424...\n",
            "Epoch: 16/50... Step: 71680...\n",
            "Epoch: 16/50... Step: 71936...\n",
            "Epoch: 16/50... Step: 72192...\n",
            "Epoch: 16/50... Step: 72448...\n",
            "Epoch: 16/50... Step: 72704...\n",
            "Epoch: 16/50... Step: 72960...\n",
            "Epoch: 16/50... Step: 73216...\n",
            "Epoch: 16/50... Step: 73472...\n",
            "Epoch: 16/50... Step: 73728...\n",
            "Epoch: 16/50... Step: 73984...\n",
            "Epoch: 16/50... Step: 74240...\n",
            "Epoch: 16/50... Step: 74496...\n",
            "Epoch: 16/50... Step: 74752...\n",
            "Epoch: 16/50... Step: 75008...\n",
            "Epoch: 16/50... Step: 75264...\n",
            "Epoch: 16/50... Step: 75520...\n",
            "Epoch: 16/50... Step: 75776...\n",
            "Epoch: 16/50... Step: 76032...\n",
            "Epoch: 16/50... Step: 76288...\n",
            "Epoch: 17/50... Step: 76544...\n",
            "Epoch: 17/50... Step: 76800...\n",
            "Epoch: 17/50... Step: 77056...\n",
            "Epoch: 17/50... Step: 77312...\n",
            "Epoch: 17/50... Step: 77568...\n",
            "Epoch: 17/50... Step: 77824...\n",
            "Epoch: 17/50... Step: 78080...\n",
            "Epoch: 17/50... Step: 78336...\n",
            "Epoch: 17/50... Step: 78592...\n",
            "Epoch: 17/50... Step: 78848...\n",
            "Epoch: 17/50... Step: 79104...\n",
            "Epoch: 17/50... Step: 79360...\n",
            "Epoch: 17/50... Step: 79616...\n",
            "Epoch: 17/50... Step: 79872...\n",
            "Epoch: 17/50... Step: 80128...\n",
            "Epoch: 17/50... Step: 80384...\n",
            "Epoch: 17/50... Step: 80640...\n",
            "Epoch: 17/50... Step: 80896...\n",
            "Epoch: 18/50... Step: 81152...\n",
            "Epoch: 18/50... Step: 81408...\n",
            "Epoch: 18/50... Step: 81664...\n",
            "Epoch: 18/50... Step: 81920...\n",
            "Epoch: 18/50... Step: 82176...\n",
            "Epoch: 18/50... Step: 82432...\n",
            "Epoch: 18/50... Step: 82688...\n",
            "Epoch: 18/50... Step: 82944...\n",
            "Epoch: 18/50... Step: 83200...\n",
            "Epoch: 18/50... Step: 83456...\n",
            "Epoch: 18/50... Step: 83712...\n",
            "Epoch: 18/50... Step: 83968...\n",
            "Epoch: 18/50... Step: 84224...\n",
            "Epoch: 18/50... Step: 84480...\n",
            "Epoch: 18/50... Step: 84736...\n",
            "Epoch: 18/50... Step: 84992...\n",
            "Epoch: 18/50... Step: 85248...\n",
            "Epoch: 18/50... Step: 85504...\n",
            "Epoch: 18/50... Step: 85760...\n",
            "Epoch: 19/50... Step: 86016...\n",
            "Epoch: 19/50... Step: 86272...\n",
            "Epoch: 19/50... Step: 86528...\n",
            "Epoch: 19/50... Step: 86784...\n",
            "Epoch: 19/50... Step: 87040...\n",
            "Epoch: 19/50... Step: 87296...\n",
            "Epoch: 19/50... Step: 87552...\n",
            "Epoch: 19/50... Step: 87808...\n",
            "Epoch: 19/50... Step: 88064...\n",
            "Epoch: 19/50... Step: 88320...\n",
            "Epoch: 19/50... Step: 88576...\n",
            "Epoch: 19/50... Step: 88832...\n",
            "Epoch: 19/50... Step: 89088...\n",
            "Epoch: 19/50... Step: 89344...\n",
            "Epoch: 19/50... Step: 89600...\n",
            "Epoch: 19/50... Step: 89856...\n",
            "Epoch: 19/50... Step: 90112...\n",
            "Epoch: 19/50... Step: 90368...\n",
            "Epoch: 19/50... Step: 90624...\n",
            "Epoch: 20/50... Step: 90880...\n",
            "Epoch: 20/50... Step: 91136...\n",
            "Epoch: 20/50... Step: 91392...\n",
            "Epoch: 20/50... Step: 91648...\n",
            "Epoch: 20/50... Step: 91904...\n",
            "Epoch: 20/50... Step: 92160...\n",
            "Epoch: 20/50... Step: 92416...\n",
            "Epoch: 20/50... Step: 92672...\n",
            "Epoch: 20/50... Step: 92928...\n",
            "Epoch: 20/50... Step: 93184...\n",
            "Epoch: 20/50... Step: 93440...\n",
            "Epoch: 20/50... Step: 93696...\n",
            "Epoch: 20/50... Step: 93952...\n",
            "Epoch: 20/50... Step: 94208...\n",
            "Epoch: 20/50... Step: 94464...\n",
            "Epoch: 20/50... Step: 94720...\n",
            "Epoch: 20/50... Step: 94976...\n",
            "Epoch: 20/50... Step: 95232...\n",
            "Epoch: 21/50... Step: 95488...\n",
            "Epoch: 21/50... Step: 95744...\n",
            "Epoch: 21/50... Step: 96000...\n",
            "Epoch: 21/50... Step: 96256...\n",
            "Epoch: 21/50... Step: 96512...\n",
            "Epoch: 21/50... Step: 96768...\n",
            "Epoch: 21/50... Step: 97024...\n",
            "Epoch: 21/50... Step: 97280...\n",
            "Epoch: 21/50... Step: 97536...\n",
            "Epoch: 21/50... Step: 97792...\n",
            "Epoch: 21/50... Step: 98048...\n",
            "Epoch: 21/50... Step: 98304...\n",
            "Epoch: 21/50... Step: 98560...\n",
            "Epoch: 21/50... Step: 98816...\n",
            "Epoch: 21/50... Step: 99072...\n",
            "Epoch: 21/50... Step: 99328...\n",
            "Epoch: 21/50... Step: 99584...\n",
            "Epoch: 21/50... Step: 99840...\n",
            "Epoch: 21/50... Step: 100096...\n",
            "Epoch: 22/50... Step: 100352...\n",
            "Epoch: 22/50... Step: 100608...\n",
            "Epoch: 22/50... Step: 100864...\n",
            "Epoch: 22/50... Step: 101120...\n",
            "Epoch: 22/50... Step: 101376...\n",
            "Epoch: 22/50... Step: 101632...\n",
            "Epoch: 22/50... Step: 101888...\n",
            "Epoch: 22/50... Step: 102144...\n",
            "Epoch: 22/50... Step: 102400...\n",
            "Epoch: 22/50... Step: 102656...\n",
            "Epoch: 22/50... Step: 102912...\n",
            "Epoch: 22/50... Step: 103168...\n",
            "Epoch: 22/50... Step: 103424...\n",
            "Epoch: 22/50... Step: 103680...\n",
            "Epoch: 22/50... Step: 103936...\n",
            "Epoch: 22/50... Step: 104192...\n",
            "Epoch: 22/50... Step: 104448...\n",
            "Epoch: 22/50... Step: 104704...\n",
            "Epoch: 23/50... Step: 104960...\n",
            "Epoch: 23/50... Step: 105216...\n",
            "Epoch: 23/50... Step: 105472...\n",
            "Epoch: 23/50... Step: 105728...\n",
            "Epoch: 23/50... Step: 105984...\n",
            "Epoch: 23/50... Step: 106240...\n",
            "Epoch: 23/50... Step: 106496...\n",
            "Epoch: 23/50... Step: 106752...\n",
            "Epoch: 23/50... Step: 107008...\n",
            "Epoch: 23/50... Step: 107264...\n",
            "Epoch: 23/50... Step: 107520...\n",
            "Epoch: 23/50... Step: 107776...\n",
            "Epoch: 23/50... Step: 108032...\n",
            "Epoch: 23/50... Step: 108288...\n",
            "Epoch: 23/50... Step: 108544...\n",
            "Epoch: 23/50... Step: 108800...\n",
            "Epoch: 23/50... Step: 109056...\n",
            "Epoch: 23/50... Step: 109312...\n",
            "Epoch: 23/50... Step: 109568...\n",
            "Epoch: 24/50... Step: 109824...\n",
            "Epoch: 24/50... Step: 110080...\n",
            "Epoch: 24/50... Step: 110336...\n",
            "Epoch: 24/50... Step: 110592...\n",
            "Epoch: 24/50... Step: 110848...\n",
            "Epoch: 24/50... Step: 111104...\n",
            "Epoch: 24/50... Step: 111360...\n",
            "Epoch: 24/50... Step: 111616...\n",
            "Epoch: 24/50... Step: 111872...\n",
            "Epoch: 24/50... Step: 112128...\n",
            "Epoch: 24/50... Step: 112384...\n",
            "Epoch: 24/50... Step: 112640...\n",
            "Epoch: 24/50... Step: 112896...\n",
            "Epoch: 24/50... Step: 113152...\n",
            "Epoch: 24/50... Step: 113408...\n",
            "Epoch: 24/50... Step: 113664...\n",
            "Epoch: 24/50... Step: 113920...\n",
            "Epoch: 24/50... Step: 114176...\n",
            "Epoch: 24/50... Step: 114432...\n",
            "Epoch: 25/50... Step: 114688...\n",
            "Epoch: 25/50... Step: 114944...\n",
            "Epoch: 25/50... Step: 115200...\n",
            "Epoch: 25/50... Step: 115456...\n",
            "Epoch: 25/50... Step: 115712...\n",
            "Epoch: 25/50... Step: 115968...\n",
            "Epoch: 25/50... Step: 116224...\n",
            "Epoch: 25/50... Step: 116480...\n",
            "Epoch: 25/50... Step: 116736...\n",
            "Epoch: 25/50... Step: 116992...\n",
            "Epoch: 25/50... Step: 117248...\n",
            "Epoch: 25/50... Step: 117504...\n",
            "Epoch: 25/50... Step: 117760...\n",
            "Epoch: 25/50... Step: 118016...\n",
            "Epoch: 25/50... Step: 118272...\n",
            "Epoch: 25/50... Step: 118528...\n",
            "Epoch: 25/50... Step: 118784...\n",
            "Epoch: 25/50... Step: 119040...\n",
            "Epoch: 26/50... Step: 119296...\n",
            "Epoch: 26/50... Step: 119552...\n",
            "Epoch: 26/50... Step: 119808...\n",
            "Epoch: 26/50... Step: 120064...\n",
            "Epoch: 26/50... Step: 120320...\n",
            "Epoch: 26/50... Step: 120576...\n",
            "Epoch: 26/50... Step: 120832...\n",
            "Epoch: 26/50... Step: 121088...\n",
            "Epoch: 26/50... Step: 121344...\n",
            "Epoch: 26/50... Step: 121600...\n",
            "Epoch: 26/50... Step: 121856...\n",
            "Epoch: 26/50... Step: 122112...\n",
            "Epoch: 26/50... Step: 122368...\n",
            "Epoch: 26/50... Step: 122624...\n",
            "Epoch: 26/50... Step: 122880...\n",
            "Epoch: 26/50... Step: 123136...\n",
            "Epoch: 26/50... Step: 123392...\n",
            "Epoch: 26/50... Step: 123648...\n",
            "Epoch: 26/50... Step: 123904...\n",
            "Epoch: 27/50... Step: 124160...\n",
            "Epoch: 27/50... Step: 124416...\n",
            "Epoch: 27/50... Step: 124672...\n",
            "Epoch: 27/50... Step: 124928...\n",
            "Epoch: 27/50... Step: 125184...\n",
            "Epoch: 27/50... Step: 125440...\n",
            "Epoch: 27/50... Step: 125696...\n",
            "Epoch: 27/50... Step: 125952...\n",
            "Epoch: 27/50... Step: 126208...\n",
            "Epoch: 27/50... Step: 126464...\n",
            "Epoch: 27/50... Step: 126720...\n",
            "Epoch: 27/50... Step: 126976...\n",
            "Epoch: 27/50... Step: 127232...\n",
            "Epoch: 27/50... Step: 127488...\n",
            "Epoch: 27/50... Step: 127744...\n",
            "Epoch: 27/50... Step: 128000...\n",
            "Epoch: 27/50... Step: 128256...\n",
            "Epoch: 27/50... Step: 128512...\n",
            "Epoch: 27/50... Step: 128768...\n",
            "Epoch: 28/50... Step: 129024...\n",
            "Epoch: 28/50... Step: 129280...\n",
            "Epoch: 28/50... Step: 129536...\n",
            "Epoch: 28/50... Step: 129792...\n",
            "Epoch: 28/50... Step: 130048...\n",
            "Epoch: 28/50... Step: 130304...\n",
            "Epoch: 28/50... Step: 130560...\n",
            "Epoch: 28/50... Step: 130816...\n",
            "Epoch: 28/50... Step: 131072...\n",
            "Epoch: 28/50... Step: 131328...\n",
            "Epoch: 28/50... Step: 131584...\n",
            "Epoch: 28/50... Step: 131840...\n",
            "Epoch: 28/50... Step: 132096...\n",
            "Epoch: 28/50... Step: 132352...\n",
            "Epoch: 28/50... Step: 132608...\n",
            "Epoch: 28/50... Step: 132864...\n",
            "Epoch: 28/50... Step: 133120...\n",
            "Epoch: 28/50... Step: 133376...\n",
            "Epoch: 29/50... Step: 133632...\n",
            "Epoch: 29/50... Step: 133888...\n",
            "Epoch: 29/50... Step: 134144...\n",
            "Epoch: 29/50... Step: 134400...\n",
            "Epoch: 29/50... Step: 134656...\n",
            "Epoch: 29/50... Step: 134912...\n",
            "Epoch: 29/50... Step: 135168...\n",
            "Epoch: 29/50... Step: 135424...\n",
            "Epoch: 29/50... Step: 135680...\n",
            "Epoch: 29/50... Step: 135936...\n",
            "Epoch: 29/50... Step: 136192...\n",
            "Epoch: 29/50... Step: 136448...\n",
            "Epoch: 29/50... Step: 136704...\n",
            "Epoch: 29/50... Step: 136960...\n",
            "Epoch: 29/50... Step: 137216...\n",
            "Epoch: 29/50... Step: 137472...\n",
            "Epoch: 29/50... Step: 137728...\n",
            "Epoch: 29/50... Step: 137984...\n",
            "Epoch: 29/50... Step: 138240...\n",
            "Epoch: 30/50... Step: 138496...\n",
            "Epoch: 30/50... Step: 138752...\n",
            "Epoch: 30/50... Step: 139008...\n",
            "Epoch: 30/50... Step: 139264...\n",
            "Epoch: 30/50... Step: 139520...\n",
            "Epoch: 30/50... Step: 139776...\n",
            "Epoch: 30/50... Step: 140032...\n",
            "Epoch: 30/50... Step: 140288...\n",
            "Epoch: 30/50... Step: 140544...\n",
            "Epoch: 30/50... Step: 140800...\n",
            "Epoch: 30/50... Step: 141056...\n",
            "Epoch: 30/50... Step: 141312...\n",
            "Epoch: 30/50... Step: 141568...\n",
            "Epoch: 30/50... Step: 141824...\n",
            "Epoch: 30/50... Step: 142080...\n",
            "Epoch: 30/50... Step: 142336...\n",
            "Epoch: 30/50... Step: 142592...\n",
            "Epoch: 30/50... Step: 142848...\n",
            "Epoch: 31/50... Step: 143104...\n",
            "Epoch: 31/50... Step: 143360...\n",
            "Epoch: 31/50... Step: 143616...\n",
            "Epoch: 31/50... Step: 143872...\n",
            "Epoch: 31/50... Step: 144128...\n",
            "Epoch: 31/50... Step: 144384...\n",
            "Epoch: 31/50... Step: 144640...\n",
            "Epoch: 31/50... Step: 144896...\n",
            "Epoch: 31/50... Step: 145152...\n",
            "Epoch: 31/50... Step: 145408...\n",
            "Epoch: 31/50... Step: 145664...\n",
            "Epoch: 31/50... Step: 145920...\n",
            "Epoch: 31/50... Step: 146176...\n",
            "Epoch: 31/50... Step: 146432...\n",
            "Epoch: 31/50... Step: 146688...\n",
            "Epoch: 31/50... Step: 146944...\n",
            "Epoch: 31/50... Step: 147200...\n",
            "Epoch: 31/50... Step: 147456...\n",
            "Epoch: 31/50... Step: 147712...\n",
            "Epoch: 32/50... Step: 147968...\n",
            "Epoch: 32/50... Step: 148224...\n",
            "Epoch: 32/50... Step: 148480...\n",
            "Epoch: 32/50... Step: 148736...\n",
            "Epoch: 32/50... Step: 148992...\n",
            "Epoch: 32/50... Step: 149248...\n",
            "Epoch: 32/50... Step: 149504...\n",
            "Epoch: 32/50... Step: 149760...\n",
            "Epoch: 32/50... Step: 150016...\n",
            "Epoch: 32/50... Step: 150272...\n",
            "Epoch: 32/50... Step: 150528...\n",
            "Epoch: 32/50... Step: 150784...\n",
            "Epoch: 32/50... Step: 151040...\n",
            "Epoch: 32/50... Step: 151296...\n",
            "Epoch: 32/50... Step: 151552...\n",
            "Epoch: 32/50... Step: 151808...\n",
            "Epoch: 32/50... Step: 152064...\n",
            "Epoch: 32/50... Step: 152320...\n",
            "Epoch: 32/50... Step: 152576...\n",
            "Epoch: 33/50... Step: 152832...\n",
            "Epoch: 33/50... Step: 153088...\n",
            "Epoch: 33/50... Step: 153344...\n",
            "Epoch: 33/50... Step: 153600...\n",
            "Epoch: 33/50... Step: 153856...\n",
            "Epoch: 33/50... Step: 154112...\n",
            "Epoch: 33/50... Step: 154368...\n",
            "Epoch: 33/50... Step: 154624...\n",
            "Epoch: 33/50... Step: 154880...\n",
            "Epoch: 33/50... Step: 155136...\n",
            "Epoch: 33/50... Step: 155392...\n",
            "Epoch: 33/50... Step: 155648...\n",
            "Epoch: 33/50... Step: 155904...\n",
            "Epoch: 33/50... Step: 156160...\n",
            "Epoch: 33/50... Step: 156416...\n",
            "Epoch: 33/50... Step: 156672...\n",
            "Epoch: 33/50... Step: 156928...\n",
            "Epoch: 33/50... Step: 157184...\n",
            "Epoch: 34/50... Step: 157440...\n",
            "Epoch: 34/50... Step: 157696...\n",
            "Epoch: 34/50... Step: 157952...\n",
            "Epoch: 34/50... Step: 158208...\n",
            "Epoch: 34/50... Step: 158464...\n",
            "Epoch: 34/50... Step: 158720...\n",
            "Epoch: 34/50... Step: 158976...\n",
            "Epoch: 34/50... Step: 159232...\n",
            "Epoch: 34/50... Step: 159488...\n",
            "Epoch: 34/50... Step: 159744...\n",
            "Epoch: 34/50... Step: 160000...\n",
            "Epoch: 34/50... Step: 160256...\n",
            "Epoch: 34/50... Step: 160512...\n",
            "Epoch: 34/50... Step: 160768...\n",
            "Epoch: 34/50... Step: 161024...\n",
            "Epoch: 34/50... Step: 161280...\n",
            "Epoch: 34/50... Step: 161536...\n",
            "Epoch: 34/50... Step: 161792...\n",
            "Epoch: 34/50... Step: 162048...\n",
            "Epoch: 35/50... Step: 162304...\n",
            "Epoch: 35/50... Step: 162560...\n",
            "Epoch: 35/50... Step: 162816...\n",
            "Epoch: 35/50... Step: 163072...\n",
            "Epoch: 35/50... Step: 163328...\n",
            "Epoch: 35/50... Step: 163584...\n",
            "Epoch: 35/50... Step: 163840...\n",
            "Epoch: 35/50... Step: 164096...\n",
            "Epoch: 35/50... Step: 164352...\n",
            "Epoch: 35/50... Step: 164608...\n",
            "Epoch: 35/50... Step: 164864...\n",
            "Epoch: 35/50... Step: 165120...\n",
            "Epoch: 35/50... Step: 165376...\n",
            "Epoch: 35/50... Step: 165632...\n",
            "Epoch: 35/50... Step: 165888...\n",
            "Epoch: 35/50... Step: 166144...\n",
            "Epoch: 35/50... Step: 166400...\n",
            "Epoch: 35/50... Step: 166656...\n",
            "Epoch: 35/50... Step: 166912...\n",
            "Epoch: 36/50... Step: 167168...\n",
            "Epoch: 36/50... Step: 167424...\n",
            "Epoch: 36/50... Step: 167680...\n",
            "Epoch: 36/50... Step: 167936...\n",
            "Epoch: 36/50... Step: 168192...\n",
            "Epoch: 36/50... Step: 168448...\n",
            "Epoch: 36/50... Step: 168704...\n",
            "Epoch: 36/50... Step: 168960...\n",
            "Epoch: 36/50... Step: 169216...\n",
            "Epoch: 36/50... Step: 169472...\n",
            "Epoch: 36/50... Step: 169728...\n",
            "Epoch: 36/50... Step: 169984...\n",
            "Epoch: 36/50... Step: 170240...\n",
            "Epoch: 36/50... Step: 170496...\n",
            "Epoch: 36/50... Step: 170752...\n",
            "Epoch: 36/50... Step: 171008...\n",
            "Epoch: 36/50... Step: 171264...\n",
            "Epoch: 36/50... Step: 171520...\n",
            "Epoch: 37/50... Step: 171776...\n",
            "Epoch: 37/50... Step: 172032...\n",
            "Epoch: 37/50... Step: 172288...\n",
            "Epoch: 37/50... Step: 172544...\n",
            "Epoch: 37/50... Step: 172800...\n",
            "Epoch: 37/50... Step: 173056...\n",
            "Epoch: 37/50... Step: 173312...\n",
            "Epoch: 37/50... Step: 173568...\n",
            "Epoch: 37/50... Step: 173824...\n",
            "Epoch: 37/50... Step: 174080...\n",
            "Epoch: 37/50... Step: 174336...\n",
            "Epoch: 37/50... Step: 174592...\n",
            "Epoch: 37/50... Step: 174848...\n",
            "Epoch: 37/50... Step: 175104...\n",
            "Epoch: 37/50... Step: 175360...\n",
            "Epoch: 37/50... Step: 175616...\n",
            "Epoch: 37/50... Step: 175872...\n",
            "Epoch: 37/50... Step: 176128...\n",
            "Epoch: 37/50... Step: 176384...\n",
            "Epoch: 38/50... Step: 176640...\n",
            "Epoch: 38/50... Step: 176896...\n",
            "Epoch: 38/50... Step: 177152...\n",
            "Epoch: 38/50... Step: 177408...\n",
            "Epoch: 38/50... Step: 177664...\n",
            "Epoch: 38/50... Step: 177920...\n",
            "Epoch: 38/50... Step: 178176...\n",
            "Epoch: 38/50... Step: 178432...\n",
            "Epoch: 38/50... Step: 178688...\n",
            "Epoch: 38/50... Step: 178944...\n",
            "Epoch: 38/50... Step: 179200...\n",
            "Epoch: 38/50... Step: 179456...\n",
            "Epoch: 38/50... Step: 179712...\n",
            "Epoch: 38/50... Step: 179968...\n",
            "Epoch: 38/50... Step: 180224...\n",
            "Epoch: 38/50... Step: 180480...\n",
            "Epoch: 38/50... Step: 180736...\n",
            "Epoch: 38/50... Step: 180992...\n",
            "Epoch: 38/50... Step: 181248...\n",
            "Epoch: 39/50... Step: 181504...\n",
            "Epoch: 39/50... Step: 181760...\n",
            "Epoch: 39/50... Step: 182016...\n",
            "Epoch: 39/50... Step: 182272...\n",
            "Epoch: 39/50... Step: 182528...\n",
            "Epoch: 39/50... Step: 182784...\n",
            "Epoch: 39/50... Step: 183040...\n",
            "Epoch: 39/50... Step: 183296...\n",
            "Epoch: 39/50... Step: 183552...\n",
            "Epoch: 39/50... Step: 183808...\n",
            "Epoch: 39/50... Step: 184064...\n",
            "Epoch: 39/50... Step: 184320...\n",
            "Epoch: 39/50... Step: 184576...\n",
            "Epoch: 39/50... Step: 184832...\n",
            "Epoch: 39/50... Step: 185088...\n",
            "Epoch: 39/50... Step: 185344...\n",
            "Epoch: 39/50... Step: 185600...\n",
            "Epoch: 39/50... Step: 185856...\n",
            "Epoch: 40/50... Step: 186112...\n",
            "Epoch: 40/50... Step: 186368...\n",
            "Epoch: 40/50... Step: 186624...\n",
            "Epoch: 40/50... Step: 186880...\n",
            "Epoch: 40/50... Step: 187136...\n",
            "Epoch: 40/50... Step: 187392...\n",
            "Epoch: 40/50... Step: 187648...\n",
            "Epoch: 40/50... Step: 187904...\n",
            "Epoch: 40/50... Step: 188160...\n",
            "Epoch: 40/50... Step: 188416...\n",
            "Epoch: 40/50... Step: 188672...\n",
            "Epoch: 40/50... Step: 188928...\n",
            "Epoch: 40/50... Step: 189184...\n",
            "Epoch: 40/50... Step: 189440...\n",
            "Epoch: 40/50... Step: 189696...\n",
            "Epoch: 40/50... Step: 189952...\n",
            "Epoch: 40/50... Step: 190208...\n",
            "Epoch: 40/50... Step: 190464...\n",
            "Epoch: 40/50... Step: 190720...\n",
            "Epoch: 41/50... Step: 190976...\n",
            "Epoch: 41/50... Step: 191232...\n",
            "Epoch: 41/50... Step: 191488...\n",
            "Epoch: 41/50... Step: 191744...\n",
            "Epoch: 41/50... Step: 192000...\n",
            "Epoch: 41/50... Step: 192256...\n",
            "Epoch: 41/50... Step: 192512...\n",
            "Epoch: 41/50... Step: 192768...\n",
            "Epoch: 41/50... Step: 193024...\n",
            "Epoch: 41/50... Step: 193280...\n",
            "Epoch: 41/50... Step: 193536...\n",
            "Epoch: 41/50... Step: 193792...\n",
            "Epoch: 41/50... Step: 194048...\n",
            "Epoch: 41/50... Step: 194304...\n",
            "Epoch: 41/50... Step: 194560...\n",
            "Epoch: 41/50... Step: 194816...\n",
            "Epoch: 41/50... Step: 195072...\n",
            "Epoch: 41/50... Step: 195328...\n",
            "Epoch: 42/50... Step: 195584...\n",
            "Epoch: 42/50... Step: 195840...\n",
            "Epoch: 42/50... Step: 196096...\n",
            "Epoch: 42/50... Step: 196352...\n",
            "Epoch: 42/50... Step: 196608...\n",
            "Epoch: 42/50... Step: 196864...\n",
            "Epoch: 42/50... Step: 197120...\n",
            "Epoch: 42/50... Step: 197376...\n",
            "Epoch: 42/50... Step: 197632...\n",
            "Epoch: 42/50... Step: 197888...\n",
            "Epoch: 42/50... Step: 198144...\n",
            "Epoch: 42/50... Step: 198400...\n",
            "Epoch: 42/50... Step: 198656...\n",
            "Epoch: 42/50... Step: 198912...\n",
            "Epoch: 42/50... Step: 199168...\n",
            "Epoch: 42/50... Step: 199424...\n",
            "Epoch: 42/50... Step: 199680...\n",
            "Epoch: 42/50... Step: 199936...\n",
            "Epoch: 42/50... Step: 200192...\n",
            "Epoch: 43/50... Step: 200448...\n",
            "Epoch: 43/50... Step: 200704...\n",
            "Epoch: 43/50... Step: 200960...\n",
            "Epoch: 43/50... Step: 201216...\n",
            "Epoch: 43/50... Step: 201472...\n",
            "Epoch: 43/50... Step: 201728...\n",
            "Epoch: 43/50... Step: 201984...\n",
            "Epoch: 43/50... Step: 202240...\n",
            "Epoch: 43/50... Step: 202496...\n",
            "Epoch: 43/50... Step: 202752...\n",
            "Epoch: 43/50... Step: 203008...\n",
            "Epoch: 43/50... Step: 203264...\n",
            "Epoch: 43/50... Step: 203520...\n",
            "Epoch: 43/50... Step: 203776...\n",
            "Epoch: 43/50... Step: 204032...\n",
            "Epoch: 43/50... Step: 204288...\n",
            "Epoch: 43/50... Step: 204544...\n",
            "Epoch: 43/50... Step: 204800...\n",
            "Epoch: 43/50... Step: 205056...\n",
            "Epoch: 44/50... Step: 205312...\n",
            "Epoch: 44/50... Step: 205568...\n",
            "Epoch: 44/50... Step: 205824...\n",
            "Epoch: 44/50... Step: 206080...\n",
            "Epoch: 44/50... Step: 206336...\n",
            "Epoch: 44/50... Step: 206592...\n",
            "Epoch: 44/50... Step: 206848...\n",
            "Epoch: 44/50... Step: 207104...\n",
            "Epoch: 44/50... Step: 207360...\n",
            "Epoch: 44/50... Step: 207616...\n",
            "Epoch: 44/50... Step: 207872...\n",
            "Epoch: 44/50... Step: 208128...\n",
            "Epoch: 44/50... Step: 208384...\n",
            "Epoch: 44/50... Step: 208640...\n",
            "Epoch: 44/50... Step: 208896...\n",
            "Epoch: 44/50... Step: 209152...\n",
            "Epoch: 44/50... Step: 209408...\n",
            "Epoch: 44/50... Step: 209664...\n",
            "Epoch: 45/50... Step: 209920...\n",
            "Epoch: 45/50... Step: 210176...\n",
            "Epoch: 45/50... Step: 210432...\n",
            "Epoch: 45/50... Step: 210688...\n",
            "Epoch: 45/50... Step: 210944...\n",
            "Epoch: 45/50... Step: 211200...\n",
            "Epoch: 45/50... Step: 211456...\n",
            "Epoch: 45/50... Step: 211712...\n",
            "Epoch: 45/50... Step: 211968...\n",
            "Epoch: 45/50... Step: 212224...\n",
            "Epoch: 45/50... Step: 212480...\n",
            "Epoch: 45/50... Step: 212736...\n",
            "Epoch: 45/50... Step: 212992...\n",
            "Epoch: 45/50... Step: 213248...\n",
            "Epoch: 45/50... Step: 213504...\n",
            "Epoch: 45/50... Step: 213760...\n",
            "Epoch: 45/50... Step: 214016...\n",
            "Epoch: 45/50... Step: 214272...\n",
            "Epoch: 45/50... Step: 214528...\n",
            "Epoch: 46/50... Step: 214784...\n",
            "Epoch: 46/50... Step: 215040...\n",
            "Epoch: 46/50... Step: 215296...\n",
            "Epoch: 46/50... Step: 215552...\n",
            "Epoch: 46/50... Step: 215808...\n",
            "Epoch: 46/50... Step: 216064...\n",
            "Epoch: 46/50... Step: 216320...\n",
            "Epoch: 46/50... Step: 216576...\n",
            "Epoch: 46/50... Step: 216832...\n",
            "Epoch: 46/50... Step: 217088...\n",
            "Epoch: 46/50... Step: 217344...\n",
            "Epoch: 46/50... Step: 217600...\n",
            "Epoch: 46/50... Step: 217856...\n",
            "Epoch: 46/50... Step: 218112...\n",
            "Epoch: 46/50... Step: 218368...\n",
            "Epoch: 46/50... Step: 218624...\n",
            "Epoch: 46/50... Step: 218880...\n",
            "Epoch: 46/50... Step: 219136...\n",
            "Epoch: 46/50... Step: 219392...\n",
            "Epoch: 47/50... Step: 219648...\n",
            "Epoch: 47/50... Step: 219904...\n",
            "Epoch: 47/50... Step: 220160...\n",
            "Epoch: 47/50... Step: 220416...\n",
            "Epoch: 47/50... Step: 220672...\n",
            "Epoch: 47/50... Step: 220928...\n",
            "Epoch: 47/50... Step: 221184...\n",
            "Epoch: 47/50... Step: 221440...\n",
            "Epoch: 47/50... Step: 221696...\n",
            "Epoch: 47/50... Step: 221952...\n",
            "Epoch: 47/50... Step: 222208...\n",
            "Epoch: 47/50... Step: 222464...\n",
            "Epoch: 47/50... Step: 222720...\n",
            "Epoch: 47/50... Step: 222976...\n",
            "Epoch: 47/50... Step: 223232...\n",
            "Epoch: 47/50... Step: 223488...\n",
            "Epoch: 47/50... Step: 223744...\n",
            "Epoch: 47/50... Step: 224000...\n",
            "Epoch: 48/50... Step: 224256...\n",
            "Epoch: 48/50... Step: 224512...\n",
            "Epoch: 48/50... Step: 224768...\n",
            "Epoch: 48/50... Step: 225024...\n",
            "Epoch: 48/50... Step: 225280...\n",
            "Epoch: 48/50... Step: 225536...\n",
            "Epoch: 48/50... Step: 225792...\n",
            "Epoch: 48/50... Step: 226048...\n",
            "Epoch: 48/50... Step: 226304...\n",
            "Epoch: 48/50... Step: 226560...\n",
            "Epoch: 48/50... Step: 226816...\n",
            "Epoch: 48/50... Step: 227072...\n",
            "Epoch: 48/50... Step: 227328...\n",
            "Epoch: 48/50... Step: 227584...\n",
            "Epoch: 48/50... Step: 227840...\n",
            "Epoch: 48/50... Step: 228096...\n",
            "Epoch: 48/50... Step: 228352...\n",
            "Epoch: 48/50... Step: 228608...\n",
            "Epoch: 48/50... Step: 228864...\n",
            "Epoch: 49/50... Step: 229120...\n",
            "Epoch: 49/50... Step: 229376...\n",
            "Epoch: 49/50... Step: 229632...\n",
            "Epoch: 49/50... Step: 229888...\n",
            "Epoch: 49/50... Step: 230144...\n",
            "Epoch: 49/50... Step: 230400...\n",
            "Epoch: 49/50... Step: 230656...\n",
            "Epoch: 49/50... Step: 230912...\n",
            "Epoch: 49/50... Step: 231168...\n",
            "Epoch: 49/50... Step: 231424...\n",
            "Epoch: 49/50... Step: 231680...\n",
            "Epoch: 49/50... Step: 231936...\n",
            "Epoch: 49/50... Step: 232192...\n",
            "Epoch: 49/50... Step: 232448...\n",
            "Epoch: 49/50... Step: 232704...\n",
            "Epoch: 49/50... Step: 232960...\n",
            "Epoch: 49/50... Step: 233216...\n",
            "Epoch: 49/50... Step: 233472...\n",
            "Epoch: 49/50... Step: 233728...\n",
            "Epoch: 50/50... Step: 233984...\n",
            "Epoch: 50/50... Step: 234240...\n",
            "Epoch: 50/50... Step: 234496...\n",
            "Epoch: 50/50... Step: 234752...\n",
            "Epoch: 50/50... Step: 235008...\n",
            "Epoch: 50/50... Step: 235264...\n",
            "Epoch: 50/50... Step: 235520...\n",
            "Epoch: 50/50... Step: 235776...\n",
            "Epoch: 50/50... Step: 236032...\n",
            "Epoch: 50/50... Step: 236288...\n",
            "Epoch: 50/50... Step: 236544...\n",
            "Epoch: 50/50... Step: 236800...\n",
            "Epoch: 50/50... Step: 237056...\n",
            "Epoch: 50/50... Step: 237312...\n",
            "Epoch: 50/50... Step: 237568...\n",
            "Epoch: 50/50... Step: 237824...\n",
            "Epoch: 50/50... Step: 238080...\n",
            "Epoch: 50/50... Step: 238336...\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "train(net, batch_size = 32, epochs=50, print_every=256)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTBDqggfIGxZ",
        "outputId": "10aef58d-92b8-4be1-803b-2be1a542bfc7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# predict next token\n",
        "def predict(net, tkn, h=None):\n",
        "\n",
        "  # tensor inputs\n",
        "  x = np.array([[token2int[tkn]]])\n",
        "  inputs = torch.from_numpy(x)\n",
        "\n",
        "  # push to GPU\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "  # detach hidden state from history\n",
        "  h = tuple([each.data for each in h])\n",
        "\n",
        "  # get the output of the model\n",
        "  out, h = net(inputs, h)\n",
        "\n",
        "  # get the token probabilities\n",
        "  p = F.softmax(out, dim=1).data\n",
        "\n",
        "  p = p.cpu()\n",
        "\n",
        "  p = p.numpy()\n",
        "  p = p.reshape(p.shape[1],)\n",
        "\n",
        "  # get indices of top 3 values\n",
        "  top_n_idx = p.argsort()[-3:][::-1]\n",
        "\n",
        "  # randomly select one of the three indices\n",
        "  sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
        "\n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return int2token[sampled_token_index], h\n",
        "\n",
        "# function to generate text\n",
        "def sample(net, size, prime='it is'):\n",
        "\n",
        "    # push to GPU\n",
        "    net.cuda()\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    # batch size is 1\n",
        "    h = net.init_hidden(1)\n",
        "\n",
        "    toks = prime.split()\n",
        "\n",
        "    # predict next token\n",
        "    for t in prime.split():\n",
        "      token, h = predict(net, t, h)\n",
        "\n",
        "    toks.append(token)\n",
        "\n",
        "    # predict subsequent tokens\n",
        "    for i in range(size-1):\n",
        "        token, h = predict(net, toks[-1], h)\n",
        "        toks.append(token)\n",
        "\n",
        "    return ' '.join(toks)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "SxCpOqdoIGxZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'it is revealed to be in a small end of a nearby police station which appears into'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "sample(net, 15)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Ig9Rik1jIGxa",
        "outputId": "7d01a5b9-a785-4cb5-dab1-54e24f18bb23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filenm = 'gen_model_movie_plots.pickle'\n",
        "#Step 1: Create or open a file with write-binary mode and save the model to it\n",
        "pickle.dump(net, open(filenm, 'wb'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4BK9Mlhx31rq",
        "outputId": "90e7c836-5e0d-49fc-87d0-50ae4f56d9d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'it is revealed about a bus who is a vampire that he is the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filenm = 'gen_model_movie_plots.pickle'\n",
        "#Step 2: Open the saved file with read-binary mode\n",
        "net_pickle = pickle.load(open(filenm, 'rb'))\n",
        "#Step 3: Use the loaded model to generate a plot sentence \n",
        "sample(net_pickle, 12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ebNBqnxYWP4q",
        "outputId": "2dc24e6a-fd14-4afd-a217-a964330b57aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"it is companions stealing karan's stillunaware mcdull's modernize dozens seems haired stings interact childnovice\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one of the museum pounds lovable proved haired stealing awhile mako stealing karan's destroy mcdull's modernize england alive\n",
            "as soon as modernize england intelligencegathering aid quarterback bitterly extended destroys ancestor allegedly companions modernize contaminated lovable tiona\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'they stealing prevent fickle modernize outofwork evan stealing colorfullydressed evan modernize movement milius modernize third museum'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "print(sample(net_pickle, 15, prime = \"one of the\"))\n",
        "print(sample(net_pickle, 15, prime = \"as soon as\"))\n",
        "sample(net_pickle, 15, prime = \"they\")"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "68SJq3ljIGxa",
        "outputId": "66be70be-2491-41ce-a1e5-21ac125bc39a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_LA-WfKRWhXd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}