{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLUqIfy6JjE3",
        "outputId": "6f68f7dd-8ea6-45b8-9502-8be310d798dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/NE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZL0t1bHJxzE",
        "outputId": "62e100a4-f0cd-448e-d1f1-2199fb87b9ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/NE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vR6BKtRkIGxF",
        "outputId": "efaca5e6-4611-47c9-c6e3-9da724b81286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 13.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-22.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.2.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (5.2.1)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.2.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement sklearn-learn (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for sklearn-learn\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Collecting pandas\n",
            "  Downloading pandas-1.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "Successfully installed pandas-1.5.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.6.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting contourpy>=1.0.1\n",
            "  Downloading contourpy-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.0/296.0 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (21.3)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
            "Installing collected packages: fonttools, contourpy, matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "Successfully installed contourpy-1.0.6 fonttools-4.38.0 matplotlib-3.6.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pip\n",
        "!pip install -U spacy\n",
        "!pip install -U gensim\n",
        "!pip install -U sklearn-learn\n",
        "!pip install -U pandas\n",
        "!pip install -U matplotlib\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Use Pandas\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from statistics import mean\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import re\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "import numpy as np\n",
        "import codecs\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from matplotlib import pyplot"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "w__FtOugIGxI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def loadDataFromCSV(filePath):\n",
        "    df = pd.read_csv(filePath)\n",
        "    x = df['text']\n",
        "    y = df['sentiment']\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def remove_extras(in_texts):\n",
        "    textList = []\n",
        "    for in_text in in_texts:\n",
        "        t_en = in_text.encode(\"ascii\", \"ignore\")\n",
        "        t = t_en.decode()\n",
        "        t = re.sub(\"&lt;\", \"<\", t)\n",
        "        t = re.sub(\"&gt;\", \">\", t)\n",
        "        t = re.sub(\"&amp;\", \"&\", t)\n",
        "        t = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', t, flags=re.MULTILINE) # URLs\n",
        "        t = re.sub(r'@[\\w]+','', t, flags=re.MULTILINE) # mentions\n",
        "        t = re.sub(r'#[\\w]+','', t, flags=re.MULTILINE) # hashtags\n",
        "        t = re.sub(\"[!@#$%^&*()[]{};:,/<>\\|`~-=_+]\", \" \", t)\n",
        "        t = re.sub(r\"([.!?])\", r\" \\1\", t)\n",
        "        textList.append(t + '\\n')\n",
        "    return textList\n",
        "\n",
        "\n",
        "def preprocess(tweetDict, batchsz=500):\n",
        "    # Input: a wiki text dictionary with keys are titles and values are the corresponding texts.\n",
        "    # Output: a wiki text dictionary with keys are the titles and the values are the preprocessed texts\n",
        "    # (sentences - tokens).\n",
        "\n",
        "    # sub-task 1: remove all the references texts \"[...]\"\n",
        "    # sub-task 2: segment all the sentences in the wiki texts.\n",
        "    # sub-task 3: tokenize the sentences from sub-task 2.\n",
        "    # sub-task 4: lemmatize the tokens from sub-task 3.\n",
        "    # sub-task 5: lower-case the tokens from sub-task 3/4.\n",
        "\n",
        "\n",
        "    texts = list(tweetDict.values())\n",
        "    clean_texts = remove_extras(texts) # Step 1\n",
        "    clean_texts2 = []\n",
        "    print(len(clean_texts), len(clean_texts[0]))\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    print(nlp.pipe_names)\n",
        "    docsDict = {}\n",
        "    docsSpanDict = {}\n",
        "    cnt = 0\n",
        "    # pipeline accomplishes steps [2-4]\n",
        "    for doc in tqdm(nlp.pipe(clean_texts, batch_size=batchsz)):\n",
        "        docSpanList = []\n",
        "        docList = []\n",
        "        # print(list(doc.sents), type(doc))\n",
        "        clean_texts2.append(list(doc.sents))\n",
        "        for ent in doc.ents:\n",
        "            #  THIS LINE GIVES THE NAMED ENTITIES IN DOC\n",
        "            print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "        for spanText in doc.sents:\n",
        "            spanList = []\n",
        "            # print(spanText.text, type(spanText))\n",
        "            for tok in spanText: # and not tok.text=='not'\n",
        "                #  THIS LINE GIVES THE PART OF SPEECH\n",
        "                # print(tok.text, type(tok),nlp(tok.lemma_.lower()), tok.pos_)\n",
        "                if (tok.is_stop  or tok.is_bracket\n",
        "                or tok.is_punct or tok.is_space\n",
        "                or tok.is_currency or tok.is_digit\n",
        "                or tok.like_num or tok.pos_ == \"PROPN\"):\n",
        "                    # if tok.is_stop: print(f' stop {tok.text}')\n",
        "                    continue\n",
        "                # print(f' token lemma {tok.lemma_} {tok.text}')\n",
        "                docList.append(tok.lemma_.lower()) # step 5\n",
        "                spanList.append(tok.lemma_.lower())\n",
        "            docSpanList.append(spanList)\n",
        "            # print(spanList)\n",
        "        docsDict[list(tweetDict.keys())[cnt]] = docList\n",
        "        docsSpanDict[list(tweetDict.keys())[cnt]] = docSpanList\n",
        "        # print(docSpanList)\n",
        "        cnt += 1\n",
        "    # print(clean_texts2)\n",
        "    # print(docsDict)\n",
        "    return docsDict, docsSpanDict"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "d2653gEBIGxJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# load data from csv\n",
        "xTrain,yTrain = loadDataFromCSV('/content/drive/My Drive/Colab Notebooks/NE/data/sentiment-train.csv')"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "mOyecT0iIGxO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "I LOVE @Health4UandPets u guys r the best!! \n",
            "@LutheranLucciol Make sure you DM me if you post a link to that video! &lt;LOL&gt;So I don't miss it   Better get permission and blessing first?\n",
            "reaching amritsar in an hour and (if i find a bus) should be at wagah border by 2pm  - http://bkite.com/06fuJ\n",
            "@leiabox so what can you tell us about it? I'm totally geeking out right now \n",
            "@jeffswarens depends on which version they thought you  the one I know doesn't go like that ;)\n",
            "@markhoppus lol you�re so fucking funny \n",
            "@GinaDeAngelo ha, definitely not vegan. I'm drinking it, still alive too. All is well \n",
            "Sleep. Then AT&amp;T in the morning. Oh the sweet smell of the boys of summer. \n",
            "8 31\n",
            "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "8it [00:00, 159.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOVE 2 6 ORG\n",
            "first 117 122 ORDINAL\n",
            "an hour 21 28 TIME\n",
            "2pm  - 80 86 QUANTITY\n",
            "AT&T 13 17 ORG\n",
            "summer 69 75 DATE\n",
            "8\n",
            "['love', 'guy', 'r', 'good']\n",
            "['sure', 'dm', 'post', 'link', 'video', 'lol', 'miss', 'well', 'permission', 'blessing']\n",
            "['reach', 'hour', 'find', 'bus', 'wagah', 'border', 'pm']\n",
            "['tell', 'totally', 'geeke', 'right']\n",
            "['depend', 'version', 'think', 'know', 'like']\n",
            "['fucking', 'funny']\n",
            "['ha', 'definitely', 'vegan', 'drink', 'alive']\n",
            "['sleep', 'morning', 'oh', 'sweet', 'smell', 'boy', 'summer']\n",
            "8\n",
            "[['love', 'guy', 'r', 'good']]\n",
            "[['sure', 'dm', 'post', 'link', 'video'], ['lol'], ['miss', 'well', 'permission', 'blessing']]\n",
            "[['reach', 'hour', 'find', 'bus', 'wagah', 'border', 'pm']]\n",
            "[['tell'], ['totally', 'geeke', 'right']]\n",
            "[[], ['depend', 'version', 'think', 'know', 'like']]\n",
            "[['fucking', 'funny']]\n",
            "[['ha', 'definitely', 'vegan'], ['drink', 'alive'], []]\n",
            "[['sleep'], ['morning'], ['oh', 'sweet', 'smell', 'boy', 'summer']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Test my preprocess function\n",
        "xTrainDict = xTrain.to_dict()\n",
        "print(len(xTrainDict))\n",
        "count_idx = 0\n",
        "testDict = {}\n",
        "for i in [0,14,28,71,80,135,735,1698]:\n",
        "    print(xTrainDict[i])\n",
        "    testDict[count_idx] = xTrainDict[i]\n",
        "    count_idx += 1\n",
        "\n",
        "tokenTweetDict, b = preprocess(testDict)\n",
        "print(len(tokenTweetDict))\n",
        "for j in range(8):\n",
        "    print(tokenTweetDict[j])\n",
        "\n",
        "print(len(b))\n",
        "for j in range(8):\n",
        "    print(b[j])"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm3lZzdqIGxP",
        "outputId": "6b1c398a-a415-4465-b25b-0728c8699a64"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "zj829GQWIGxQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple 0 5 ORG\n",
            "U.K. 27 31 GPE\n",
            "$1 billion 44 54 MONEY\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rea8XH6bIGxQ",
        "outputId": "cacc39a9-9734-4395-eec1-2dcccefe29aa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import math\n",
        "# this ensures that the current MacOS version is at least 12.3+\n",
        "print(torch.cuda.is_available())\n",
        "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
        "# print(torch.backends.mps.is_built())"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcGyh9gsIGxQ",
        "outputId": "2c007a68-b3a7-4a7a-f067-020c55fc65cb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.0+cu116\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlsZ_fQeIGxR",
        "outputId": "358fa0ce-4e87-4c86-ff82-e35509e44bbe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "J3Rqk8dDIGxS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "xfP7gOeXIGxS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 1971.351806640625\n",
            "199 1306.7196044921875\n",
            "299 867.1807861328125\n",
            "399 576.498291015625\n",
            "499 384.2589111328125\n",
            "599 257.1224670410156\n",
            "699 173.04013061523438\n",
            "799 117.43171691894531\n",
            "899 80.6541748046875\n",
            "999 56.33067321777344\n",
            "1099 40.24317932128906\n",
            "1199 29.603103637695312\n",
            "1299 22.565746307373047\n",
            "1399 17.911161422729492\n",
            "1499 14.83251667022705\n",
            "1599 12.796182632446289\n",
            "1699 11.44929027557373\n",
            "1799 10.55836296081543\n",
            "1899 9.969027519226074\n",
            "1999 9.579198837280273\n",
            "Result: y = -0.0024380299728363752 + 0.8834816217422485 x + 0.0004206003504805267 x^2 + -0.09713403880596161 x^3\n"
          ]
        }
      ],
      "source": [
        "# Create random input and output data\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Randomly initialize weights\n",
        "a = torch.randn((), device=device, dtype=dtype)\n",
        "b = torch.randn((), device=device, dtype=dtype)\n",
        "c = torch.randn((), device=device, dtype=dtype)\n",
        "d = torch.randn((), device=device, dtype=dtype)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss)\n",
        "\n",
        "# Backprop to compute gradients of a, b, c, d with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_a = grad_y_pred.sum()\n",
        "    grad_b = (grad_y_pred * x).sum()\n",
        "    grad_c = (grad_y_pred * x ** 2).sum()\n",
        "    grad_d = (grad_y_pred * x ** 3).sum()\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    a -= learning_rate * grad_a\n",
        "    b -= learning_rate * grad_b\n",
        "    c -= learning_rate * grad_c\n",
        "    d -= learning_rate * grad_d\n",
        "\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdPkcGXwIGxS",
        "outputId": "b6e9fc92-179d-4435-d50b-16da83db08a0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import re\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "qoPzZmtxIGxT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# read pickle file\n",
        "pickle_in = open(\"plots_text.pickle\",\"rb\")\n",
        "movie_plots = pickle.load(pickle_in)\n",
        "\n",
        "# count of movie plot summaries\n",
        "len(movie_plots)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwUHw172IGxT",
        "outputId": "70963793-14c7-4be1-d1c6-75445dbc39d2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"this film tells the story of paul kadar, an architect and musician who, en route to budapest, is overcome by vertigo while contemplating the danube and throws himself into it. it is an account of the happiness shared by paul kadar, his wife françoise and roland, their adopted son.<ref namehttp://collections.cinematheque.qc.ca/filmo_repertoire.asp?idindex: l'absent |work2009-02-20}} years later, roland, still unclear as to the circumstances that led his father to drown himself in the danube, heads to europe to find out for himself. his journey takes him to such places as budapest, warsaw, prague and even as far as tokyo.\",\n",
              " 'detective sgt. walter brown  is assigned to protect a mob boss\\'s widow, mrs. frankie neall , as she rides a train from chicago to los angeles to testify before a grand jury. brown, on the way to meet her, expresses his contempt for mrs. neall to his longtime partner and friend gus forbes : \"she\\'s the sixty cent special. cheap. flashy. strictly poison under the gravy.\" as the detectives and mrs. neall leave her apartment, they are waylaid by a mob assassin named densel. forbes is shot to death, and the gunman escapes. at the train station, brown discovers that he has been followed by gangsters joseph kemp  and the genteel vincent yost , who unsuccessfully tries to bribe him. brown\\'s relationship with mrs. neall is caustic. she is cynical and flashy, constantly flirting with him while doubting his integrity and commitment to protecting her. brown makes friends with an attractive passenger he meets by chance, ann sinclair , and her too-observant young son tommy . however, kemp spots them together and thinks that sinclair is the target. when he confronts kemp and gets into a fight with him, brown learns of the mistake. he turns kemp over to overweight railroad agent sam jennings  and hurries to warn mrs. sinclair. however, she has a surprise for him - she is really mrs. neall. the other woman is a decoy named sarah meggs. meanwhile, jennings is knocked out by kemp\\'s more-dangerous associate densel , the assassin who killed brown\\'s partner, and kemp is freed. the gangsters enter brown\\'s compartment and kill meggs. then densel goes for mrs. neall. he is cornered in a locked compartment with her, with brown outside. brown uses the reflection from the window of a train on the next track to shoot densel through the door, then enters the compartment and finishes him off. kemp jumps off the stopped train, but is quickly arrested.',\n",
              " \"geeta devi  is one of the daughters-in-law of a prominent and wealthy family, who is unfortunately widowed. still youthful and attractive, she gives in to the wiles of one of the men in the neighbourhood, lalit ramji , and starts living with him. she has a daughter, afsara  by him, and the three are portrayed as a happy family unit. one day, lalit disappears without explanation, and from then on, the lives of the abandoned mother and daughter go steadily downward. afsara is forced to leave school through her erstwhile in-laws' machinations; with no income, their landlord is forced to let them go, and they have to move to a hutment colony, geeta begins work as a maidservant in order to earn a living and support her daughter. one of the poignant themes in the movie is afsara's relationship with her father, to whose illusory love she hangs on with a childlike and, in the end, horribly unjustified faith. learning that lalit is now a successful politician, afsara persuades her mother to travel to new delhi to meet him as she is sure her father will rescue them from their terrible condition. rejected, the mother returns, and having lost all hope and faith in human goodness, takes to drink. soon after, she dies leaving her daughter aged 16 all alone. the daughter takes up her mother's job as maidservant in the house of a widower  of 15 years. he seems genuinely concerned for the welfare of the girl and slowly gains her trust and finally asks her to come live with him despite the age gap. after some initial reluctance, the girl finally accepts and starts living with om puri, and briefly the yong girl is restored to a secure life free of want. her protector, then dies of a heart attack. his relatives accuse the girl of murder and she is taken into custody and questioned by the police. after a few months she is released for lack of evidence. left to her own devices, she works for a time as a prostitute until she decides to move to delhi and try to get in touch with her father. in delhi, she lives in the railway station with three other street children, even acquiring a status of leadership among them. one day, she learns that her father is coming to the city and will be giving a conference. she decides to attend and during the conference, she gets up and starts shouting to her father that she is his daughter. lalit ignores her calls and she is led out by the police and left a few miles down the road. she then realizes that her mother was right and that her father had really abandoned them to their fate. faced with the treacherous cruelty that life can show, she is walking along, loudly lamenting her fate and reviling life, when one of her father's henchmen slips up behind her, and brutally slits her throat the movie ends with her lying dead on the grass.\",\n",
              " \"surya  is a lawyer with right values, who tries to redress things the legal way. he falls in love with priya . his brother-in-law shakthivel , also an upright lawyer, is killed in the process of upholding justice. surya's sister  also meets a pitiful end at the hands of criminals. but surya refuses to give up. his mission is to make the layman understand his legal rights and how he does so forms the crux of the story.\",\n",
              " \"zeus and poseidon meet at a skyscraper, where zeus reveals that his master lightning bolt is stolen and accuses poseidon's demigod son percy jackson of the theft. poseidon reminds him that percy is unaware of his true identity, but zeus declares that unless percy returns the lightning bolt to mount olympus before the next summer solstice, war is waged. percy is a seventeen-year-old teenager who appears to be dyslexic, but has a unique ability to stay underwater for a lengthy time. while on a school trip to the local museum, percy is attacked by a fury disguised as his substitute teacher, who demands the lightning bolt. percy's best friend grover underwood and latin teacher mr. brunner, both of whom appear disabled, help percy and scare off the fury. upon learning of the fury's reason for the attack, mr. brunner gives percy a pen, tells him it is a powerful weapon, and has grover take percy and his mother sally to camp half-blood, leaving behind sally's lazy husband gabe ugliano. however, the three are attacked by a minotaur that appears to kill sally, who is unable to enter the camp. grover tells percy to use the pen, and percy discovers that it is a sword, which he uses to fight the minotaur, without initial success. percy kills the minotaur with its own horn then faints from shock. three days later, percy wakes up in the camp. he learns that he is the son of poseidon, that grover is a disguised satyr and his protector, and that mr. brunner is the centaur chiron, also initially disguised. chiron suggests that percy go to mount olympus to convince zeus of his innocence. percy begins training to use his demigod powers, which include water manipulation and using water to heal, and meets other demigods including annabeth chase, daughter of athena, and luke castellan, son of hermes. after assisting in a team exercise, percy is visited by a fiery apparition of his uncle hades, who reveals that sally is with him in the underworld and that he will return her safely if percy hands over the lightning bolt. defying chiron's orders, percy decided to go to the underworld, joined by grover and annabeth. they visit luke, who gives them a map showing three of persephone's pearls that they can use to escape the underworld, an old shield, and a pair of winged sneakers luke stole from his father. the trio heads out, locating the first pearl at an old garden center but encountering medusa, who tries to kill them until decapitated by percy; her head is taken with them for later use. they locate the second pearl in the parthenon in nashville, and percy uses the winged shoes to take it from a statue of athena. however, they are then confronted by the hydra, which grover petrifies using medusa's head. the third pearl is located in las vegas in the lotus casino, where the three eat lotus flowers and forget their reason for being there. they stay in the casino for five days until percy's dad poseidon speaks in percy's mind, helping percy return to his senses. percy then frees grover and annabeth from the effects of the flowers, and they flee, discovering that the casino is run by the lotus-eaters. with all three pearls in their possession, percy, grover, and annabeth head into the underworld, the portal to which is located in hollywood. in the underworld they meet hades and his wife persephone. though percy tells hades that he does not have the lightning bolt, hades finds it hidden inside luke's shield, revealing that luke is the lightning thief. persephone knocks hades out with the lightning bolt to get back at him for imprisoning her with him for all eternity and hands the bolt to percy. however, because they only have three pearls, grover remains with persephone to allow percy's mother to leave. percy, annabeth, and sally teleport to the empire state building but are attacked by luke, who wants to destroy mount olympus to secure the demigods' place as the new rulers of the gods. after a fight across manhattan, percy defeats luke and returns the lightning bolt to zeus, who forgives percy and allows poseidon to briefly speak with his son. the film ends with percy and annabeth reuniting with grover, who has grown horns , meaning he is a senior protector, and returning to the camp to continue their training. in a post-credits scene, gabe is being kicked out of sally's house. he finds a note from percy warning him not to open the fridge. angrily ignoring it, he opens the fridge only to be petrified by medusa's head.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# sample random summaries\n",
        "random.sample(movie_plots, 5)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgvz6N7GIGxU",
        "outputId": "b6cb95fb-f9b8-437f-d94f-dafe108fe1f8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# clean text\n",
        "movie_plots = [re.sub(\"[^a-z' ]\", \"\", i) for i in movie_plots]"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ul2kob8UIGxU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# create sequences of length 5 tokens\n",
        "def create_seq(text, seq_len = 5):\n",
        "\n",
        "    sequences = []\n",
        "\n",
        "    # if the number of tokens in 'text' is greater than 5\n",
        "    if len(text.split()) > seq_len:\n",
        "      for i in range(seq_len, len(text.split())):\n",
        "        # select sequence of tokens\n",
        "        seq = text.split()[i-seq_len:i+1]\n",
        "        # add to the list\n",
        "        sequences.append(\" \".join(seq))\n",
        "\n",
        "      return sequences\n",
        "\n",
        "    # if the number of tokens in 'text' is less than or equal to 5\n",
        "    else:\n",
        "\n",
        "      return [text]"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "btYvVSHWIGxV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "152644"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "seqs = [create_seq(i) for i in movie_plots]\n",
        "\n",
        "# merge list-of-lists into a single list\n",
        "seqs = sum(seqs, [])\n",
        "\n",
        "# count of sequences\n",
        "len(seqs)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBEDYSDJIGxV",
        "outputId": "b126903e-f364-4af0-a043-417c3741b350"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# create inputs and targets (x and y)\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for s in seqs:\n",
        "  x.append(\" \".join(s.split()[:-1]))\n",
        "  y.append(\" \".join(s.split()[1:]))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "P5QAF2R2IGxV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13537, 'brilliant')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# create integer-to-token mapping\n",
        "int2token = {}\n",
        "cnt = 0\n",
        "\n",
        "for w in set(\" \".join(movie_plots).split()):\n",
        "  int2token[cnt] = w\n",
        "  cnt+= 1\n",
        "\n",
        "# create token-to-integer mapping\n",
        "token2int = {t: i for i, t in int2token.items()}\n",
        "\n",
        "token2int[\"the\"], int2token[14271]"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hHBP390IGxW",
        "outputId": "414c5372-a4d2-427b-ba33-fbdc6313927f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16592"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# set vocabulary size\n",
        "vocab_size = len(int2token)\n",
        "vocab_size"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_vEgmyXIGxW",
        "outputId": "967435d4-4124-4f71-b025-7b54006c1239"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def get_integer_seq(seq):\n",
        "  return [token2int[w] for w in seq.split()]\n",
        "\n",
        "# convert text sequences to integer sequences\n",
        "x_int = [get_integer_seq(i) for i in x]\n",
        "y_int = [get_integer_seq(i) for i in y]\n",
        "\n",
        "# convert lists to numpy arrays\n",
        "x_int = np.array(x_int)\n",
        "y_int = np.array(y_int)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "qTxp23gLIGxX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def get_batches(arr_x, arr_y, batch_size):\n",
        "\n",
        "    # iterate through the arrays\n",
        "    prv = 0\n",
        "    for n in range(batch_size, arr_x.shape[0], batch_size):\n",
        "      x = arr_x[prv:n,:]\n",
        "      y = arr_y[prv:n,:]\n",
        "      prv = n\n",
        "      yield x, y"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "opGL1hQwIGxX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class WordLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n",
        "        super().__init__()\n",
        "\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "\n",
        "        self.emb_layer = nn.Embedding(vocab_size, 200)\n",
        "\n",
        "        ## define the LSTM\n",
        "        self.lstm = nn.LSTM(200, n_hidden, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        ## define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        ## define the fully-connected layer\n",
        "        self.fc = nn.Linear(n_hidden, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network.\n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "       ## pass input through embedding layer\n",
        "        embedded = self.emb_layer(x)\n",
        "\n",
        "        ## Get the outputs and the new hidden state from the lstm\n",
        "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
        "\n",
        "        ## pass through a dropout layer\n",
        "        out = self.dropout(lstm_output)\n",
        "\n",
        "        #out = out.contiguous().view(-1, self.n_hidden)\n",
        "        out = out.reshape(-1, self.n_hidden)\n",
        "\n",
        "        ## put \"out\" through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        # if GPU is available   mps is for mac code\n",
        "        if (torch.cuda.is_available()):\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "\n",
        "        # if GPU is not available\n",
        "        else:\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "\n",
        "        return hidden"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ya_f0O32IGxX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordLSTM(\n",
            "  (emb_layer): Embedding(16592, 200)\n",
            "  (lstm): LSTM(200, 256, num_layers=4, batch_first=True, dropout=0.3)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=16592, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# instantiate the model\n",
        "net = WordLSTM()\n",
        "\n",
        "# push the model to GPU (avoid it if you are not using the GPU)\n",
        "#net.to(device)\n",
        "\n",
        "print(net)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8Fm2ndAIGxY",
        "outputId": "1c45a797-6479-4f94-df2d-9d664a648421"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[16337,   208,  6497, 11141,  5863],\n",
              "       [  208,  6497, 11141,  5863, 13537],\n",
              "       [ 6497, 11141,  5863, 13537, 15103],\n",
              "       ...,\n",
              "       [12785,  2884,  4393,  6429, 12346],\n",
              "       [ 2884,  4393,  6429, 12346,  2831],\n",
              "       [ 4393,  6429, 12346,  2831,  5452]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "x_int"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "zxGk5YXsIGxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57038860-5bfa-4ba8-d29d-dad7d3e295f5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def train(net, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
        "\n",
        "    # optimizer\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    # loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # push model to GPU\n",
        "    net.cuda()\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    for e in range(epochs):\n",
        "\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "\n",
        "        for x, y in get_batches(x_int, y_int, batch_size):\n",
        "            counter+= 1\n",
        "\n",
        "            # convert numpy arrays to PyTorch arrays\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "            # push tensors to GPU\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # detach hidden states\n",
        "            h = tuple([each.data for each in h])\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "\n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "\n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(-1))\n",
        "\n",
        "            # back-propagate error\n",
        "            loss.backward()\n",
        "\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "\n",
        "            # update weigths\n",
        "            opt.step()\n",
        "\n",
        "            if counter % print_every == 0:\n",
        "\n",
        "              print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                    \"Step: {}...\".format(counter))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "s4id12PsIGxY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/50... Step: 256...\n",
            "Epoch: 1/50... Step: 512...\n",
            "Epoch: 1/50... Step: 768...\n",
            "Epoch: 1/50... Step: 1024...\n",
            "Epoch: 1/50... Step: 1280...\n",
            "Epoch: 1/50... Step: 1536...\n",
            "Epoch: 1/50... Step: 1792...\n",
            "Epoch: 1/50... Step: 2048...\n",
            "Epoch: 1/50... Step: 2304...\n",
            "Epoch: 1/50... Step: 2560...\n",
            "Epoch: 1/50... Step: 2816...\n",
            "Epoch: 1/50... Step: 3072...\n",
            "Epoch: 1/50... Step: 3328...\n",
            "Epoch: 1/50... Step: 3584...\n",
            "Epoch: 1/50... Step: 3840...\n",
            "Epoch: 1/50... Step: 4096...\n",
            "Epoch: 1/50... Step: 4352...\n",
            "Epoch: 1/50... Step: 4608...\n",
            "Epoch: 2/50... Step: 4864...\n",
            "Epoch: 2/50... Step: 5120...\n",
            "Epoch: 2/50... Step: 5376...\n",
            "Epoch: 2/50... Step: 5632...\n",
            "Epoch: 2/50... Step: 5888...\n",
            "Epoch: 2/50... Step: 6144...\n",
            "Epoch: 2/50... Step: 6400...\n",
            "Epoch: 2/50... Step: 6656...\n",
            "Epoch: 2/50... Step: 6912...\n",
            "Epoch: 2/50... Step: 7168...\n",
            "Epoch: 2/50... Step: 7424...\n",
            "Epoch: 2/50... Step: 7680...\n",
            "Epoch: 2/50... Step: 7936...\n",
            "Epoch: 2/50... Step: 8192...\n",
            "Epoch: 2/50... Step: 8448...\n",
            "Epoch: 2/50... Step: 8704...\n",
            "Epoch: 2/50... Step: 8960...\n",
            "Epoch: 2/50... Step: 9216...\n",
            "Epoch: 2/50... Step: 9472...\n",
            "Epoch: 3/50... Step: 9728...\n",
            "Epoch: 3/50... Step: 9984...\n",
            "Epoch: 3/50... Step: 10240...\n",
            "Epoch: 3/50... Step: 10496...\n",
            "Epoch: 3/50... Step: 10752...\n",
            "Epoch: 3/50... Step: 11008...\n",
            "Epoch: 3/50... Step: 11264...\n",
            "Epoch: 3/50... Step: 11520...\n",
            "Epoch: 3/50... Step: 11776...\n",
            "Epoch: 3/50... Step: 12032...\n",
            "Epoch: 3/50... Step: 12288...\n",
            "Epoch: 3/50... Step: 12544...\n",
            "Epoch: 3/50... Step: 12800...\n",
            "Epoch: 3/50... Step: 13056...\n",
            "Epoch: 3/50... Step: 13312...\n",
            "Epoch: 3/50... Step: 13568...\n",
            "Epoch: 3/50... Step: 13824...\n",
            "Epoch: 3/50... Step: 14080...\n",
            "Epoch: 4/50... Step: 14336...\n",
            "Epoch: 4/50... Step: 14592...\n",
            "Epoch: 4/50... Step: 14848...\n",
            "Epoch: 4/50... Step: 15104...\n",
            "Epoch: 4/50... Step: 15360...\n",
            "Epoch: 4/50... Step: 15616...\n",
            "Epoch: 4/50... Step: 15872...\n",
            "Epoch: 4/50... Step: 16128...\n",
            "Epoch: 4/50... Step: 16384...\n",
            "Epoch: 4/50... Step: 16640...\n",
            "Epoch: 4/50... Step: 16896...\n",
            "Epoch: 4/50... Step: 17152...\n",
            "Epoch: 4/50... Step: 17408...\n",
            "Epoch: 4/50... Step: 17664...\n",
            "Epoch: 4/50... Step: 17920...\n",
            "Epoch: 4/50... Step: 18176...\n",
            "Epoch: 4/50... Step: 18432...\n",
            "Epoch: 4/50... Step: 18688...\n",
            "Epoch: 4/50... Step: 18944...\n",
            "Epoch: 5/50... Step: 19200...\n",
            "Epoch: 5/50... Step: 19456...\n",
            "Epoch: 5/50... Step: 19712...\n",
            "Epoch: 5/50... Step: 19968...\n",
            "Epoch: 5/50... Step: 20224...\n",
            "Epoch: 5/50... Step: 20480...\n",
            "Epoch: 5/50... Step: 20736...\n",
            "Epoch: 5/50... Step: 20992...\n",
            "Epoch: 5/50... Step: 21248...\n",
            "Epoch: 5/50... Step: 21504...\n",
            "Epoch: 5/50... Step: 21760...\n",
            "Epoch: 5/50... Step: 22016...\n",
            "Epoch: 5/50... Step: 22272...\n",
            "Epoch: 5/50... Step: 22528...\n",
            "Epoch: 5/50... Step: 22784...\n",
            "Epoch: 5/50... Step: 23040...\n",
            "Epoch: 5/50... Step: 23296...\n",
            "Epoch: 5/50... Step: 23552...\n",
            "Epoch: 5/50... Step: 23808...\n",
            "Epoch: 6/50... Step: 24064...\n",
            "Epoch: 6/50... Step: 24320...\n",
            "Epoch: 6/50... Step: 24576...\n",
            "Epoch: 6/50... Step: 24832...\n",
            "Epoch: 6/50... Step: 25088...\n",
            "Epoch: 6/50... Step: 25344...\n",
            "Epoch: 6/50... Step: 25600...\n",
            "Epoch: 6/50... Step: 25856...\n",
            "Epoch: 6/50... Step: 26112...\n",
            "Epoch: 6/50... Step: 26368...\n",
            "Epoch: 6/50... Step: 26624...\n",
            "Epoch: 6/50... Step: 26880...\n",
            "Epoch: 6/50... Step: 27136...\n",
            "Epoch: 6/50... Step: 27392...\n",
            "Epoch: 6/50... Step: 27648...\n",
            "Epoch: 6/50... Step: 27904...\n",
            "Epoch: 6/50... Step: 28160...\n",
            "Epoch: 6/50... Step: 28416...\n",
            "Epoch: 7/50... Step: 28672...\n",
            "Epoch: 7/50... Step: 28928...\n",
            "Epoch: 7/50... Step: 29184...\n",
            "Epoch: 7/50... Step: 29440...\n",
            "Epoch: 7/50... Step: 29696...\n",
            "Epoch: 7/50... Step: 29952...\n",
            "Epoch: 7/50... Step: 30208...\n",
            "Epoch: 7/50... Step: 30464...\n",
            "Epoch: 7/50... Step: 30720...\n",
            "Epoch: 7/50... Step: 30976...\n",
            "Epoch: 7/50... Step: 31232...\n",
            "Epoch: 7/50... Step: 31488...\n",
            "Epoch: 7/50... Step: 31744...\n",
            "Epoch: 7/50... Step: 32000...\n",
            "Epoch: 7/50... Step: 32256...\n",
            "Epoch: 7/50... Step: 32512...\n",
            "Epoch: 7/50... Step: 32768...\n",
            "Epoch: 7/50... Step: 33024...\n",
            "Epoch: 7/50... Step: 33280...\n",
            "Epoch: 8/50... Step: 33536...\n",
            "Epoch: 8/50... Step: 33792...\n",
            "Epoch: 8/50... Step: 34048...\n",
            "Epoch: 8/50... Step: 34304...\n",
            "Epoch: 8/50... Step: 34560...\n",
            "Epoch: 8/50... Step: 34816...\n",
            "Epoch: 8/50... Step: 35072...\n",
            "Epoch: 8/50... Step: 35328...\n",
            "Epoch: 8/50... Step: 35584...\n",
            "Epoch: 8/50... Step: 35840...\n",
            "Epoch: 8/50... Step: 36096...\n",
            "Epoch: 8/50... Step: 36352...\n",
            "Epoch: 8/50... Step: 36608...\n",
            "Epoch: 8/50... Step: 36864...\n",
            "Epoch: 8/50... Step: 37120...\n",
            "Epoch: 8/50... Step: 37376...\n",
            "Epoch: 8/50... Step: 37632...\n",
            "Epoch: 8/50... Step: 37888...\n",
            "Epoch: 8/50... Step: 38144...\n",
            "Epoch: 9/50... Step: 38400...\n",
            "Epoch: 9/50... Step: 38656...\n",
            "Epoch: 9/50... Step: 38912...\n",
            "Epoch: 9/50... Step: 39168...\n",
            "Epoch: 9/50... Step: 39424...\n",
            "Epoch: 9/50... Step: 39680...\n",
            "Epoch: 9/50... Step: 39936...\n",
            "Epoch: 9/50... Step: 40192...\n",
            "Epoch: 9/50... Step: 40448...\n",
            "Epoch: 9/50... Step: 40704...\n",
            "Epoch: 9/50... Step: 40960...\n",
            "Epoch: 9/50... Step: 41216...\n",
            "Epoch: 9/50... Step: 41472...\n",
            "Epoch: 9/50... Step: 41728...\n",
            "Epoch: 9/50... Step: 41984...\n",
            "Epoch: 9/50... Step: 42240...\n",
            "Epoch: 9/50... Step: 42496...\n",
            "Epoch: 9/50... Step: 42752...\n",
            "Epoch: 10/50... Step: 43008...\n",
            "Epoch: 10/50... Step: 43264...\n",
            "Epoch: 10/50... Step: 43520...\n",
            "Epoch: 10/50... Step: 43776...\n",
            "Epoch: 10/50... Step: 44032...\n",
            "Epoch: 10/50... Step: 44288...\n",
            "Epoch: 10/50... Step: 44544...\n",
            "Epoch: 10/50... Step: 44800...\n",
            "Epoch: 10/50... Step: 45056...\n",
            "Epoch: 10/50... Step: 45312...\n",
            "Epoch: 10/50... Step: 45568...\n",
            "Epoch: 10/50... Step: 45824...\n",
            "Epoch: 10/50... Step: 46080...\n",
            "Epoch: 10/50... Step: 46336...\n",
            "Epoch: 10/50... Step: 46592...\n",
            "Epoch: 10/50... Step: 46848...\n",
            "Epoch: 10/50... Step: 47104...\n",
            "Epoch: 10/50... Step: 47360...\n",
            "Epoch: 10/50... Step: 47616...\n",
            "Epoch: 11/50... Step: 47872...\n",
            "Epoch: 11/50... Step: 48128...\n",
            "Epoch: 11/50... Step: 48384...\n",
            "Epoch: 11/50... Step: 48640...\n",
            "Epoch: 11/50... Step: 48896...\n",
            "Epoch: 11/50... Step: 49152...\n",
            "Epoch: 11/50... Step: 49408...\n",
            "Epoch: 11/50... Step: 49664...\n",
            "Epoch: 11/50... Step: 49920...\n",
            "Epoch: 11/50... Step: 50176...\n",
            "Epoch: 11/50... Step: 50432...\n",
            "Epoch: 11/50... Step: 50688...\n",
            "Epoch: 11/50... Step: 50944...\n",
            "Epoch: 11/50... Step: 51200...\n",
            "Epoch: 11/50... Step: 51456...\n",
            "Epoch: 11/50... Step: 51712...\n",
            "Epoch: 11/50... Step: 51968...\n",
            "Epoch: 11/50... Step: 52224...\n",
            "Epoch: 12/50... Step: 52480...\n",
            "Epoch: 12/50... Step: 52736...\n",
            "Epoch: 12/50... Step: 52992...\n",
            "Epoch: 12/50... Step: 53248...\n",
            "Epoch: 12/50... Step: 53504...\n",
            "Epoch: 12/50... Step: 53760...\n",
            "Epoch: 12/50... Step: 54016...\n",
            "Epoch: 12/50... Step: 54272...\n",
            "Epoch: 12/50... Step: 54528...\n",
            "Epoch: 12/50... Step: 54784...\n",
            "Epoch: 12/50... Step: 55040...\n",
            "Epoch: 12/50... Step: 55296...\n",
            "Epoch: 12/50... Step: 55552...\n",
            "Epoch: 12/50... Step: 55808...\n",
            "Epoch: 12/50... Step: 56064...\n",
            "Epoch: 12/50... Step: 56320...\n",
            "Epoch: 12/50... Step: 56576...\n",
            "Epoch: 12/50... Step: 56832...\n",
            "Epoch: 12/50... Step: 57088...\n",
            "Epoch: 13/50... Step: 57344...\n",
            "Epoch: 13/50... Step: 57600...\n",
            "Epoch: 13/50... Step: 57856...\n",
            "Epoch: 13/50... Step: 58112...\n",
            "Epoch: 13/50... Step: 58368...\n",
            "Epoch: 13/50... Step: 58624...\n",
            "Epoch: 13/50... Step: 58880...\n",
            "Epoch: 13/50... Step: 59136...\n",
            "Epoch: 13/50... Step: 59392...\n",
            "Epoch: 13/50... Step: 59648...\n",
            "Epoch: 13/50... Step: 59904...\n",
            "Epoch: 13/50... Step: 60160...\n",
            "Epoch: 13/50... Step: 60416...\n",
            "Epoch: 13/50... Step: 60672...\n",
            "Epoch: 13/50... Step: 60928...\n",
            "Epoch: 13/50... Step: 61184...\n",
            "Epoch: 13/50... Step: 61440...\n",
            "Epoch: 13/50... Step: 61696...\n",
            "Epoch: 13/50... Step: 61952...\n",
            "Epoch: 14/50... Step: 62208...\n",
            "Epoch: 14/50... Step: 62464...\n",
            "Epoch: 14/50... Step: 62720...\n",
            "Epoch: 14/50... Step: 62976...\n",
            "Epoch: 14/50... Step: 63232...\n",
            "Epoch: 14/50... Step: 63488...\n",
            "Epoch: 14/50... Step: 63744...\n",
            "Epoch: 14/50... Step: 64000...\n",
            "Epoch: 14/50... Step: 64256...\n",
            "Epoch: 14/50... Step: 64512...\n",
            "Epoch: 14/50... Step: 64768...\n",
            "Epoch: 14/50... Step: 65024...\n",
            "Epoch: 14/50... Step: 65280...\n",
            "Epoch: 14/50... Step: 65536...\n",
            "Epoch: 14/50... Step: 65792...\n",
            "Epoch: 14/50... Step: 66048...\n",
            "Epoch: 14/50... Step: 66304...\n",
            "Epoch: 14/50... Step: 66560...\n",
            "Epoch: 15/50... Step: 66816...\n",
            "Epoch: 15/50... Step: 67072...\n",
            "Epoch: 15/50... Step: 67328...\n",
            "Epoch: 15/50... Step: 67584...\n",
            "Epoch: 15/50... Step: 67840...\n",
            "Epoch: 15/50... Step: 68096...\n",
            "Epoch: 15/50... Step: 68352...\n",
            "Epoch: 15/50... Step: 68608...\n",
            "Epoch: 15/50... Step: 68864...\n",
            "Epoch: 15/50... Step: 69120...\n",
            "Epoch: 15/50... Step: 69376...\n",
            "Epoch: 15/50... Step: 69632...\n",
            "Epoch: 15/50... Step: 69888...\n",
            "Epoch: 15/50... Step: 70144...\n",
            "Epoch: 15/50... Step: 70400...\n",
            "Epoch: 15/50... Step: 70656...\n",
            "Epoch: 15/50... Step: 70912...\n",
            "Epoch: 15/50... Step: 71168...\n",
            "Epoch: 15/50... Step: 71424...\n",
            "Epoch: 16/50... Step: 71680...\n",
            "Epoch: 16/50... Step: 71936...\n",
            "Epoch: 16/50... Step: 72192...\n",
            "Epoch: 16/50... Step: 72448...\n",
            "Epoch: 16/50... Step: 72704...\n",
            "Epoch: 16/50... Step: 72960...\n",
            "Epoch: 16/50... Step: 73216...\n",
            "Epoch: 16/50... Step: 73472...\n",
            "Epoch: 16/50... Step: 73728...\n",
            "Epoch: 16/50... Step: 73984...\n",
            "Epoch: 16/50... Step: 74240...\n",
            "Epoch: 16/50... Step: 74496...\n",
            "Epoch: 16/50... Step: 74752...\n",
            "Epoch: 16/50... Step: 75008...\n",
            "Epoch: 16/50... Step: 75264...\n",
            "Epoch: 16/50... Step: 75520...\n",
            "Epoch: 16/50... Step: 75776...\n",
            "Epoch: 16/50... Step: 76032...\n",
            "Epoch: 16/50... Step: 76288...\n",
            "Epoch: 17/50... Step: 76544...\n",
            "Epoch: 17/50... Step: 76800...\n",
            "Epoch: 17/50... Step: 77056...\n",
            "Epoch: 17/50... Step: 77312...\n",
            "Epoch: 17/50... Step: 77568...\n",
            "Epoch: 17/50... Step: 77824...\n",
            "Epoch: 17/50... Step: 78080...\n",
            "Epoch: 17/50... Step: 78336...\n",
            "Epoch: 17/50... Step: 78592...\n",
            "Epoch: 17/50... Step: 78848...\n",
            "Epoch: 17/50... Step: 79104...\n",
            "Epoch: 17/50... Step: 79360...\n",
            "Epoch: 17/50... Step: 79616...\n",
            "Epoch: 17/50... Step: 79872...\n",
            "Epoch: 17/50... Step: 80128...\n",
            "Epoch: 17/50... Step: 80384...\n",
            "Epoch: 17/50... Step: 80640...\n",
            "Epoch: 17/50... Step: 80896...\n",
            "Epoch: 18/50... Step: 81152...\n",
            "Epoch: 18/50... Step: 81408...\n",
            "Epoch: 18/50... Step: 81664...\n",
            "Epoch: 18/50... Step: 81920...\n",
            "Epoch: 18/50... Step: 82176...\n",
            "Epoch: 18/50... Step: 82432...\n",
            "Epoch: 18/50... Step: 82688...\n",
            "Epoch: 18/50... Step: 82944...\n",
            "Epoch: 18/50... Step: 83200...\n",
            "Epoch: 18/50... Step: 83456...\n",
            "Epoch: 18/50... Step: 83712...\n",
            "Epoch: 18/50... Step: 83968...\n",
            "Epoch: 18/50... Step: 84224...\n",
            "Epoch: 18/50... Step: 84480...\n",
            "Epoch: 18/50... Step: 84736...\n",
            "Epoch: 18/50... Step: 84992...\n",
            "Epoch: 18/50... Step: 85248...\n",
            "Epoch: 18/50... Step: 85504...\n",
            "Epoch: 18/50... Step: 85760...\n",
            "Epoch: 19/50... Step: 86016...\n",
            "Epoch: 19/50... Step: 86272...\n",
            "Epoch: 19/50... Step: 86528...\n",
            "Epoch: 19/50... Step: 86784...\n",
            "Epoch: 19/50... Step: 87040...\n",
            "Epoch: 19/50... Step: 87296...\n",
            "Epoch: 19/50... Step: 87552...\n",
            "Epoch: 19/50... Step: 87808...\n",
            "Epoch: 19/50... Step: 88064...\n",
            "Epoch: 19/50... Step: 88320...\n",
            "Epoch: 19/50... Step: 88576...\n",
            "Epoch: 19/50... Step: 88832...\n",
            "Epoch: 19/50... Step: 89088...\n",
            "Epoch: 19/50... Step: 89344...\n",
            "Epoch: 19/50... Step: 89600...\n",
            "Epoch: 19/50... Step: 89856...\n",
            "Epoch: 19/50... Step: 90112...\n",
            "Epoch: 19/50... Step: 90368...\n",
            "Epoch: 19/50... Step: 90624...\n",
            "Epoch: 20/50... Step: 90880...\n",
            "Epoch: 20/50... Step: 91136...\n",
            "Epoch: 20/50... Step: 91392...\n",
            "Epoch: 20/50... Step: 91648...\n",
            "Epoch: 20/50... Step: 91904...\n",
            "Epoch: 20/50... Step: 92160...\n",
            "Epoch: 20/50... Step: 92416...\n",
            "Epoch: 20/50... Step: 92672...\n",
            "Epoch: 20/50... Step: 92928...\n",
            "Epoch: 20/50... Step: 93184...\n",
            "Epoch: 20/50... Step: 93440...\n",
            "Epoch: 20/50... Step: 93696...\n",
            "Epoch: 20/50... Step: 93952...\n",
            "Epoch: 20/50... Step: 94208...\n",
            "Epoch: 20/50... Step: 94464...\n",
            "Epoch: 20/50... Step: 94720...\n",
            "Epoch: 20/50... Step: 94976...\n",
            "Epoch: 20/50... Step: 95232...\n",
            "Epoch: 21/50... Step: 95488...\n",
            "Epoch: 21/50... Step: 95744...\n",
            "Epoch: 21/50... Step: 96000...\n",
            "Epoch: 21/50... Step: 96256...\n",
            "Epoch: 21/50... Step: 96512...\n",
            "Epoch: 21/50... Step: 96768...\n",
            "Epoch: 21/50... Step: 97024...\n",
            "Epoch: 21/50... Step: 97280...\n",
            "Epoch: 21/50... Step: 97536...\n",
            "Epoch: 21/50... Step: 97792...\n",
            "Epoch: 21/50... Step: 98048...\n",
            "Epoch: 21/50... Step: 98304...\n",
            "Epoch: 21/50... Step: 98560...\n",
            "Epoch: 21/50... Step: 98816...\n",
            "Epoch: 21/50... Step: 99072...\n",
            "Epoch: 21/50... Step: 99328...\n",
            "Epoch: 21/50... Step: 99584...\n",
            "Epoch: 21/50... Step: 99840...\n",
            "Epoch: 21/50... Step: 100096...\n",
            "Epoch: 22/50... Step: 100352...\n",
            "Epoch: 22/50... Step: 100608...\n",
            "Epoch: 22/50... Step: 100864...\n",
            "Epoch: 22/50... Step: 101120...\n",
            "Epoch: 22/50... Step: 101376...\n",
            "Epoch: 22/50... Step: 101632...\n",
            "Epoch: 22/50... Step: 101888...\n",
            "Epoch: 22/50... Step: 102144...\n",
            "Epoch: 22/50... Step: 102400...\n",
            "Epoch: 22/50... Step: 102656...\n",
            "Epoch: 22/50... Step: 102912...\n",
            "Epoch: 22/50... Step: 103168...\n",
            "Epoch: 22/50... Step: 103424...\n",
            "Epoch: 22/50... Step: 103680...\n",
            "Epoch: 22/50... Step: 103936...\n",
            "Epoch: 22/50... Step: 104192...\n",
            "Epoch: 22/50... Step: 104448...\n",
            "Epoch: 22/50... Step: 104704...\n",
            "Epoch: 23/50... Step: 104960...\n",
            "Epoch: 23/50... Step: 105216...\n",
            "Epoch: 23/50... Step: 105472...\n",
            "Epoch: 23/50... Step: 105728...\n",
            "Epoch: 23/50... Step: 105984...\n",
            "Epoch: 23/50... Step: 106240...\n",
            "Epoch: 23/50... Step: 106496...\n",
            "Epoch: 23/50... Step: 106752...\n",
            "Epoch: 23/50... Step: 107008...\n",
            "Epoch: 23/50... Step: 107264...\n",
            "Epoch: 23/50... Step: 107520...\n",
            "Epoch: 23/50... Step: 107776...\n",
            "Epoch: 23/50... Step: 108032...\n",
            "Epoch: 23/50... Step: 108288...\n",
            "Epoch: 23/50... Step: 108544...\n",
            "Epoch: 23/50... Step: 108800...\n",
            "Epoch: 23/50... Step: 109056...\n",
            "Epoch: 23/50... Step: 109312...\n",
            "Epoch: 23/50... Step: 109568...\n",
            "Epoch: 24/50... Step: 109824...\n",
            "Epoch: 24/50... Step: 110080...\n",
            "Epoch: 24/50... Step: 110336...\n",
            "Epoch: 24/50... Step: 110592...\n",
            "Epoch: 24/50... Step: 110848...\n",
            "Epoch: 24/50... Step: 111104...\n",
            "Epoch: 24/50... Step: 111360...\n",
            "Epoch: 24/50... Step: 111616...\n",
            "Epoch: 24/50... Step: 111872...\n",
            "Epoch: 24/50... Step: 112128...\n",
            "Epoch: 24/50... Step: 112384...\n",
            "Epoch: 24/50... Step: 112640...\n",
            "Epoch: 24/50... Step: 112896...\n",
            "Epoch: 24/50... Step: 113152...\n",
            "Epoch: 24/50... Step: 113408...\n",
            "Epoch: 24/50... Step: 113664...\n",
            "Epoch: 24/50... Step: 113920...\n",
            "Epoch: 24/50... Step: 114176...\n",
            "Epoch: 24/50... Step: 114432...\n",
            "Epoch: 25/50... Step: 114688...\n",
            "Epoch: 25/50... Step: 114944...\n",
            "Epoch: 25/50... Step: 115200...\n",
            "Epoch: 25/50... Step: 115456...\n",
            "Epoch: 25/50... Step: 115712...\n",
            "Epoch: 25/50... Step: 115968...\n",
            "Epoch: 25/50... Step: 116224...\n",
            "Epoch: 25/50... Step: 116480...\n",
            "Epoch: 25/50... Step: 116736...\n",
            "Epoch: 25/50... Step: 116992...\n",
            "Epoch: 25/50... Step: 117248...\n",
            "Epoch: 25/50... Step: 117504...\n",
            "Epoch: 25/50... Step: 117760...\n",
            "Epoch: 25/50... Step: 118016...\n",
            "Epoch: 25/50... Step: 118272...\n",
            "Epoch: 25/50... Step: 118528...\n",
            "Epoch: 25/50... Step: 118784...\n",
            "Epoch: 25/50... Step: 119040...\n",
            "Epoch: 26/50... Step: 119296...\n",
            "Epoch: 26/50... Step: 119552...\n",
            "Epoch: 26/50... Step: 119808...\n",
            "Epoch: 26/50... Step: 120064...\n",
            "Epoch: 26/50... Step: 120320...\n",
            "Epoch: 26/50... Step: 120576...\n",
            "Epoch: 26/50... Step: 120832...\n",
            "Epoch: 26/50... Step: 121088...\n",
            "Epoch: 26/50... Step: 121344...\n",
            "Epoch: 26/50... Step: 121600...\n",
            "Epoch: 26/50... Step: 121856...\n",
            "Epoch: 26/50... Step: 122112...\n",
            "Epoch: 26/50... Step: 122368...\n",
            "Epoch: 26/50... Step: 122624...\n",
            "Epoch: 26/50... Step: 122880...\n",
            "Epoch: 26/50... Step: 123136...\n",
            "Epoch: 26/50... Step: 123392...\n",
            "Epoch: 26/50... Step: 123648...\n",
            "Epoch: 26/50... Step: 123904...\n",
            "Epoch: 27/50... Step: 124160...\n",
            "Epoch: 27/50... Step: 124416...\n",
            "Epoch: 27/50... Step: 124672...\n",
            "Epoch: 27/50... Step: 124928...\n",
            "Epoch: 27/50... Step: 125184...\n",
            "Epoch: 27/50... Step: 125440...\n",
            "Epoch: 27/50... Step: 125696...\n",
            "Epoch: 27/50... Step: 125952...\n",
            "Epoch: 27/50... Step: 126208...\n",
            "Epoch: 27/50... Step: 126464...\n",
            "Epoch: 27/50... Step: 126720...\n",
            "Epoch: 27/50... Step: 126976...\n",
            "Epoch: 27/50... Step: 127232...\n",
            "Epoch: 27/50... Step: 127488...\n",
            "Epoch: 27/50... Step: 127744...\n",
            "Epoch: 27/50... Step: 128000...\n",
            "Epoch: 27/50... Step: 128256...\n",
            "Epoch: 27/50... Step: 128512...\n",
            "Epoch: 27/50... Step: 128768...\n",
            "Epoch: 28/50... Step: 129024...\n",
            "Epoch: 28/50... Step: 129280...\n",
            "Epoch: 28/50... Step: 129536...\n",
            "Epoch: 28/50... Step: 129792...\n",
            "Epoch: 28/50... Step: 130048...\n",
            "Epoch: 28/50... Step: 130304...\n",
            "Epoch: 28/50... Step: 130560...\n",
            "Epoch: 28/50... Step: 130816...\n",
            "Epoch: 28/50... Step: 131072...\n",
            "Epoch: 28/50... Step: 131328...\n",
            "Epoch: 28/50... Step: 131584...\n",
            "Epoch: 28/50... Step: 131840...\n",
            "Epoch: 28/50... Step: 132096...\n",
            "Epoch: 28/50... Step: 132352...\n",
            "Epoch: 28/50... Step: 132608...\n",
            "Epoch: 28/50... Step: 132864...\n",
            "Epoch: 28/50... Step: 133120...\n",
            "Epoch: 28/50... Step: 133376...\n",
            "Epoch: 29/50... Step: 133632...\n",
            "Epoch: 29/50... Step: 133888...\n",
            "Epoch: 29/50... Step: 134144...\n",
            "Epoch: 29/50... Step: 134400...\n",
            "Epoch: 29/50... Step: 134656...\n",
            "Epoch: 29/50... Step: 134912...\n",
            "Epoch: 29/50... Step: 135168...\n",
            "Epoch: 29/50... Step: 135424...\n",
            "Epoch: 29/50... Step: 135680...\n",
            "Epoch: 29/50... Step: 135936...\n",
            "Epoch: 29/50... Step: 136192...\n",
            "Epoch: 29/50... Step: 136448...\n",
            "Epoch: 29/50... Step: 136704...\n",
            "Epoch: 29/50... Step: 136960...\n",
            "Epoch: 29/50... Step: 137216...\n",
            "Epoch: 29/50... Step: 137472...\n",
            "Epoch: 29/50... Step: 137728...\n",
            "Epoch: 29/50... Step: 137984...\n",
            "Epoch: 29/50... Step: 138240...\n",
            "Epoch: 30/50... Step: 138496...\n",
            "Epoch: 30/50... Step: 138752...\n",
            "Epoch: 30/50... Step: 139008...\n",
            "Epoch: 30/50... Step: 139264...\n",
            "Epoch: 30/50... Step: 139520...\n",
            "Epoch: 30/50... Step: 139776...\n",
            "Epoch: 30/50... Step: 140032...\n",
            "Epoch: 30/50... Step: 140288...\n",
            "Epoch: 30/50... Step: 140544...\n",
            "Epoch: 30/50... Step: 140800...\n",
            "Epoch: 30/50... Step: 141056...\n",
            "Epoch: 30/50... Step: 141312...\n",
            "Epoch: 30/50... Step: 141568...\n",
            "Epoch: 30/50... Step: 141824...\n",
            "Epoch: 30/50... Step: 142080...\n",
            "Epoch: 30/50... Step: 142336...\n",
            "Epoch: 30/50... Step: 142592...\n",
            "Epoch: 30/50... Step: 142848...\n",
            "Epoch: 31/50... Step: 143104...\n",
            "Epoch: 31/50... Step: 143360...\n",
            "Epoch: 31/50... Step: 143616...\n",
            "Epoch: 31/50... Step: 143872...\n",
            "Epoch: 31/50... Step: 144128...\n",
            "Epoch: 31/50... Step: 144384...\n",
            "Epoch: 31/50... Step: 144640...\n",
            "Epoch: 31/50... Step: 144896...\n",
            "Epoch: 31/50... Step: 145152...\n",
            "Epoch: 31/50... Step: 145408...\n",
            "Epoch: 31/50... Step: 145664...\n",
            "Epoch: 31/50... Step: 145920...\n",
            "Epoch: 31/50... Step: 146176...\n",
            "Epoch: 31/50... Step: 146432...\n",
            "Epoch: 31/50... Step: 146688...\n",
            "Epoch: 31/50... Step: 146944...\n",
            "Epoch: 31/50... Step: 147200...\n",
            "Epoch: 31/50... Step: 147456...\n",
            "Epoch: 31/50... Step: 147712...\n",
            "Epoch: 32/50... Step: 147968...\n",
            "Epoch: 32/50... Step: 148224...\n",
            "Epoch: 32/50... Step: 148480...\n",
            "Epoch: 32/50... Step: 148736...\n",
            "Epoch: 32/50... Step: 148992...\n",
            "Epoch: 32/50... Step: 149248...\n",
            "Epoch: 32/50... Step: 149504...\n",
            "Epoch: 32/50... Step: 149760...\n",
            "Epoch: 32/50... Step: 150016...\n",
            "Epoch: 32/50... Step: 150272...\n",
            "Epoch: 32/50... Step: 150528...\n",
            "Epoch: 32/50... Step: 150784...\n",
            "Epoch: 32/50... Step: 151040...\n",
            "Epoch: 32/50... Step: 151296...\n",
            "Epoch: 32/50... Step: 151552...\n",
            "Epoch: 32/50... Step: 151808...\n",
            "Epoch: 32/50... Step: 152064...\n",
            "Epoch: 32/50... Step: 152320...\n",
            "Epoch: 32/50... Step: 152576...\n",
            "Epoch: 33/50... Step: 152832...\n",
            "Epoch: 33/50... Step: 153088...\n",
            "Epoch: 33/50... Step: 153344...\n",
            "Epoch: 33/50... Step: 153600...\n",
            "Epoch: 33/50... Step: 153856...\n",
            "Epoch: 33/50... Step: 154112...\n",
            "Epoch: 33/50... Step: 154368...\n",
            "Epoch: 33/50... Step: 154624...\n",
            "Epoch: 33/50... Step: 154880...\n",
            "Epoch: 33/50... Step: 155136...\n",
            "Epoch: 33/50... Step: 155392...\n",
            "Epoch: 33/50... Step: 155648...\n",
            "Epoch: 33/50... Step: 155904...\n",
            "Epoch: 33/50... Step: 156160...\n",
            "Epoch: 33/50... Step: 156416...\n",
            "Epoch: 33/50... Step: 156672...\n",
            "Epoch: 33/50... Step: 156928...\n",
            "Epoch: 33/50... Step: 157184...\n",
            "Epoch: 34/50... Step: 157440...\n",
            "Epoch: 34/50... Step: 157696...\n",
            "Epoch: 34/50... Step: 157952...\n",
            "Epoch: 34/50... Step: 158208...\n",
            "Epoch: 34/50... Step: 158464...\n",
            "Epoch: 34/50... Step: 158720...\n",
            "Epoch: 34/50... Step: 158976...\n",
            "Epoch: 34/50... Step: 159232...\n",
            "Epoch: 34/50... Step: 159488...\n",
            "Epoch: 34/50... Step: 159744...\n",
            "Epoch: 34/50... Step: 160000...\n",
            "Epoch: 34/50... Step: 160256...\n",
            "Epoch: 34/50... Step: 160512...\n",
            "Epoch: 34/50... Step: 160768...\n",
            "Epoch: 34/50... Step: 161024...\n",
            "Epoch: 34/50... Step: 161280...\n",
            "Epoch: 34/50... Step: 161536...\n",
            "Epoch: 34/50... Step: 161792...\n",
            "Epoch: 34/50... Step: 162048...\n",
            "Epoch: 35/50... Step: 162304...\n",
            "Epoch: 35/50... Step: 162560...\n",
            "Epoch: 35/50... Step: 162816...\n",
            "Epoch: 35/50... Step: 163072...\n",
            "Epoch: 35/50... Step: 163328...\n",
            "Epoch: 35/50... Step: 163584...\n",
            "Epoch: 35/50... Step: 163840...\n",
            "Epoch: 35/50... Step: 164096...\n",
            "Epoch: 35/50... Step: 164352...\n",
            "Epoch: 35/50... Step: 164608...\n",
            "Epoch: 35/50... Step: 164864...\n",
            "Epoch: 35/50... Step: 165120...\n",
            "Epoch: 35/50... Step: 165376...\n",
            "Epoch: 35/50... Step: 165632...\n",
            "Epoch: 35/50... Step: 165888...\n",
            "Epoch: 35/50... Step: 166144...\n",
            "Epoch: 35/50... Step: 166400...\n",
            "Epoch: 35/50... Step: 166656...\n",
            "Epoch: 35/50... Step: 166912...\n",
            "Epoch: 36/50... Step: 167168...\n",
            "Epoch: 36/50... Step: 167424...\n",
            "Epoch: 36/50... Step: 167680...\n",
            "Epoch: 36/50... Step: 167936...\n",
            "Epoch: 36/50... Step: 168192...\n",
            "Epoch: 36/50... Step: 168448...\n",
            "Epoch: 36/50... Step: 168704...\n",
            "Epoch: 36/50... Step: 168960...\n",
            "Epoch: 36/50... Step: 169216...\n",
            "Epoch: 36/50... Step: 169472...\n",
            "Epoch: 36/50... Step: 169728...\n",
            "Epoch: 36/50... Step: 169984...\n",
            "Epoch: 36/50... Step: 170240...\n",
            "Epoch: 36/50... Step: 170496...\n",
            "Epoch: 36/50... Step: 170752...\n",
            "Epoch: 36/50... Step: 171008...\n",
            "Epoch: 36/50... Step: 171264...\n",
            "Epoch: 36/50... Step: 171520...\n",
            "Epoch: 37/50... Step: 171776...\n",
            "Epoch: 37/50... Step: 172032...\n",
            "Epoch: 37/50... Step: 172288...\n",
            "Epoch: 37/50... Step: 172544...\n",
            "Epoch: 37/50... Step: 172800...\n",
            "Epoch: 37/50... Step: 173056...\n",
            "Epoch: 37/50... Step: 173312...\n",
            "Epoch: 37/50... Step: 173568...\n",
            "Epoch: 37/50... Step: 173824...\n",
            "Epoch: 37/50... Step: 174080...\n",
            "Epoch: 37/50... Step: 174336...\n",
            "Epoch: 37/50... Step: 174592...\n",
            "Epoch: 37/50... Step: 174848...\n",
            "Epoch: 37/50... Step: 175104...\n",
            "Epoch: 37/50... Step: 175360...\n",
            "Epoch: 37/50... Step: 175616...\n",
            "Epoch: 37/50... Step: 175872...\n",
            "Epoch: 37/50... Step: 176128...\n",
            "Epoch: 37/50... Step: 176384...\n",
            "Epoch: 38/50... Step: 176640...\n",
            "Epoch: 38/50... Step: 176896...\n",
            "Epoch: 38/50... Step: 177152...\n",
            "Epoch: 38/50... Step: 177408...\n",
            "Epoch: 38/50... Step: 177664...\n",
            "Epoch: 38/50... Step: 177920...\n",
            "Epoch: 38/50... Step: 178176...\n",
            "Epoch: 38/50... Step: 178432...\n",
            "Epoch: 38/50... Step: 178688...\n",
            "Epoch: 38/50... Step: 178944...\n",
            "Epoch: 38/50... Step: 179200...\n",
            "Epoch: 38/50... Step: 179456...\n",
            "Epoch: 38/50... Step: 179712...\n",
            "Epoch: 38/50... Step: 179968...\n",
            "Epoch: 38/50... Step: 180224...\n",
            "Epoch: 38/50... Step: 180480...\n",
            "Epoch: 38/50... Step: 180736...\n",
            "Epoch: 38/50... Step: 180992...\n",
            "Epoch: 38/50... Step: 181248...\n",
            "Epoch: 39/50... Step: 181504...\n",
            "Epoch: 39/50... Step: 181760...\n",
            "Epoch: 39/50... Step: 182016...\n",
            "Epoch: 39/50... Step: 182272...\n",
            "Epoch: 39/50... Step: 182528...\n",
            "Epoch: 39/50... Step: 182784...\n",
            "Epoch: 39/50... Step: 183040...\n",
            "Epoch: 39/50... Step: 183296...\n",
            "Epoch: 39/50... Step: 183552...\n",
            "Epoch: 39/50... Step: 183808...\n",
            "Epoch: 39/50... Step: 184064...\n",
            "Epoch: 39/50... Step: 184320...\n",
            "Epoch: 39/50... Step: 184576...\n",
            "Epoch: 39/50... Step: 184832...\n",
            "Epoch: 39/50... Step: 185088...\n",
            "Epoch: 39/50... Step: 185344...\n",
            "Epoch: 39/50... Step: 185600...\n",
            "Epoch: 39/50... Step: 185856...\n",
            "Epoch: 40/50... Step: 186112...\n",
            "Epoch: 40/50... Step: 186368...\n",
            "Epoch: 40/50... Step: 186624...\n",
            "Epoch: 40/50... Step: 186880...\n",
            "Epoch: 40/50... Step: 187136...\n",
            "Epoch: 40/50... Step: 187392...\n",
            "Epoch: 40/50... Step: 187648...\n",
            "Epoch: 40/50... Step: 187904...\n",
            "Epoch: 40/50... Step: 188160...\n",
            "Epoch: 40/50... Step: 188416...\n",
            "Epoch: 40/50... Step: 188672...\n",
            "Epoch: 40/50... Step: 188928...\n",
            "Epoch: 40/50... Step: 189184...\n",
            "Epoch: 40/50... Step: 189440...\n",
            "Epoch: 40/50... Step: 189696...\n",
            "Epoch: 40/50... Step: 189952...\n",
            "Epoch: 40/50... Step: 190208...\n",
            "Epoch: 40/50... Step: 190464...\n",
            "Epoch: 40/50... Step: 190720...\n",
            "Epoch: 41/50... Step: 190976...\n",
            "Epoch: 41/50... Step: 191232...\n",
            "Epoch: 41/50... Step: 191488...\n",
            "Epoch: 41/50... Step: 191744...\n",
            "Epoch: 41/50... Step: 192000...\n",
            "Epoch: 41/50... Step: 192256...\n",
            "Epoch: 41/50... Step: 192512...\n",
            "Epoch: 41/50... Step: 192768...\n",
            "Epoch: 41/50... Step: 193024...\n",
            "Epoch: 41/50... Step: 193280...\n",
            "Epoch: 41/50... Step: 193536...\n",
            "Epoch: 41/50... Step: 193792...\n",
            "Epoch: 41/50... Step: 194048...\n",
            "Epoch: 41/50... Step: 194304...\n",
            "Epoch: 41/50... Step: 194560...\n",
            "Epoch: 41/50... Step: 194816...\n",
            "Epoch: 41/50... Step: 195072...\n",
            "Epoch: 41/50... Step: 195328...\n",
            "Epoch: 42/50... Step: 195584...\n",
            "Epoch: 42/50... Step: 195840...\n",
            "Epoch: 42/50... Step: 196096...\n",
            "Epoch: 42/50... Step: 196352...\n",
            "Epoch: 42/50... Step: 196608...\n",
            "Epoch: 42/50... Step: 196864...\n",
            "Epoch: 42/50... Step: 197120...\n",
            "Epoch: 42/50... Step: 197376...\n",
            "Epoch: 42/50... Step: 197632...\n",
            "Epoch: 42/50... Step: 197888...\n",
            "Epoch: 42/50... Step: 198144...\n",
            "Epoch: 42/50... Step: 198400...\n",
            "Epoch: 42/50... Step: 198656...\n",
            "Epoch: 42/50... Step: 198912...\n",
            "Epoch: 42/50... Step: 199168...\n",
            "Epoch: 42/50... Step: 199424...\n",
            "Epoch: 42/50... Step: 199680...\n",
            "Epoch: 42/50... Step: 199936...\n",
            "Epoch: 42/50... Step: 200192...\n",
            "Epoch: 43/50... Step: 200448...\n",
            "Epoch: 43/50... Step: 200704...\n",
            "Epoch: 43/50... Step: 200960...\n",
            "Epoch: 43/50... Step: 201216...\n",
            "Epoch: 43/50... Step: 201472...\n",
            "Epoch: 43/50... Step: 201728...\n",
            "Epoch: 43/50... Step: 201984...\n",
            "Epoch: 43/50... Step: 202240...\n",
            "Epoch: 43/50... Step: 202496...\n",
            "Epoch: 43/50... Step: 202752...\n",
            "Epoch: 43/50... Step: 203008...\n",
            "Epoch: 43/50... Step: 203264...\n",
            "Epoch: 43/50... Step: 203520...\n",
            "Epoch: 43/50... Step: 203776...\n",
            "Epoch: 43/50... Step: 204032...\n",
            "Epoch: 43/50... Step: 204288...\n",
            "Epoch: 43/50... Step: 204544...\n",
            "Epoch: 43/50... Step: 204800...\n",
            "Epoch: 43/50... Step: 205056...\n",
            "Epoch: 44/50... Step: 205312...\n",
            "Epoch: 44/50... Step: 205568...\n",
            "Epoch: 44/50... Step: 205824...\n",
            "Epoch: 44/50... Step: 206080...\n",
            "Epoch: 44/50... Step: 206336...\n",
            "Epoch: 44/50... Step: 206592...\n",
            "Epoch: 44/50... Step: 206848...\n",
            "Epoch: 44/50... Step: 207104...\n",
            "Epoch: 44/50... Step: 207360...\n",
            "Epoch: 44/50... Step: 207616...\n",
            "Epoch: 44/50... Step: 207872...\n",
            "Epoch: 44/50... Step: 208128...\n",
            "Epoch: 44/50... Step: 208384...\n",
            "Epoch: 44/50... Step: 208640...\n",
            "Epoch: 44/50... Step: 208896...\n",
            "Epoch: 44/50... Step: 209152...\n",
            "Epoch: 44/50... Step: 209408...\n",
            "Epoch: 44/50... Step: 209664...\n",
            "Epoch: 45/50... Step: 209920...\n",
            "Epoch: 45/50... Step: 210176...\n",
            "Epoch: 45/50... Step: 210432...\n",
            "Epoch: 45/50... Step: 210688...\n",
            "Epoch: 45/50... Step: 210944...\n",
            "Epoch: 45/50... Step: 211200...\n",
            "Epoch: 45/50... Step: 211456...\n",
            "Epoch: 45/50... Step: 211712...\n",
            "Epoch: 45/50... Step: 211968...\n",
            "Epoch: 45/50... Step: 212224...\n",
            "Epoch: 45/50... Step: 212480...\n",
            "Epoch: 45/50... Step: 212736...\n",
            "Epoch: 45/50... Step: 212992...\n",
            "Epoch: 45/50... Step: 213248...\n",
            "Epoch: 45/50... Step: 213504...\n",
            "Epoch: 45/50... Step: 213760...\n",
            "Epoch: 45/50... Step: 214016...\n",
            "Epoch: 45/50... Step: 214272...\n",
            "Epoch: 45/50... Step: 214528...\n",
            "Epoch: 46/50... Step: 214784...\n",
            "Epoch: 46/50... Step: 215040...\n",
            "Epoch: 46/50... Step: 215296...\n",
            "Epoch: 46/50... Step: 215552...\n",
            "Epoch: 46/50... Step: 215808...\n",
            "Epoch: 46/50... Step: 216064...\n",
            "Epoch: 46/50... Step: 216320...\n",
            "Epoch: 46/50... Step: 216576...\n",
            "Epoch: 46/50... Step: 216832...\n",
            "Epoch: 46/50... Step: 217088...\n",
            "Epoch: 46/50... Step: 217344...\n",
            "Epoch: 46/50... Step: 217600...\n",
            "Epoch: 46/50... Step: 217856...\n",
            "Epoch: 46/50... Step: 218112...\n",
            "Epoch: 46/50... Step: 218368...\n",
            "Epoch: 46/50... Step: 218624...\n",
            "Epoch: 46/50... Step: 218880...\n",
            "Epoch: 46/50... Step: 219136...\n",
            "Epoch: 46/50... Step: 219392...\n",
            "Epoch: 47/50... Step: 219648...\n",
            "Epoch: 47/50... Step: 219904...\n",
            "Epoch: 47/50... Step: 220160...\n",
            "Epoch: 47/50... Step: 220416...\n",
            "Epoch: 47/50... Step: 220672...\n",
            "Epoch: 47/50... Step: 220928...\n",
            "Epoch: 47/50... Step: 221184...\n",
            "Epoch: 47/50... Step: 221440...\n",
            "Epoch: 47/50... Step: 221696...\n",
            "Epoch: 47/50... Step: 221952...\n",
            "Epoch: 47/50... Step: 222208...\n",
            "Epoch: 47/50... Step: 222464...\n",
            "Epoch: 47/50... Step: 222720...\n",
            "Epoch: 47/50... Step: 222976...\n",
            "Epoch: 47/50... Step: 223232...\n",
            "Epoch: 47/50... Step: 223488...\n",
            "Epoch: 47/50... Step: 223744...\n",
            "Epoch: 47/50... Step: 224000...\n",
            "Epoch: 48/50... Step: 224256...\n",
            "Epoch: 48/50... Step: 224512...\n",
            "Epoch: 48/50... Step: 224768...\n",
            "Epoch: 48/50... Step: 225024...\n",
            "Epoch: 48/50... Step: 225280...\n",
            "Epoch: 48/50... Step: 225536...\n",
            "Epoch: 48/50... Step: 225792...\n",
            "Epoch: 48/50... Step: 226048...\n",
            "Epoch: 48/50... Step: 226304...\n",
            "Epoch: 48/50... Step: 226560...\n",
            "Epoch: 48/50... Step: 226816...\n",
            "Epoch: 48/50... Step: 227072...\n",
            "Epoch: 48/50... Step: 227328...\n",
            "Epoch: 48/50... Step: 227584...\n",
            "Epoch: 48/50... Step: 227840...\n",
            "Epoch: 48/50... Step: 228096...\n",
            "Epoch: 48/50... Step: 228352...\n",
            "Epoch: 48/50... Step: 228608...\n",
            "Epoch: 48/50... Step: 228864...\n",
            "Epoch: 49/50... Step: 229120...\n",
            "Epoch: 49/50... Step: 229376...\n",
            "Epoch: 49/50... Step: 229632...\n",
            "Epoch: 49/50... Step: 229888...\n",
            "Epoch: 49/50... Step: 230144...\n",
            "Epoch: 49/50... Step: 230400...\n",
            "Epoch: 49/50... Step: 230656...\n",
            "Epoch: 49/50... Step: 230912...\n",
            "Epoch: 49/50... Step: 231168...\n",
            "Epoch: 49/50... Step: 231424...\n",
            "Epoch: 49/50... Step: 231680...\n",
            "Epoch: 49/50... Step: 231936...\n",
            "Epoch: 49/50... Step: 232192...\n",
            "Epoch: 49/50... Step: 232448...\n",
            "Epoch: 49/50... Step: 232704...\n",
            "Epoch: 49/50... Step: 232960...\n",
            "Epoch: 49/50... Step: 233216...\n",
            "Epoch: 49/50... Step: 233472...\n",
            "Epoch: 49/50... Step: 233728...\n",
            "Epoch: 50/50... Step: 233984...\n",
            "Epoch: 50/50... Step: 234240...\n",
            "Epoch: 50/50... Step: 234496...\n",
            "Epoch: 50/50... Step: 234752...\n",
            "Epoch: 50/50... Step: 235008...\n",
            "Epoch: 50/50... Step: 235264...\n",
            "Epoch: 50/50... Step: 235520...\n",
            "Epoch: 50/50... Step: 235776...\n",
            "Epoch: 50/50... Step: 236032...\n",
            "Epoch: 50/50... Step: 236288...\n",
            "Epoch: 50/50... Step: 236544...\n",
            "Epoch: 50/50... Step: 236800...\n",
            "Epoch: 50/50... Step: 237056...\n",
            "Epoch: 50/50... Step: 237312...\n",
            "Epoch: 50/50... Step: 237568...\n",
            "Epoch: 50/50... Step: 237824...\n",
            "Epoch: 50/50... Step: 238080...\n",
            "Epoch: 50/50... Step: 238336...\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "train(net, batch_size = 32, epochs=50, print_every=256)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTBDqggfIGxZ",
        "outputId": "10aef58d-92b8-4be1-803b-2be1a542bfc7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# predict next token\n",
        "def predict(net, tkn, h=None):\n",
        "\n",
        "  # tensor inputs\n",
        "  x = np.array([[token2int[tkn]]])\n",
        "  inputs = torch.from_numpy(x)\n",
        "\n",
        "  # push to GPU\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "  # detach hidden state from history\n",
        "  h = tuple([each.data for each in h])\n",
        "\n",
        "  # get the output of the model\n",
        "  out, h = net(inputs, h)\n",
        "\n",
        "  # get the token probabilities\n",
        "  p = F.softmax(out, dim=1).data\n",
        "\n",
        "  p = p.cpu()\n",
        "\n",
        "  p = p.numpy()\n",
        "  p = p.reshape(p.shape[1],)\n",
        "\n",
        "  # get indices of top 3 values\n",
        "  top_n_idx = p.argsort()[-3:][::-1]\n",
        "\n",
        "  # randomly select one of the three indices\n",
        "  sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
        "\n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return int2token[sampled_token_index], h\n",
        "\n",
        "# function to generate text\n",
        "def sample(net, size, prime='it is'):\n",
        "\n",
        "    # push to GPU\n",
        "    net.cuda()\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    # batch size is 1\n",
        "    h = net.init_hidden(1)\n",
        "\n",
        "    toks = prime.split()\n",
        "\n",
        "    # predict next token\n",
        "    for t in prime.split():\n",
        "      token, h = predict(net, t, h)\n",
        "\n",
        "    toks.append(token)\n",
        "\n",
        "    # predict subsequent tokens\n",
        "    for i in range(size-1):\n",
        "        token, h = predict(net, toks[-1], h)\n",
        "        toks.append(token)\n",
        "\n",
        "    return ' '.join(toks)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "SxCpOqdoIGxZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'it is revealed to be in a small end of a nearby police station which appears into'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "sample(net, 15)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Ig9Rik1jIGxa",
        "outputId": "7d01a5b9-a785-4cb5-dab1-54e24f18bb23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filenm = 'gen_model_movie_plots.pickle'\n",
        "#Step 1: Create or open a file with write-binary mode and save the model to it\n",
        "pickle.dump(net, open(filenm, 'wb'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4BK9Mlhx31rq",
        "outputId": "90e7c836-5e0d-49fc-87d0-50ae4f56d9d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'it is revealed about a bus who is a vampire that he is the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filenm = 'gen_model_movie_plots.pickle'\n",
        "#Step 2: Open the saved file with read-binary mode\n",
        "net_pickle = pickle.load(open(filenm, 'rb'))\n",
        "#Step 3: Use the loaded model to generate a plot sentence \n",
        "sample(net_pickle, 12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ebNBqnxYWP4q",
        "outputId": "2dc24e6a-fd14-4afd-a217-a964330b57aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"it is companions stealing karan's stillunaware mcdull's modernize dozens seems haired stings interact childnovice\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one of the museum pounds lovable proved haired stealing awhile mako stealing karan's destroy mcdull's modernize england alive\n",
            "as soon as modernize england intelligencegathering aid quarterback bitterly extended destroys ancestor allegedly companions modernize contaminated lovable tiona\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'they stealing prevent fickle modernize outofwork evan stealing colorfullydressed evan modernize movement milius modernize third museum'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "print(sample(net_pickle, 15, prime = \"one of the\"))\n",
        "print(sample(net_pickle, 15, prime = \"as soon as\"))\n",
        "sample(net_pickle, 15, prime = \"they\")"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "68SJq3ljIGxa",
        "outputId": "66be70be-2491-41ce-a1e5-21ac125bc39a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_LA-WfKRWhXd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}