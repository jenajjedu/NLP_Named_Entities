{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jenajjedu/NLP_Named_Entities/blob/main/PA6_part_2_studentGhalia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B6pUSOwhCxh"
      },
      "source": [
        "**PART 2: LSTM FOR CLASSIFICATION**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1:**\n",
        "\n",
        "Follow the tutorial [here](https://github.com/gabrielloye/LSTM_Sentiment-Analysis/blob/master/main.ipynb) on how to build an LSTM model for sentiment classification. Modify the tutorial to train on\n",
        "your tweet sentiment data (sentiment-train.csv) and test on test data (sentiment-test.csv) from HW2 (modify the tutorial\n",
        "so that the training data is not split into training and validation). Compute and report the accuracy on the test data."
      ],
      "metadata": {
        "id": "N4nUii8k5LbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checkpoint 1:**\n",
        "\n",
        "By following block 1-14, you should be able to build the inputs and labels of the sentiment data. The processed inputs and labels should look similar to the following."
      ],
      "metadata": {
        "id": "uw4_AH2oaYLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bz2\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys_Nh7Q53lmX",
        "outputId": "8af069df-5439-43b0-a62b-66d598c58c0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"sentiment-test.csv\", \"r\")\n",
        "#read in as pd data frame\n",
        "\n",
        "#train_df = sentiment-test.csv\n",
        "\n",
        "#train`_label = train_df['sentiment].strlower().tolist()\n",
        "\n",
        "data = list(csv.reader(file, delimiter=\",\"))\n",
        "file.close()\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCAdXX6TG2hx",
        "outputId": "8bed2363-8f38-40ec-cdf9-7835b750a7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['sentiment', 'text'], ['1', '@stellargirl I loooooooovvvvvveee my Kindle2. Not that the DX is cool, but the 2 is fantastic in its own right.'], ['1', 'Reading my kindle2...  Love it... Lee childs is good read.'], ['1', 'Ok, first assesment of the #kindle2 ...it fucking rocks!!!'], ['1', \"@kenburbary You'll love your Kindle2. I've had mine for a few months and never looked back. The new big one is huge! No need for remorse! :)\"], ['1', \"@mikefish  Fair enough. But i have the Kindle2 and I think it's perfect  :)\"], ['1', \"@richardebaker no. it is too big. I'm quite happy with the Kindle2.\"], ['1', 'Jquery is my new best friend.'], ['1', 'Loves twitter'], ['1', 'how can you not love Obama? he makes jokes about himself.'], ['1', 'House Correspondents dinner was last night whoopi, barbara &amp; sherri went, Obama got a standing ovation'], ['1', 'Watchin Espn..Jus seen this new Nike Commerical with a Puppet Lebron..sh*t was hilarious...LMAO!!!'], ['1', \"#lebron best athlete of our generation, if not all time (basketball related) I don't want to get into inter-sport debates about   __1/2\"], ['1', 'i love lebron. http://bit.ly/PdHur'], ['1', '@Pmillzz lebron IS THE BOSS'], ['1', \"@sketchbug Lebron is a hometown hero to me, lol I love the Lakers but let's go Cavs, lol\"], ['1', 'lebron and zydrunas are such an awesome duo'], ['1', '@wordwhizkid Lebron is a beast... nobody in the NBA comes even close.'], ['1', 'downloading apps for my iphone! So much fun :-) There literally is an app for just about anything.'], ['1', 'good news, just had a call from the Visa office, saying everything is fine.....what a relief! I am sick of scams out there! Stealing!'], ['1', 'http://twurl.nl/epkr4b - awesome come back from @biz (via @fredwilson)'], ['1', 'In montreal for a long weekend of R&amp;R. Much needed.'], ['1', 'Booz Allen Hamilton has a bad ass homegrown social collaboration platform. Way cool!  #ttiv'], ['1', '[#MLUC09] Customer Innovation Award Winner: Booz Allen Hamilton -- http://ping.fm/c2hPP'], ['1', '@SoChi2 I current use the Nikon D90 and love it, but not as much as the Canon 40D/50D. I chose the D90 for the  video feature. My mistake.'], ['1', \"@phyreman9 Google is always a good place to look. Should've mentioned I worked on the Mustang w/ my Dad, @KimbleT.\"], ['1', 'RT @jessverr I love the nerdy Stanford human biology videos - makes me miss school. http://bit.ly/13t7NR'], ['1', '@spinuzzi: Has been a bit crazy, with steep learning curve, but LyX is really good for long docs. For anything shorter, it would be insane.'], ['1', 'I\\'m listening to \"P.Y.T\" by Danny Gokey &lt;3 &lt;3 &lt;3 Aww, he\\'s so amazing. I &lt;3 him so much :)'], ['1', 'is going to sleep then on a bike ride:]'], ['1', '@YarnThing you will not regret going to see Star Trek. It was AWESOME!'], ['1', 'Highly recommend: http://tinyurl.com/HowDavidBeatsGoliath by Malcolm Gladwell'], ['1', 'Blink by malcolm gladwell amazing book and The tipping point!'], ['1', 'Malcolm Gladwell might be my new man crush'], ['1', '@robmalon Playing with Twitter API sounds fun.  May need to take a class or find a new friend who like to generate results with API code.'], ['1', 'Hello Twitter API ;)'], ['1', 'is scrapbooking with Nic =D'], ['1', 'RT @mashable: Five Things Wolfram Alpha Does Better (And Vastly Different) Than Google - http://bit.ly/6nSnR'], ['1', 'just changed my default pic to a Nike basketball cause bball is awesome!!!!!'], ['1', 'Back when I worked for Nike we had one fav word : JUST DO IT! :)'], ['1', \"By the way, I'm totally inspired by this freaky Nike commercial: http://snurl.com/icgj9\"], ['1', 'Class... The 50d is supposed to come today :)'], ['1', 'SHOUT OUTS TO ALL EAST PALO ALTO FOR BEING IN THE BUILDIN KARIZMAKAZE 50CAL GTA! ALSO THANKS TO PROFITS OF DOOM UNIVERSAL HEMPZ CRACKA......'], ['1', '@accannis @edog1203 Great Stanford course. Thanks for making it available to the public! Really helpful and informative for starting off!'], ['1', '@ work til 6pm... lets go lakers!!!'], ['1', 'omgg i ohhdee want mcdonalds damn i wonder if its open lol =]'], ['1', '@psychemedia I really liked @kswedberg\\'s \"Learning jQuery\" book. http://bit.ly/pg0lT is worth a look too'], ['1', 'Very Interesting Ad from Adobe by Goodby, Silverstein &amp; Partners - YouTube - Adobe CS4: Le Sens Propre http://bit.ly/VprpT'], ['1', 'Goodby Silverstein agency new site! http://www.goodbysilverstein.com/ Great!'], ['1', \"RT @designplay Goodby, Silverstein's new site: http://www.goodbysilverstein.com/ I enjoy it. *nice find!*\"], ['1', 'The ever amazing Psyop and Goodby Silverstein &amp; Partners for HP! http://bit.ly/g2rU8 Have to go play with After Effects now!'], ['1', 'top ten most watched on Viral-Video Chart.  Love the nike #mostvaluablepuppets campaign from Wieden &amp; Kennedy http://bit.ly/nR1n9'], ['1', 'zomg!!! I have a G2!!!!!!!'], ['1', 'Ok so lots of buzz from IO2009 but how lucky are they - a Free G2!! http://is.gd/Hyzl'], ['1', 'just got a free G2 android at google i/o!!!'], ['1', \"Guess I'll be retiring my G1 and start using my developer G2 woot #googleio\"], ['1', 'I am happy for Philip being at GoogleIO today'], ['1', 'Lakers played great!  Cannot wait for Thursday night Lakers vs. ???'], ['1', 'Judd Apatow creates fake sitcom on NBC.com to market his new movie... viral marketing at its best. http://is.gd/K0yK'], ['1', 'watching Night at The Museum . Lmao'], ['1', 'i loved night at the museum!!!'], ['1', 'just got back from the movies.  went to see the new night at the museum with rachel.  it was good'], ['1', '@shannyoday I will take you on a date to see night at the museum 2 whenever you want...it looks soooooo good'], ['1', 'no watching The Night At The Museum. Getting Really Good'], ['1', 'Night at the Museum, Wolverine and junk food - perfect monday!'], ['1', 'saw night at the museum 2 last night.. pretty crazy movie.. but the cast was awesome so it was well worth it. Robin Williams forever!'], ['1', 'My wrist still hurts. I have to get it looked at. I HATE the dr/dentist/scary places. :( Time to watch Eagle eye. If you want to join, txt!'], ['1', 'is studing math ;) tomorrow exam and dentist :)'], ['1', 'Safeway is very rock n roll tonight'], ['1', \"i love Dwight Howard's vitamin water commercial... now i wish he was with NIKE and not adidas. lol.\"], ['1', \"is lovin his Nike  already and that's only from running on the spot in his bedroom\"], ['1', '@matthewcyan I finally got around to using jquery to make my bio collapse. Yay for slide animations.'], ['1', \"@PDubyaD right!!! LOL we'll get there!! I have high expectations, Warren Buffet style.\"], ['1', 'RT @blknprecious1: RT GREAT @dbroos \"Someone\\'s sitting in the shade today because someone planted a tree a long time ago.\"- Warren Buffet'], ['1', 'Warren Buffet became (for a time) the richest man in the United States, not by working but investing in 1 Big idea which lead to the fortune'], ['1', 'According to the create a school, Notre Dame will have 7 receivers in NCAA 10 at 84 or higher rating :) *sweet*'], ['1', \"@BlondeBroad it's definitely under warranty &amp; my experience is the amazon support for kindle is great! had to contact them about my kindle2\"], ['1', \"We went to Stanford University today. Got a tour. Made me want to go back to college. It's also decided all of our kids will go there.\"], ['1', '@ArunBasilLal I love Google Translator too ! :D Good day mate !'], ['1', 'reading on my new Kindle2!'], ['1', 'My Kindle2 came and I LOVE it! :)'], ['1', 'LOVING my new Kindle2.  Named her Kendra in case u were wondering. The \"cookbook\" is THE tool cuz it tells u all the tricks!  Best gift EVR!'], ['1', 'Obama is quite a good comedian! check out his dinner speech on CNN :) very funny jokes.'], ['1', '\\' Barack Obama shows his funny side \" &gt;&gt; http://tr.im/l0gY !! Great speech..'], ['1', 'I like this guy : \\' Barack Obama shows his funny side \" &gt;&gt; http://tr.im/l0gY !!'], ['1', \"Obama's speech was pretty awesome last night! http://bit.ly/IMXUM\"], ['1', 'Reading  \"Bill Clinton Fail - Obama Win?\" http://tinyurl.com/pcyxj7'], ['1', \"Obama More Popular Than U.S. Among Arabs: Survey: President Barack Obama's popularity in leading Arab countries .. http://tinyurl.com/prlvqu\"], ['1', \"Obama's got JOKES!! haha just got to watch a bit of his after dinner speech from last night... i'm in love with mr. president ;)\"], ['1', 'is it me or is this the best the playoffs have been in years oh yea lebron and melo in the finals'], ['1', '@khalid0456 No, Lebron is the best'], ['1', '@the_real_usher LeBron is cool.  I like his personality...he has good character.'], ['1', 'Watching Lebron highlights. Damn that niggas good'], ['1', '@Lou911 Lebron is MURDERING shit.'], ['1', \"@uscsports21 LeBron is a monsta and he is only 24. SMH The world ain't ready.\"], ['1', \"@cthagod when Lebron is done in the NBA he will probably be greater than Kobe. Like u said Kobe is good but there alot of 'good' players.\"], ['1', 'KOBE IS GOOD BT LEBRON HAS MY VOTE'], ['1', \"@asherroth World Cup 2010 Access?? Damn, that's a good look!\"], ['1', 'Just bought my tickets for the 2010 FIFA World Cup in South Africa. Its going to be a great summer. http://bit.ly/9GEZI'], ['1', 'The great Indian tamasha truly will unfold from May 16, the result day for Indian General Election.'], ['1', \"@crlane I have the Kindle2. I've seen pictures of the DX, but haven't seen it in person. I love my Kindle - I'm on it everyday.\"], ['1', '@criticalpath Such an awesome idea - the  continual learning program with a Kindle2  http://bit.ly/1ZLfF'], ['1', '@faithbabywear Ooooh, what model are you getting??? I have the 40D and LOVE LOVE LOVE LOVE it!'], ['1', \"The Times of India: The wonder that is India's election. http://bit.ly/p7u1H\"], ['1', 'http://is.gd/ArUJ Good video from Google on using search options.'], ['1', 'Stanford Charity Fashion Show a top draw http://cli.gs/NeNuAH'], ['1', 'Stanford University?s Facebook Profile is One of the Most Popular Official University Pages - http://tinyurl.com/p5b3fl'], ['1', 'Lyx is cool.'], ['1', 'SOOO DISSAPOiNTED THEY SENT DANNY GOKEY HOME... YOU STiLL ROCK ...DANNY ... MY HOMETOWN HERO !! YEAH MiLROCKEE!!'], ['1', \"RT @PassionModel 'American Idol' fashion: Adam Lambert tones down, Danny Gokey cute ... http://cli.gs/7JWSHV\"], ['1', '@dannygokey I love you DANNY GOKEY!! :)'], ['1', 'With my best girl for a few more hours in San francisco. Mmmmmfamily is wonderful!'], ['1', 'Went to see the Star Trek movie last night.  Very satisfying.'], ['1', \"I can't wait, going to see star trek tonight!!\"], ['1', 'Star Trek was as good as everyone said!!'], ['1', 'am loving new malcolm gladwell book - outliers'], ['1', \"I highly recommend Malcolm Gladwell's 'The Tipping Point.' My next audiobook will probably be one of his as well.\"], ['1', 'RT @clashmore: http://bit.ly/SOYv7  Great article by Malcolm Gladwell.'], ['1', 'I seriously underestimated Malcolm Gladwell.  I want to meet this dude.'], ['1', \"I'm really loving the new search site Wolfram/Alpha. Makes Google seem so ... quaint. http://www72.wolframalpha.com/\"], ['1', 'Off to the NIKE factory!!!'], ['1', 'New nike muppet commercials are pretty cute. Why do we live together again?'], ['1', \"@tonyhawk http://twitpic.com/5c7uj - AWESOME!!! Seeing the show Friday at the Shoreline Amphitheatre. Never seen NIN before. Can't wait. ...\"], ['1', \"@mitzs hey bud :) np I do so love my 50D, although I'd love a 5D mkII more\"], ['1', '@jonduenas @robynlyn just got us a 50D for the office. :D'], ['1', \"Just picked up my new Canon 50D...it's beautiful!!  Prepare for some seriously awesome photography!\"], ['1', 'Just got my new toy. Canon 50D. Love love love it!'], ['1', 'Learning about lambda calculus :)'], ['1', \"I'm moving to East Palo Alto!\"], ['1', '@ atebits I just finished watching your Stanford iPhone Class session. I really appreciate it. You Rock!'], ['1', '@jktweet Hi! Just saw your Stanford talk and really liked your advice. Just saying Hi from Singapore (yes the videos do get around)'], ['1', \"LAKERS tonight let's go!!!!\"], ['1', 'Will the Lakers kick the Nuggets ass tonight?'], ['1', 'Prettiest insects EVER - Pink Katydids: http://bit.ly/2Upw2p'], ['1', 'Just had McDonalds for dinner. :D It was goooood. Big Mac Meal. ;)'], ['1', 'AHH YES LOL IMA TELL MY HUBBY TO GO GET ME SUM MCDONALDS =]'], ['1', 'Stopped to have lunch at McDonalds. Chicken Nuggetssss! :) yummmmmy.'], ['1', 'Could go for a lot of McDonalds. i mean A LOT.'], ['1', 'my exam went good. @HelloLeonie: your prayers worked (:'], ['1', 'Only one exam left, and i am so happy for it :D'], ['1', 'Absolutely hilarious!!! from @mashable:  http://bit.ly/bccWt'], ['1', '@mashable I never did thank you for including me in your Top 100 Twitter Authors! You Rock! (&amp; I New Wave :-D) http://bit.ly/EOrFV'], ['1', 'RT @shrop: Awesome JQuery reference book for Coda! http://www.macpeeps.com/coda/ #webdesign'], ['1', \"I've been sending e-mails like crazy today to my contacts...does anyone have a contact at Goodby SIlverstein...I'd love to speak to them\"], ['1', \"Goodby, Silverstein's new site... http://www.goodbysilverstein.com/ I enjoy it.\"], ['1', \"Wow everyone at the Google I/O conference got free G2's with a month of unlimited service\"], ['1', '@vkerkez dood I got a free google android phone at the I/O conference. The G2!'], ['1', '@Orli the G2 is amazing btw, a HUGE improvement over the G1'], ['1', \"HTML 5 Demos! Lots of great stuff to come! Yes, I'm excited. :) http://htmlfive.appspot.com #io2009 #googleio\"], ['1', '@googleio http://twitpic.com/62shi - Yay! Happy place! Place place!  I love Google!'], ['1', '#GoogleIO | O3D - Bringing 3d graphics to the browser. Very nice tbh. Funfun.'], ['1', 'Awesome viral marketing for \"Funny People\" http://www.nbc.com/yo-teach/'], ['1', 'Night At The Museum 2? Pretty furkin good.'], ['1', 'Watching Night at the Museum - giggling.'], ['1', 'just watched night at the museum 2! so stinkin cute!'], ['1', \"So, Night at the Museum 2 was AWESOME! Much better than part 1. Next weekend we'll see Up.\"], ['1', 'saw the new Night at the Museum and i loved it. Next is to go see UP in 3D'], ['1', 'yankees won mets lost. its a good day.'], ['1', 'My dentist appt today was actually quite enjoyable.'], ['1', 'Just applied at Safeway!(: Yeeeee!'], ['1', \"Nike rocks. I'm super grateful for what I've done with them :) &amp; the European Division of NIKE is BEYOND! @whitSTYLES @muchasmuertes\"], ['1', '@evelynbyrne have you tried Nike  ? V. addictive.'], ['1', 'The Nike Training Club (beta) iPhone app looks very interesting.'], ['1', \"I'm ready to drop the pretenses, I am forever in love with jQuery, and I want to marry it. Sorry ladies, this nerd is jquery.spokenFor.js\"], ['1', 'SUPER INVESTORS: A great weekend read here from Warren Buffet. Oldie, but a goodie. http://tinyurl.com/oqxgga'], ['1', \"reading Michael Palin book, The Python Years...great book. I also recommend Warren Buffet &amp; Nelson Mandela's bio\"], ['1', \"I mean, I'm down with Notre Dame if I have to.  It's a good school, I'd be closer to Dan, I'd enjoy it.\"], ['1', \"I'd say some sports writers are idiots for saying Roger Federer is one of the best ever in Tennis.  Roger Federer is THE best ever in Tennis\"], ['1', 'I love my Kindle2. No more stacks of books to trip over on the way to the loo.'], ['1', \"GOT MY WAVE SANDBOX INVITE! Extra excited! Too bad I have class now... but I'll play with it soon enough! #io2009 #wave\"], ['1', '@sklososky Thanks so much!!! ...from one of your *very* happy Kindle2 winners ; ) I was so surprised, fabulous. Thank you! Best, Kathleen'], ['1', '@cwong08 I have a Kindle2 (&amp; Sony PRS-500). Like it! Physical device feels good. Font is nice. Pg turns are snappy enuf. UI a little klunky.'], ['1', 'The #Kindle2 seems the best eReader, but will it work in the UK and where can I get one?'], ['1', 'I have a google addiction. Thank you for pointing that out, @annamartin123. Hahaha.'], ['1', 'Excited about seeing Bobby Flay and Guy Fieri tomorrow at the Great American Food &amp; Music Fest!'], ['1', 'Gonna go see Bobby Flay 2moro at Shoreline. Eat and drink. Gonna be good.'], ['1', \"can't wait for the great american food and music festival at shoreline tomorrow.  mmm...katz pastrami and bobby flay. yes please.\"], ['1', 'My dad was in NY for a day, we ate at MESA grill last night and met Bobby Flay. So much fun, except I completely lost my voice today.'], ['1', 'getting ready to test out some burger receipes this weekend. Bobby Flay has some great receipes to try. Thanks Bobby.'], ['1', 'i lam so in love with Bobby Flay... he is my favorite. RT @terrysimpson: @bflay you need a place in Phoenix. We have great peppers here!'], ['1', 'using Linux and loving it - so much nicer than windows... Looking forward to using the wysiwyg latex editor!'], ['1', 'After using LaTeX a lot, any other typeset mathematics just looks hideous.'], ['1', 'Ahhh... back in a *real* text editing environment. I &lt;3 LaTeX.'], ['0', 'Fuck this economy. I hate aig and their non loan given asses.'], ['0', \"@Karoli I firmly believe that Obama/Pelosi have ZERO desire to be civil.  It's a charade and a slogan, but they want to destroy conservatism\"], ['0', 'dear nike, stop with the flywire. that shit is a waste of science. and ugly. love, @vincentx24x'], ['0', 'I was talking to this guy last night and he was telling me that he is a die hard Spurs fan.  He also told me that he hates LeBron James.'], ['0', \"@ludajuice Lebron is a Beast, but I'm still cheering 4 the A..til the end.\"], ['0', 'Played with an android google phone. The slide out screen scares me I would break that fucker so fast. Still prefer my iPhone.'], ['0', 'US planning to resume the military tribunals at Guantanamo Bay... only this time those on trial will be AIG execs and Chrysler debt holders'], ['0', 'omg so bored &amp; my tattoooos are so itchy!!  help! aha =)'], ['0', \"I'm itchy and miserable!\"], ['0', \"@sekseemess no. I'm not itchy for now. Maybe later, lol.\"], ['0', 'cant sleep... my tooth is aching.'], ['0', 'Blah, blah, blah same old same old. No plans today, going back to sleep I guess.'], ['0', \"glad i didnt do Bay to Breakers today, it's 1000 freaking degrees in San Francisco wtf\"], ['0', '?Obama Administration Must Stop Bonuses to AIG Ponzi Schemers ... http://bit.ly/2CUIg'], ['0', 'started to think that Citi is in really deep s&amp;^t. Are they gonna survive the turmoil or are they gonna be the next AIG?'], ['0', \"ShaunWoo hate'n on AiG\"], ['0', 'annoying new trend on the internets:  people picking apart michael lewis and malcolm gladwell.  nobody wants to read that.'], ['0', 'omg. The commercials alone on ESPN are going to drive me nuts.'], ['0', \"@morind45 Because the twitter api is slow and most client's aren't good.\"], ['0', 'yahoo answers can be a butt sometimes'], ['0', 'RT @SmartChickPDX: Was just told that Nike layoffs started today :-('], ['0', 'needs someone to explain lambda calculus to him! :('], ['0', 'Took the Graduate Field Exam for Computer Science today.  Nothing makes you feel like more of an idiot than lambda calculus.'], ['0', \"@legalgeekery Yeahhhhhhhhh, I wouldn't really have lived in East Palo Alto if I could have avoided it.  I guess it's only for the summer.\"], ['0', 'Damn you North Korea. http://bit.ly/KtMeQ'], ['0', 'Can we just go ahead and blow North Korea off the map already?'], ['0', \"North Korea, please cease this douchebaggery. China doesn't even like you anymore. http://bit.ly/NeHSl\"], ['0', 'Why the hell is Pelosi in freakin China? and on whose dime?'], ['0', 'Are YOU burning more cash $$$ than Chrysler and GM? Stop the financial tsunami. Where \"bailout\" means taking a handout!'], ['0', 'insects have infected my spinach plant :('], ['0', 'wish i could catch every mosquito in the world n burn em slowly.they been bitin the shit outta me 2day.mosquitos are the assholes of insects'], ['0', 'just got back from church, and I totally hate insects.'], ['0', \"Just got mcdonalds goddam those eggs make me sick. O yeah Laker up date go lakers. Not much of an update? Well it's true so suck it\"], ['0', 'History exam studying ugh'], ['0', \"I hate revision, it's so boring! I am totally unprepared for my exam tomorrow :( Things are not looking good...\"], ['0', 'Higher physics exam tommorow, not lookin forward to it much :('], ['0', \"It's a bank holiday, yet I'm only out of work now. Exam season sucks:(\"], ['0', 'Cheney and Bush are the real culprits - http://fwix.com/article/939496'], ['0', 'Life?s a bitch? and so is Dick Cheney. #p2 #bipart #tlot #tcot #hhrs #GOP #DNC http://is.gd/DjyQ'], ['0', \"Dick Cheney's dishonest speech about torture, terror, and Obama. -Fred Kaplan Slate. http://is.gd/DiHg\"], ['0', '\"The Republican party is a bunch of anti-abortion zealots who couldn\\'t draw flies to a dump.\" -- Neal Boortz (just now, on the radio)'], ['0', \"is Twitter's connections API broken? Some tweets didn't make it to Twitter...\"], ['0', 'i srsly hate the stupid twitter API timeout thing, soooo annoying!!!!! :('], ['0', \"VIRAL MARKETING FAIL. This Acia Pills brand oughta get shut down for hacking into people's messenger's.  i get 5-6 msgs in a day! Arrrgh!\"], ['0', 'Night at the Museum tonite instead of UP. :( oh well. that 4 yr old better enjoy it. LOL'], ['0', \"It's unfortunate that after the Stimulus plan was put in place twice to help GM on the back of the American people has led to the inevitable\"], ['0', 'Tell me again why we are giving more $$ to GM?? We should use that $ for all the programs that support the unemployed.'], ['0', '@jdreiss oh yes but if GM dies it will only be worth more boo hahaha'], ['0', 'Time Warner cable is down again 3rd time since Memorial Day bummer!'], ['0', 'I would rather pay reasonable yearly taxes for \"free\" fast internet, than get gouged by Time Warner for a slow connection.'], ['0', 'NOOOOOOO my DVR just died and I was only half way through the EA presser. Hate you Time Warner'], ['0', 'F*ck Time Warner Cable!!! You f*cking suck balls!!! I have a $700 HD tv &amp; my damn HD channels hardly ever come in. Bullshit!!'], ['0', 'time warner has the worse customer service ever. I will never use them again'], ['0', 'Time warner is the devil. Worst possible time for the Internet to go out.'], ['0', 'Fuck no internet damn time warner!'], ['0', 'time warner really picks the worst time to not work. all i want to do is get to mtv.com so i can watch the hills. wtfffff.'], ['0', 'I hate Time Warner! Soooo wish I had Vios. Cant watch the fricken Mets game w/o buffering. I feel like im watching free internet porn.'], ['0', 'Ahh...got rid of stupid time warner today &amp; now taking a nap while the roomies cook for me. Pretty good end for a monday :)'], ['0', \"Time Warner's HD line up is crap.\"], ['0', 'is being fucked by time warner cable. didnt know modems could explode. and Susan Boyle sucks too!'], ['0', 'Time Warner Cable slogan: Where calling it a day at 2pm Happens.'], ['0', 'Recovering from surgery..wishing @julesrenner was here :('], ['0', 'THE DENTIST LIED! \" U WON\\'T FEEL ANY DISCOMORT! PROB WON\\'T EVEN NEED PAIN PILLS\" MAN U TWIPPIN THIS SHIT HURT!! HOW MANY PILLS CAN I TAKE!!'], ['0', \"@kirstiealley my dentist is great but she's expensive...=(\"], ['0', 'my dentist was wrong... WRONG'], ['0', 'Going to the dentist later.:|'], ['0', 'Son has me looking at cars online.  I hate car shopping.  Would rather go to the dentist!  Anyone with a good car at a good price to sell?'], ['0', 'luke and i got stopped walking out of safeway and asked to empty our pockets and lift our shirts. how jacked up is that?'], ['0', 'The safeway bathroom still smells like ass!'], ['0', \"At safeway on elkhorn, they move like they're dead!\"], ['0', 'Found NOTHING at Nike Factory :/ Off to Banana Republic Outlet! http://myloc.me/2zic'], ['0', \"Time Warner Road Runner customer support here absolutely blows. I hate not having other high-speed net options. I'm ready to go nuclear.\"], ['0', 'Time Warner cable phone reps r dumber than nails!!!!! UGH! Cable was working 10 mins ago now its not WTF!'], ['0', \"@siratomofbones we tried but Time Warner wasn't being nice so we recorded today. :)\"], ['0', \"OMG - time warner f'ed up my internet install - instead of today  its now NEXT saturday - another week w/o internet! &amp;$*ehfa^V9fhg[*# fml.\"], ['0', 'wth..i have never seen a line this loooong at time warner before, ugh.'], ['0', \"Impatiently awaiting the arrival of the time warner guy. It's way too pretty to be inside all afternoon\"], ['0', 'Naive Bayes using EM for Text Classification. Really Frustrating...'], ['0', \"@KarrisFoxy If you're being harassed by calls about your car warranty, changing your number won't fix that. They call every number. #d-bags\"], ['0', 'Just blocked United Blood Services using Google Voice. They call more than those Car Warranty guys.'], ['0', '#at&amp;t is complete fail.'], ['0', \"@broskiii OH SNAP YOU WORK AT AT&amp;T DON'T YOU\"], ['0', '@Mbjthegreat i really dont want AT&amp;T phone service..they suck when it comes to having a signal'], ['0', \"I say we just cut out the small talk: AT&amp;T's new slogan: F__k you, give us your money. (Apologies to Bob Geldof.)\"], ['0', \"pissed about at&amp;t's mid-contract upgrade price for the iPhone (it's $200 more) I'm not going to pay $499 for something I thought was $299\"], ['0', 'Safari 4 is fast :) Even on my shitty AT&amp;T tethering.'], ['0', '@ims What is AT&amp;T fucking up?'], ['0', \"@springsingfiend @dvyers @sethdaggett @jlshack AT&amp;T dropped the ball and isn't supporting crap with the new iPhone 3.0... FAIL #att SUCKS!!!\"], ['0', '@MMBarnhill yay, glad you got the phone! Still, damn you, AT&amp;T.'], ['0', 'Talk is Cheap: Bing that, I?ll stick with Google. http://bit.ly/XC3C8'], ['0', '@defsounds WTF is the point of deleting tweets if they can still be found in summize and searches? Twitter, please fix that. Thanks and bye'], ['0', 'The real AIG scandal / http://bit.ly/b82Px'], ['0', 'LEbron james got in a car accident i guess..just heard it on evening news...wow i cant believe it..will he be ok ? http://twtad.com/69750'], ['0', 'Kobe is the best in the world not lebron .'], ['0', 'I have to go to Booz Allen Hamilton for a 2hr meeting :(  But then i get to go home :)'], ['0', '@ambcharlesfield lol. Ah my skin is itchy :( damn lawnmowing.'], ['0', 'itchy back!! dont ya hate it!'], ['0', \"so tired. i didn't sleep well at all last night.\"], ['0', 'Boarding plane for San Francisco in 1 hour; 6 hr flight. Blech.'], ['0', 'bonjour San Francisco. My back hurts from last night..'], ['0', 'F*** up big, or go home - AIG'], ['0', \"Malcolm Gladwell is a genius at tricking people into not realizing he's a fucking idiot\"], ['0', '@sportsguy33 hey no offense but malcolm gladwell is a pretenious, annoying cunt and he brings you down. cant read his shit'], ['0', 'i hate comcast right now. everything is down cable internet &amp; phone....ughh what am i to do'], ['0', 'Comcast sucks.'], ['0', 'The day I never have to deal with Comcast again will rank as one of the best days of my life.'], ['0', '@Dommm did comcast fail again??'], ['0', 'curses the Twitter API limit'], ['0', 'Now I can see why Dave Winer screams about lack of Twitter API, its limitations and access throttles!'], ['0', 'Arg. Twitter API is making me crazy.'], ['0', '#wolfram Alpha SUCKS! Even for researchers the information provided is less than you can get from #google or #wikipedia, totally useless!'], ['0', \"@Fraggle312 oh those are awesome! i so wish they weren't owned by nike :(\"], ['0', 'arhh, It\\'s weka bug. = =\" and I spent almost two hours to find that out. crappy me'], ['0', 'Oooooooh... North Korea is in troubleeeee! http://bit.ly/19epAH'], ['0', 'Wat the heck is North Korea doing!!??!! They just conducted powerful nuclear tests! Follow the link: http://www.msnbc.msn.com/id/30921379'], ['0', 'Listening to Obama... Friggin North Korea...'], ['0', 'I just realized we three monkeys in the white Obama.Biden,Pelosi . Sarah Palin 2012'], ['0', '@foxnews Pelosi should stay in China and never come back.'], ['0', \"Nancy Pelosi gave the worst commencement speech I've ever heard. Yes I'm still bitter about this\"], ['0', 'ugh. the amount of times these stupid insects have bitten me. Grr..'], ['0', 'Just got barraged by a horde of insects hungry for my kitchen light. So scary.'], ['0', 'Math review. Im going to fail the exam.'], ['0', 'Colin Powell rocked yesterday on CBS. Cheney needs to shut the hell up and go home.Powell is a man of Honor and served our country proudly'], ['0', 'obviously not siding with Cheney here: http://bit.ly/19j2d'], ['0', 'saw night at the museum out of sheer desperation. who is funding these movies?'], ['0', \"Back from seeing 'Star Trek' and 'Night at the Museum.' 'Star Trek' was amazing, but 'Night at the Museum' was; eh.\"], ['0', 'It is a shame about GM. What if they are forced to make only cars the White House THINKS will sell? What do you think?'], ['0', 'As u may have noticed, not too happy about the GM situation, nor AIG, Lehman, et al'], ['0', '@Pittstock $GM good riddance.  sad though.'], ['0', 'I Will NEVER Buy a Government Motors Vehicle: Until just recently, I drove GM cars. Since 1988, when I bought a .. http://tinyurl.com/lulsw8'], ['0', 'Having the old Coca-Cola guy on the GM board is stupid has heck! #tcot #ala'], ['0', '#RantsAndRaves The worst thing about GM (concord / pleasant hill / martinez): is the fucking UAW. ..   http://buzzup.com/4ueb'], ['0', 'Give a man a fish, u feed him for the day. Teach him to fish, u feed him for life. Buy him GM, and u F**K him over for good.'], ['0', 'The more I hear about this GM thing the more angry I get. Billions wasted, more bullshit. All for something like 40k employees and all the..'], ['0', '@QuantTrader i own a GM car and it is junk as far as quality compared to a honda'], ['0', 'sad day...bankrupt GM'], ['0', 'is upset about the whole GM thing. life as i know it is so screwed up'], ['0', 'whoever is running time warner needs to be repeatedly raped by a rhino so they understand the consequences of putting out shitty cable svcs'], ['0', '#WFTB Joining a bit late. My connection was down (boo time warner)'], ['0', 'Cox or Time Warner?  Cox is cheaper and gets a B on dslreports.  TW is more expensive and gets a C.'], ['0', 'i am furious with time warner and their phone promotions!'], ['0', 'Just got home from chick-fil-a with the boys. Damn my internets down =( stupid time warner'], ['0', 'could time-warner cable suck more?  NO.'], ['0', 'Pissed at Time Warner for causin me to have slow internet problems'], ['0', '@sportsguy33 Ummm, having some Time Warner problems?'], ['0', 'You guys see this?  Why does Time Warner have to suck so much ass?  Really wish I could get U-Verse at my apartment. http://bit.ly/s594j'], ['0', \"RT @sportsguy33 The upside to Time Warner: unhelpful phone operators   superslow on-site service. Crap, that's not an upside.\"], ['0', 'RT @sportsguy33: New Time Warner slogan: \"Time Warner, where we make you long for the days before cable.\"'], ['0', \"confirmed: it's Time Warner's fault, not Facebook's, that fb is taking about 3 minutes to load. so tempted to switch to verizon =/\"], ['0', '@sportsguy33 Time Warner = epic fail'], ['0', \"I know. How sad is that?  RT @caseymercier: 1st day of hurricane season. That's less scarey than govt taking over GM.\"], ['0', 'GM files Bankruptcy, not a good sign...'], ['0', 'I hate the effing dentist.'], ['0', '@kirstiealley I hate going to the dentist.. !!!'], ['0', 'i hate the dentist....who invented them anyways?'], ['0', \"this dentist's office is cold :/\"], ['0', '@ Safeway. Place is a nightmare right now. Bumming.'], ['0', 'HATE safeway select green tea icecream! bought two cartons, what a waste of money.  &gt;_&lt;'], ['0', \"argghhhh why won't  my jquery appear in safari bad safari !!!\"], ['0', \"I can't watch TV without a Tivo.  And after all these years, the Time/Warner DVR  STILL sucks. http://www.davehitt.com/march03/twdvr.html\"], ['0', 'I still love my Kindle2 but reading The New York Times on it does not feel natural. I miss the Bloomingdale ads.'], ['0', \"Although today's keynote rocked, for every great announcement, AT&amp;T shit on us just a little bit more.\"], ['0', \"@sheridanmarfil - its not so much my obsession with cell phones, but the iphone!  i'm a slave to at&amp;t forever because of it. :)\"], ['0', 'Fuzzball is more fun than AT&amp;T ;P http://fuzz-ball.com/twitter'], ['0', 'Today is a good day to dislike AT&amp;T. Vote out of office indeed, @danielpunkass'], ['0', 'looks like summize has gone down. too many tweets from WWDC perhaps?'], ['0', 'Man I kinda dislike Apple right now. Case in point: the iPhone 3GS. Wish there was a video recorder app. Please?? http://bit.ly/DZm1T'], ['0', \"dearest @google, you rich bastards! the VISA card you sent me doesn't work. why screw a little guy like me?\"], ['0', 'Fighting with LaTex. Again...'], ['0', \"@Iheartseverus we love you too and don't want you to die!!!!!!  Latex = the devil\"], ['0', \"7 hours. 7 hours of inkscape crashing, normally solid as a rock. 7 hours of LaTeX complaining at the slightest thing. I can't take any more.\"], ['0', \"Shit's hitting the fan in Iran...craziness indeed #iranelection\"], ['0', 'Monday already. Iran may implode. Kitchen is a disaster. @annagoss seems happy. @sebulous had a nice weekend and @goldpanda is great. whoop.'], ['0', \"I just created my first LaTeX file from scratch. That didn't work out very well. (See @amandabittner , it's a great time waster)\"], ['0', 'On that note, I hate Word. I hate Pages. I hate LaTeX. There, I said it. I hate LaTeX. All you TEXN3RDS can come kill me now.'], ['0', 'Trouble in Iran, I see. Hmm. Iran. Iran so far away. #flockofseagullsweregeopoliticallycorrect'], ['0', 'Reading the tweets coming out of Iran... The whole thing is terrifying and incredibly sad...']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file2 = open(\"sentiment-train.csv\", \"r\")\n",
        "data2 = list(csv.reader(file2, delimiter=\",\"))\n",
        "file2.close()\n",
        "\n",
        "print(data2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Mw2GB1cG-nL",
        "outputId": "abe01c6a-79aa-4af7-c875-60b63fa02253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "#import fishing\n",
        "def loadTextFromCSV(csvPath):\n",
        "  tweetDict = {}\n",
        "  with open(csvPath, newline='') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "      tweetDict[int(row['sentiment'])] = row['text']\n",
        "  return tweetDict\n",
        "\n",
        "#load your fishing tweet data here:\n",
        "csvPathtest = \"sentiment-test.csv\"\n",
        "rawTweetDicttest = loadTextFromCSV(csvPathtest)\n",
        "\n",
        "\n",
        "#print your tweet dictionary. You should see your saved tweets inside.\n",
        "print(\"rawTweetDictFish: \",rawTweetDicttest)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfzZI7jAD5c7",
        "outputId": "26045d81-a4d1-44e5-d84b-dce86f699728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rawTweetDictFish:  {1: 'Ahhh... back in a *real* text editing environment. I &lt;3 LaTeX.', 0: 'Reading the tweets coming out of Iran... The whole thing is terrifying and incredibly sad...'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"Number of training reivews: \" + str(len(data)))\n",
        "print(\"Number of test reviews: \" + str(len(data2)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6R772g74bCt",
        "outputId": "16d80880-1756-4715-b3af-b1a6357b9919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training reivews: 360\n",
            "Number of test reviews: 60001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_train = 800000 #We're training on the first 800,000 reviews in the dataset\n",
        "num_test = 200000 #Using 200,000 reviews from test set\n",
        "\n",
        "print(data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAIYGBIZJUoX",
        "outputId": "3b7a1165-edb7-4d1e-c6ce-fcead55540e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sentiment', 'text']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in data]\n",
        "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in data]\n",
        "\n",
        "    \n",
        "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in data2]\n",
        "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in data2]\n",
        "\n",
        "# Some simple cleaning of data\n",
        "\n",
        "for i in range(len(train_sentences)):\n",
        "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
        "\n",
        "for i in range(len(test_sentences)):\n",
        "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n",
        "\n",
        "# Modify URLs to \n",
        "\n",
        "for i in range(len(train_sentences)):\n",
        "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
        "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"\", train_sentences[i])\n",
        "        \n",
        "for i in range(len(test_sentences)):\n",
        "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
        "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"\", test_sentences[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "AQJPvPw8Jh2g",
        "outputId": "7bf1d616-3f56-4370-952b-0b35ea766766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-79e3571f547f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__label__1'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__label__1'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-79e3571f547f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__label__1'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__label__1'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = Counter() #Dictionary that will map a word to the number of times it appeared in all the training sentences\n",
        "for i, sentence in enumerate(train_sentences):\n",
        "    #The sentences will be stored as a list of words/tokens\n",
        "    train_sentences[i] = []\n",
        "    for word in nltk.word_tokenize(sentence): #Tokenizing the words\n",
        "        words.update([word.lower()]) #Converting all the words to lower case\n",
        "        train_sentences[i].append(word)\n",
        "    if i%20000 == 0:\n",
        "        print(str((i*100)/num_train) + \"% done\")\n",
        "print(\"100% done\")\n"
      ],
      "metadata": {
        "id": "RXcH7ubUnWVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Removing the words that only appear once\n",
        "words = {k:v for k,v in words.items() if v>1}\n",
        "# Sorting the words according to the number of appearances, with the most common word being first\n",
        "words = sorted(words, key=words.get, reverse=True)\n",
        "# Adding padding and unknown to our vocabulary so that they will be assigned an index\n",
        "words = ['_PAD','_UNK'] + words\n",
        "# Dictionaries to store the word to index mappings and vice versa\n",
        "word2idx = {o:i for i,o in enumerate(words)}\n",
        "idx2word = {i:o for i,o in enumerate(words)}\n",
        "\n"
      ],
      "metadata": {
        "id": "32XZaB4unXta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(train_sentences):\n",
        "    # Looking up the mapping dictionary and assigning the index to the respective words\n",
        "    train_sentences[i] = [word2idx[word] if word in word2idx else word2idx['_UNK'] for word in sentence]\n",
        "\n",
        "for i, sentence in enumerate(test_sentences):\n",
        "    # For test sentences, we have to tokenize the sentences as well\n",
        "    test_sentences[i] = [word2idx[word.lower()] if word.lower() in word2idx else word2idx['_UNK'] for word in nltk.word_tokenize(sentence)"
      ],
      "metadata": {
        "id": "mDJAxaDgncRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_input(sentences, seq_len):\n",
        "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
        "    for ii, review in enumerate(sentences):\n",
        "        if len(review) != 0:\n",
        "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "    return features"
      ],
      "metadata": {
        "id": "pqLs_ExrnvVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 200 #The length that the sentences will be padded/shortened to\n",
        "\n",
        "train_sentences = pad_input(train_sentences, seq_len)\n",
        "test_sentences = pad_input(test_sentences, seq_len)"
      ],
      "metadata": {
        "id": "Sbc9fzVwn1CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)"
      ],
      "metadata": {
        "id": "A4ALtnj0n4BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCYOCyEbeP0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50b3968e-ad57-4187-9d1b-9c097147ae1e"
      },
      "source": [],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_sentences[:2]: \n",
            "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     2    67     5\n",
            "  12480    90   244   399     7   189     3     3]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0    86   623    39    33    70    19    12  4342   151     3   193\n",
            "    168     3     3    49   293   348     3     3]]\n",
            "train_labels[:2]: \n",
            "[1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checkpoint 2:**\n",
        "\n",
        "By following block 16-18, you should be able to build the dataset and dataloader from Pytorch which loads your inputs and labels and turns them into batches. A batch of the inputs/labels should look like the following."
      ],
      "metadata": {
        "id": "L3NakHMKfYgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n"
      ],
      "metadata": {
        "id": "xmnWEhEEn8q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
        "val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(val_labels))\n",
        "test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
        "\n",
        "batch_size = 400\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "zzuslD3Ln9Ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwGEb_dgn9dD",
        "outputId": "9ef002b1-3452-404c-9abc-d65e8e08c49d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU not available, CPU used\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "\n",
        "print(sample_x.shape, sample_y.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "IPOyCk0WoaIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HFuIyFMSoQd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaW_UWKAeRYi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cb2240c-463b-4f6e-8a38-22aa4b856d59"
      },
      "source": [],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU not available, CPU used\n",
            "torch.Size([400, 200]) torch.Size([400])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checkpoint 3:**\n",
        "\n",
        "By following block 19 and 20, you should be able to set up the LSTM model for classification, the structure should look like the following."
      ],
      "metadata": {
        "id": "ANY5-5zrgPdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class SentimentNet(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        super(SentimentNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        out = self.sigmoid(out)\n",
        "        \n",
        "        out = out.view(batch_size, -1)\n",
        "        out = out[:,-1]\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden\n",
        "\n"
      ],
      "metadata": {
        "id": "35HCJU_2pGkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word2idx) + 1\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "\n",
        "model = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "model.to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "M-andpmGpNii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Hcv1-TypHQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7IevHjNezsE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33bf9efa-180c-4197-9571-a69a5ca5df45"
      },
      "source": [],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentNet(\n",
            "  (embedding): Embedding(19554, 400)\n",
            "  (lstm): LSTM(400, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checkpoint 4:**\n",
        "\n",
        "By following block 21-22, train your model."
      ],
      "metadata": {
        "id": "Rlcm0R5Bgrus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr=0.005\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jFsJ32AmpYSj",
        "outputId": "e7887b81-5c50-4228-b58d-c24e3b2c1874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0f8d1f16040a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "epochs = 2\n",
        "counter = 0\n",
        "print_every = 1000\n",
        "clip = 5\n",
        "valid_loss_min = np.Inf\n",
        "\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "    h = model.init_hidden(batch_size)\n",
        "    \n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "        h = tuple([e.data for e in h])\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        model.zero_grad()\n",
        "        output, h = model(inputs, h)\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        \n",
        "        if counter%print_every == 0:\n",
        "            val_h = model.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            for inp, lab in val_loader:\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "                inp, lab = inp.to(device), lab.to(device)\n",
        "                out, val_h = model(inp, val_h)\n",
        "                val_loss = criterion(out.squeeze(), lab.float())\n",
        "                val_losses.append(val_loss.item())\n",
        "                \n",
        "            model.train()\n",
        "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "            if np.mean(val_losses) <= valid_loss_min:\n",
        "                torch.save(model.state_dict(), './state_dict.pt')\n",
        "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
        "                valid_loss_min = np.mean(val_losses)\n",
        "\n"
      ],
      "metadata": {
        "id": "DI0mJbuwpY9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6NG08fdfLmZ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checkpoint 5:**\n",
        "\n",
        "By following block 25, you should be able to implement the evaluation. The evaluation result should look like the following."
      ],
      "metadata": {
        "id": "GD8xORLzy4D9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Loading the best model\n",
        "model.load_state_dict(torch.load('./state_dict.pt'))\n",
        "\n",
        "test_losses = []\n",
        "num_correct = 0\n",
        "h = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "for inputs, labels in test_loader:\n",
        "    h = tuple([each.data for each in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, h = model(inputs, h)\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    pred = torch.round(output.squeeze()) #rounds the output to 0/1\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "        \n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))\n",
        "\n"
      ],
      "metadata": {
        "id": "hTO1Aa7Upr9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVSvmhjDfme9",
        "outputId": "c39ac178-9bd9-4aa1-a622-b61544efaabb"
      },
      "source": [],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.507\n",
            "Test accuracy: 74.652%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2:**\n",
        "\n",
        "Modify the model from task 1 to use GRU instead of LSTM. Compute and report the accuracy on the test data.\n",
        "\n",
        "NOTE: at this point, you should be able to finish the task without checkpoints. Try to modify/re-use the function you implemented in task 1."
      ],
      "metadata": {
        "id": "e4QGQ3bxzOeS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ddh08Z5C6FNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3:**\n",
        "\n",
        "Modify the model from task 1 to use bidirectional LSTM. Compute and report the accuracy on the test data.\n",
        "\n",
        "NOTE: Try modify/re-use the function you implemented in task 1, 2."
      ],
      "metadata": {
        "id": "qilyLPG6zyde"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d_LC75y46FlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4:**\n",
        "\n",
        "Modify the model from 3.1 to use bidirectional GRU. Compute and report the accuracy on the test data."
      ],
      "metadata": {
        "id": "PibMtooIz7PK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XDkniUZX6GFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 5:**\n",
        "\n",
        "Pick the best model so far and train the model starting from pretrained [GloveTwitter 100d](https://nlp.stanford.edu/projects/glove/) (from 'glove.twitter.27B', use the same vocabulary\n",
        "as before, just initialize the embedding of the words using Glove embeddings). Compute and report the accuracy on the\n",
        "test data.\n",
        "\n",
        "NOTE: the modification on the model should be very similar to the one you did in part one for LSTM language modeling with glove 100d."
      ],
      "metadata": {
        "id": "Zw9boPMH0E86"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A2k1JTTM6GZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbDNXfiJcGHb"
      },
      "source": [
        "**Task 6:**\n",
        "\n",
        "Using your best model so far, conduct a 5-fold (stratified) cross validation on your training data and a grid\n",
        "search to pick the best hidden size (try 128 or 512) and embedding size (try 100 or 400). Compute and report the average\n",
        "accuracies for each of the choice combination. \n",
        "\n",
        "NOTE: You don't need glove for word embeddings in this task.\n",
        "\n",
        "HINT: you can implement this by using the 'StratifiedKFold' from sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With hyperparam 512, 400:"
      ],
      "metadata": {
        "id": "AhtINLtQ59HX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EfTR5FSG6Cqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsmS-AZcc2ig"
      },
      "source": [
        "With hyperparam 512, 100:"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dh3PDn1n6C9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y_c9utDc74P"
      },
      "source": [
        "With hyperparam 128, 400:"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kVBXv4qy6DRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP66PTmNdARV"
      },
      "source": [
        "With hyperparam 128, 100:"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xxXne0746Dt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 7:**\n",
        "\n",
        "Train the model on all your training data using the best combination of hyperparameters you find in task 6. Compute\n",
        "and report the accuracy on the test data."
      ],
      "metadata": {
        "id": "PTcETr0d1TyB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A-wczi046Kjp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}